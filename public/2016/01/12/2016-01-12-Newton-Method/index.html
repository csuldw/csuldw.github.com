<!doctype html>
<html class="theme-next use-motion ">
<head>
  

<meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>


<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />








  <link rel="stylesheet" type="text/css" href="/vendors/fancybox/source/jquery.fancybox.css?v=2.1.5"/>



  
    <link href='//fonts.useso.com/css?family=Lato:300,400,700,400italic&subset=latin,latin-ext' rel='stylesheet' type='text/css'>
  


<link rel="stylesheet" type="text/css" href="/vendors/font-awesome/css/font-awesome.min.css?v=4.4.0" />

<link rel="stylesheet" type="text/css" href="/css/main.css?v=0.4.5.2"/>


    <meta name="description" content="中南大学在读硕士，关注机器学习、数据挖掘和人工智能领域." />



  <meta name="keywords" content="GLM,Machine Learning,指数分布族,牛顿方法," />



  <link rel="alternate" href="/atom.xml" title="D.W's Notes - Machine Learning" type="application/atom+xml" />



  <link rel="shortcut icon" type="image/x-icon" href="/images/favicon.ico?v=0.4.5.2" />


<meta name="description" content="回头再温习一下Andrew Ng的机器学习视频课，顺便把没写完的笔记写完。

本节内容

牛顿方法
指数分布族
广义线性模型">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习-牛顿方法&指数分布族&GLM">
<meta property="og:url" content="http://csuldw.github.io/2016/01/12/2016-01-12-Newton-Method/index.html">
<meta property="og:site_name" content="D.W's Notes - Machine Learning">
<meta property="og:description" content="回头再温习一下Andrew Ng的机器学习视频课，顺便把没写完的笔记写完。

本节内容

牛顿方法
指数分布族
广义线性模型">
<meta property="og:image" content="http://img.blog.csdn.net/20151006094200515">
<meta property="og:image" content="http://img.blog.csdn.net/20151006095845371">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24a_n%20%3D%20a_%7Bn-1%7D-%5Cfrac%7Bf%28a_%7Bn-1%7D%29%7D%7Bf%27%28a_%7Bn-1%7D%29%7D%24%24">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24H_%7Bij%7D%3D%5Cdfrac%7B%5Cpartial%5E%7B2%7D%5Cell%28%5Ctheta%29%7D%7B%5Cpartial%5Ctheta_%7Bi%7D%5Ctheta_%7Bj%7D%7D%24%24">
<meta property="og:image" content="http://img.blog.csdn.net/20151006105233062">
<meta property="og:image" content="http://img.blog.csdn.net/20151006105357353">
<meta property="og:image" content="http://img.blog.csdn.net/20151006105614842">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24%5Cphi_%7Bk%7D%20%3D%201-%20%5Csum_%7Bi%3D1%7D%5E%7Bk-1%7D%5Cphi_%7Bi%7D%24%24">
<meta property="og:image" content="http://img.blog.csdn.net/20151006125938706">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24E%5B%28T%28y%29%29_%7Bi%7D%5D%3D%5Csum_%7By%3D1%7D%5E%7Bk%7D%28T%28y%29%29%7B%5Cphi%7D_i%3D%5Csum_%7By%3D1%7D%5E%7Bk%7DI%28y%3Di%29%5Cphi_i%3D%5Cphi_i%24%24">
<meta property="og:image" content="http://img.blog.csdn.net/20151006131133675">
<meta property="og:image" content="http://img.blog.csdn.net/20151006131218488">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24%5Ceta_%7Bi%7D%3Dlog%5Cfrac%7B%5Cphi_%7Bi%7D%7D%7B%5Cphi_%7Bk%7D%7D%5CRightarrow%20%5Cphi_%7Bi%7D%3D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%24%24">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24%5Ceta_%7Bk%7D%20%3D%20log%20%5Cfrac%7B%5Cphi_%7Bk%7D%7D%7B%5Cphi_%7Bk%7D%7D%3D0%24%24">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%5Cphi_%7Bi%7D%3D%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%3D1%20%5CRightarrow%20%5Cphi_%7Bk%7D%3D%5Cfrac%7B1%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%24%24">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%5Ceta_%7Bi%7D%3Dlog%5Cfrac%7B%5Cphi_%7Bi%7D%7D%7B%5Cphi_%7Bk%7D%7D%5CRightarrow%20%5Cphi_%7Bi%7D%3D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24%5Cphi_%7Bi%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B1+%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%24%24">
<meta property="og:image" content="http://latex.codecogs.com/gif.latex?%24%24P%28y%3Di%29%3D%5Cphi_%7Bi%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B1+%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%3D%5Cfrac%7Be%5E%7B%5Ctheta_%7Bi%7D%5E%7BT%7Dx%7D%7D%7B1+%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ctheta_%7Bj%7D%5E%7BT%7Dx%7D%7D%24%24">
<meta property="og:image" content="http://img.blog.csdn.net/20151006132537928">
<meta property="og:image" content="http://img.blog.csdn.net/20151006132628356">
<meta property="og:updated_time" content="2016-03-14T13:43:20.336Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="机器学习-牛顿方法&指数分布族&GLM">
<meta name="twitter:description" content="回头再温习一下Andrew Ng的机器学习视频课，顺便把没写完的笔记写完。

本节内容

牛顿方法
指数分布族
广义线性模型">


<script type="text/javascript" id="hexo.configuration">
  var CONFIG = {
    scheme: '',
    sidebar: 'post'
  };
</script>



  <title> 机器学习-牛顿方法&指数分布族&GLM | D.W's Notes - Machine Learning </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  <!--[if lte IE 8]>
  <div style=' clear: both; height: 59px; padding:0 0 0 15px; position: relative;margin:0 auto;'>
    <a href="http://windows.microsoft.com/en-US/internet-explorer/products/ie/home?ocid=ie6_countdown_bannercode">
      <img src="http://7u2nvr.com1.z0.glb.clouddn.com/picouterie.jpg" border="0" height="42" width="820"
           alt="You are using an outdated browser. For a faster, safer browsing experience, upgrade for free today or use other browser ,like chrome firefox safari."
           style='margin-left:auto;margin-right:auto;display: block;'/>
    </a>
  </div>
<![endif]-->
  






  <div id="container" class="container one-column page-post-detail">

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-meta ">
  
</div>

<div class="site-nav-toggle">
  <button>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
    <span class="btn-bar"></span>
  </button>
</div>

<nav class="site-nav">
  
  
    <ul id="menu" class="menu ">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-user"></i> <br />
            
            关于
          </a>
        </li>
      

      
      
	  	<span style="font-size:14px;float:right;padding:39px 40px 0 0;">——悄悄是别离的笙箫，沉默是今晚的康桥.</span>
    </ul>
  

  
</nav>

 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">

        	<div id="content" class="content"> 

  <div id="posts" class="posts-expand">
    

  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                机器学习-牛顿方法&指数分布族&GLM
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            发表于
            <time itemprop="dateCreated" datetime="2016-01-12T10:24:00+08:00" content="2016-01-12">
              2016-01-12
            </time>
          </span>

          
            <span class="post-category" >
              &nbsp; | &nbsp; 分类于
              
                <span itemprop="about" itemscope itemtype="https://schema.org/Thing">
                  <a href="/categories/ML/" itemprop="url" rel="index">
                    <span itemprop="name">ML</span>
                  </a>
                </span>

                
                

              
            </span>
          

          
            
              <span class="post-comments-count">
                &nbsp; | &nbsp;
                <a href="/2016/01/12/2016-01-12-Newton-Method/#comments" itemprop="discussionUrl">
                  <span class="post-comments-count ds-thread-count" data-thread-key="2016/01/12/2016-01-12-Newton-Method/" itemprop="commentsCount"></span>
                </a>
              </span>
            
          

          
        </div>
      </header>
    


    <div class="post-body">

      
      

      
        <span itemprop="articleBody"><font color="green">回头再温习一下Andrew Ng的机器学习视频课，顺便把没写完的笔记写完。</font>

<p>本节内容</p>
<ul>
<li>牛顿方法</li>
<li>指数分布族</li>
<li>广义线性模型</li>
</ul>
<a id="more"></a>
<p>之前学习了梯度下降方法，关于梯度下降（gradient descent），这里简单的回顾下【参考感知机学习部分提到的梯度下降(<a href="http://blog.csdn.net/dream_angel_z/article/details/48915561" target="_blank" rel="external">gradient descent</a>)】。在最小化损失函数时，采用的就是梯度下降的方法逐步逼近最优解，规则为$\theta := \theta - \eta \nabla_{\theta} \ell(\theta)$。其实梯度下降属于一种优化方法，但梯度下降找到的是局部最优解。如下图：</p>
<center><br><img src="http://img.blog.csdn.net/20151006094200515" alt="这里写图片描述"><br></center>


<p>本节首先讲解的是牛顿方法（NewTon’s Method）。牛顿方法也是一种优化方法，它考虑的是<strong>全局最优</strong>。接着还会讲到指数分布族和广义线性模型。下面来详细介绍。</p>
<h2 id="1-牛顿方法"><strong>1.牛顿方法</strong></h2><p>现在介绍另一种最小化损失函数$\ell(\theta)$的方法——牛顿方法,参考<a href="http://www.phengkimving.com/calc_of_one_real_var/08_app_of_the_der_part_2/08_05_approx_of_roots_of_func_newtons_meth.htm" target="_blank" rel="external">Approximations Of Roots Of Functions – Newton’s Method</a><br>。它与梯度下降不同，其基本思想如下：</p>
<p>假设一个函数$f(x) = 0$,我们需要求解此时的$x$值。如下图所示：</p>
<center><br><img src="http://img.blog.csdn.net/20151006095845371" alt="这里写图片描述"><br>图1 $f(x0) = 0, a1, a2, a3, … 逐步接近 x0$.<br></center>

<p>在<br>$a_1$点的时候，$f(x)$切线的目标函数$y = f(a_1) + f ‘(a_1)(x – a_1)$. 由于$(a_2,0)$在这条线上，所以我们有$ 0 = f(a_1) + f ‘(a_1)(a_2 – a_1)$,so:</p>
<p>$$a_2 = a_1-\frac{f(a_1)}{f’(a_1)}$$</p>
<p>同理，在$a_2$点的时候，切线的目标函数$y = f(a_2) + f ‘(a_2)(x – a_2)$. 由于$(a_3,0)$在这条线上，所以我们有$ 0 = f(a_2) + f ‘(a_2)(a_3– a_2)$,so:</p>
<p>$$a_3 = a_2-\frac{f(a_2)}{f’(a_2)}$$</p>
<p>假设在第$n$次迭代，有$f(a_n)=0$,那么此时有下面这个递推公式：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24a_n%20%3D%20a_%7Bn-1%7D-%5Cfrac%7Bf%28a_%7Bn-1%7D%29%7D%7Bf%27%28a_%7Bn-1%7D%29%7D%24%24" alt=""></p>
<p>其中$n&gt;=2$.</p>
<p>最后得到的公式也就是牛顿方法的学习规则，为了和梯度下降对比，我们来替换一下变量，公式如下：</p>
<p>$$\theta := \theta - \frac{f(\theta)}{f’(\theta)}$$</p>
<font color="green"><strong>那么问题来了，怎么将牛顿方法应用到我们的问题上，最小化损失函数$\ell(\theta)$(或者是求极大似然估计的极大值)呢？</strong><br></font>

<p>  对于机器学习问题，现在我们优化的目标函数为极大似然估计$\ell$，当极大似然估计函数取值最大时，其导数为 0，这样就和上面函数f取 0 的问题一致了，令$f(\theta) = \ell’(\theta)$。极大似然函数的求解更新规则是：</p>
<p>$$\theta := \theta - \frac{\ell’(\theta)}{\ell’’(\theta)}$$</p>
<p>对于$\ell$，当一阶导数为零时，有极值；此时，如果二阶导数大于零，则$\ell$有极小值，如果二阶导数小于零，则有极大值。</p>
<p>上面的式子是当参数$\theta$为实数时的情况，下面我们要求出一般式。当参数为向量时，更新规则变为如下公式：</p>
<p>$$\theta := \theta - H^{-1} \nabla_{\theta}\ell(\theta)$$</p>
<p>其中$\nabla$后半部分$和之前梯度下降中提到的一样，是梯度，$H$是一个$n*n$的矩阵，$H $是函数的二次导数矩阵，被成为$Hessian$矩阵。其某个元素$ H_{ij}$ 计算公式如下：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24H_%7Bij%7D%3D%5Cdfrac%7B%5Cpartial%5E%7B2%7D%5Cell%28%5Ctheta%29%7D%7B%5Cpartial%5Ctheta_%7Bi%7D%5Ctheta_%7Bj%7D%7D%24%24" alt="$$H_{ij}=\dfrac{\partial^{2}\ell(\theta)}{\partial\theta_{i}\theta_{j}}$$"></p>
<font color="red"><strong>和梯度下降相比，牛顿方法的收敛速度更快，通常只要十几次或者更少就可以收敛，牛顿方法也被称为二次收敛（quadratic convergence），因为当迭代到距离收敛值比较近的时候，每次迭代都能使误差变为原来的平方。缺点是当参数向量较大的时候，每次迭代都需要计算一次 Hessian 矩阵的逆，比较耗时。</strong><br></font>

<h2 id="2-指数分布族（The_exponential_family）"><strong>2.指数分布族（The exponential family）</strong></h2><p>指数分布族是指可以表示为指数形式的概率分布。指数分布的形式如下：</p>
<p>$$P(y;\eta)=b(y)exp(\eta^{T}T(y)-a(\eta))$$</p>
<p>其中，η成为分布的<strong>自然参数</strong>（nature parameter）；T(y)是<strong>充分统计量</strong>（sufficient statistic），通常 <strong>T(y)=y</strong>。当参数 a、b、T 都固定的时候，就定义了一个以η为参数的函数族。</p>
<p>下面介绍两种分布，伯努利分布和高斯分布，分别把它们表示成指数分布族的形式。</p>
<h3 id="伯努利分布"><strong>伯努利分布</strong></h3><p>伯努利分布是对0，1问题进行建模的，对于Bernoulli（$\varphi$）,$y\epsilon{0, 1}$.有$p(y=1; \varphi ) = \varphi; p(y=0; \varphi ) = 1- \varphi$，下面将其推导成指数分布族形式：</p>
<center><br><img src="http://img.blog.csdn.net/20151006105233062" alt="这里写图片描述"><br></center>

<p>将其与指数族分布形式对比，可以看出：</p>
<center><br><img src="http://img.blog.csdn.net/20151006105357353" alt="这里写图片描述"><br></center>

<p>表明伯努利分布也是指数分布族的一种。从上述式子可以看到，$\eta$的形式与logistic函数（sigmoid）一致，这是因为 logistic模型对问题的前置概率估计其实就是伯努利分布。</p>
<h3 id="高斯分布"><strong>高斯分布</strong></h3><p>下面对高斯分布进行推导，推导公式如下（为了方便计算，我们将方差 $\sigma$设置为1）：</p>
<center><br><img src="http://img.blog.csdn.net/20151006105614842" alt="这里写图片描述"><br></center>

<p>将上式与指数族分布形式比对，可知：</p>
<p>$$b(y) = \frac{1}{\sqrt{2\pi}}exp(-\frac{1}{2}y^{2})$$</p>
<p>$$T(y) = y$$</p>
<p>$$\eta = \mu$$</p>
<p>$$a(\eta)=\frac{1}{2}\mu^{2}$$</p>
<p>两个典型的指数分布族，伯努利和高斯分布。其实大多数概率分布都可以表示成指数分布族形式，如下所示：</p>
<ul>
<li>伯努利分布（Bernoulli）：对 0、1 问题进行建模；</li>
<li>多项式分布（Multinomial）：多有 K 个离散结果的事件建模；</li>
<li>泊松分布（Poisson）：对计数过程进行建模，比如网站访问量的计数问题，放射性衰变的数目，商店顾客数量等问题；</li>
<li>伽马分布（gamma）与指数分布（exponential）：对有间隔的正数进行建模，比如公交车的到站时间问题；</li>
<li>β 分布：对小数建模；</li>
<li>Dirichlet 分布：对概率分布进建模；</li>
<li>Wishart 分布：协方差矩阵的分布；</li>
<li>高斯分布（Gaussian）；</li>
</ul>
<p>下面来介绍下广义线性模型（Generalized Linear Model, GLM）。</p>
<h2 id="3-广义线性模型（Generalized_Linear_Model,_GLM）"><strong>3.广义线性模型（Generalized Linear Model, GLM）</strong></h2><p>你可能会问，指数分布族究竟有何用？其实我们的目的是要引出GLM，通过指数分布族引出广义线性模型。</p>
<p>仔细观察伯努利分布和高斯分布的指数分布族形式中的$\eta$变量。可以发现，在伯努利的指数分布族形式中，$\eta$与伯努利分布的参数$\varphi$是一个logistic函数（下面会介绍logistic回归的推导）。此外，在高斯分布的指数分布族表示形式中，$\eta$与正态分布的参数$\mu$相等，下面会根据它推导出普通最小二乘法（Ordinary Least Squares）。通过这两个例子，我们大致可以得到一个结论，<font color="red"><strong>$η$以不同的映射函数与其它概率分布函数中的参数发生联系，从而得到不同的模型，广义线性模型正是将指数分布族中的所有成员（每个成员正好有一个这样的联系）都作为线性模型的扩展，通过各种非线性的连接函数将线性函数映射到其他空间，从而大大扩大了线性模型可解决的问题。</strong></font></p>
<p>下面我们看 GLM 的形式化定义，GLM 有三个假设：</p>
<ul>
<li>(1) $y|x; \theta~ExponentialFamily（\eta）$；给定样本$ x $与参数$θ$，样本分类$ y$ 服从指数分布族中的某个分布；</li>
<li>(2) 给定一个 $x$，我们需要的目标函数为$h_{\theta}(x)=E[T(y)|x]$;</li>
<li>(3)$\eta=\theta^{T}x$。</li>
</ul>
<p>依据这三个假设，我们可以推导出logistic模型与普通最小二乘模型。首先根据伯努利分布推导Logistic模型，推导过程如下:</p>
<p>$$h_{\theta}(x) = E[T(y)|x]=E[y|x]=p(y=1|x;\theta)$$</p>
<p>$$=\varphi$$</p>
<p>$$=\frac{1}{1+e^{-\eta}}$$</p>
<p>$$=\frac{1}{1+e^{-\theta^{T}x}}$$</p>
<p>公式第一行来自假设(2)，公式第二行通过伯努利分布计算得出，第三行通过伯努利的指数分布族表示形式得出，然后在公式第四行，根据假设三替换变量得到。</p>
<p>同样，可以根据高斯分布推导出普通最小二乘，如下：</p>
<p>$$h_{\theta}(x) = E(T(y)|x)=E[y|x]$$</p>
<p>$$=\mu$$</p>
<p>$$=\eta$$</p>
<p>$$=\theta^{T}x$$</p>
<p>公式第一行来自假设（2），第二行是通过高斯分布$y|x;\theta$~$ N(\mu,\sigma^{2})$计算得出，第三行是通过高斯分布的指数分布族形式表示得出，第四行即为假设（3）。</p>
<p>其中，将η与原始概率分布中的参数联系起来的函数成为正则相应函数（canonical response function），如$φ =\frac{1}{1+e^{-\eta}}、μ = η$即是正则响应函数。正则响应函数的逆成为正则关联函数（canonical link function）。</p>
<p>所以，对于广义线性模型，需要决策的是选用什么样的分布，当选取高斯分布时，我们就得到最小二乘模型，当选取伯努利分布时，我们得到 logistic 模型，这里所说的模型是假设函数 h 的形式。</p>
<p>最后总结一下：<font color="red"><strong>广义线性模型通过假设一个概率分布，得到不同的模型，而梯度下降和牛顿方法都是为了求取模型中的线性部分$(\theta^{T}x)$的参数$\theta$的。</strong></font></p>
<p><strong>多分类模型-Softmax Regression</strong></p>
<p>下面再给出GLM的一个例子——<strong>Softmax Regression</strong>.</p>
<p>假设一个分类问题，y可取k个值，即$y \epsilon{1,2,…,k}$。现在考虑的不再是一个二分类问题，现在的类别可以是多个。如邮件分类：垃圾邮件、个人邮件、工作相关邮件。下面要介绍的是多项式分布（multinomial distribution）。</p>
<p>多项式分布推导出的GLM可以解决多类分类问题，是 logistic 模型的扩展。对于多项式分布中的各个y的取值，我们可以使用k个参数$\phi_1,\phi_2,…,\phi_k$来表示这k个取值的概率。即</p>
<p>$$P(y=i) = \phi_{i}$$</p>
<p>但是，这些参数可能会冗余，更正式的说可能不独立，因为$\sum\phi<em>i=1$，知道了前k-1个，就可以通过$1-\sum</em>{i=1}^{k-1}\phi<em>{i}$计算出第k个概率。所以，我们只假定前k-1个结果的概率参数$\phi_1$,$\phi_2$,…,$\phi</em>{k-1}$，第k个输出的概率通过下面的式子计算得出：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24%5Cphi_%7Bk%7D%20%3D%201-%20%5Csum_%7Bi%3D1%7D%5E%7Bk-1%7D%5Cphi_%7Bi%7D%24%24" alt="$$\phi_{k} = 1- \sum_{i=1}^{k-1}\phi_{i}$$"></p>
<p>为了使多项式分布能够写成指数分布族的形式，我们首先定义 T(y)，如下所示：</p>
<center><br><img src="http://img.blog.csdn.net/20151006125938706" alt=""><br></center>

<p>和之前的不一样，这里我们的$T(y)$不等$y$，$T(y)$现在是一个$k-1$维的向量，而不是一个真实值。接下来，我们将使用$(T(y))_{i}$表示$T(y)$的第i个元素。</p>
<p>下面我们引入指数函数I，使得：</p>
<p>$$I(True)=1,I(False)=0$$</p>
<p>这样，$T(y)$向量中的某个元素还可以表示成：</p>
<p>$$(T(y))_{i}=I(y=i)$$</p>
<p>举例来说，当$ y=2 时，T(2)_2=I(2=2)=1，T(2)_3=I(2=3)=0$。根据公式 15，我们还可以得到：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24E%5B%28T%28y%29%29_%7Bi%7D%5D%3D%5Csum_%7By%3D1%7D%5E%7Bk%7D%28T%28y%29%29%7B%5Cphi%7D_i%3D%5Csum_%7By%3D1%7D%5E%7Bk%7DI%28y%3Di%29%5Cphi_i%3D%5Cphi_i%24%24" alt="$$E[(T(y))_{i}]=\sum_{y=1}^{k}(T(y)){\phi}_i=\sum_{y=1}^{k}I(y=i)\phi_i=\phi_i$$"></p>
<p>$$\sum_{i=1}^{k}I(y=i)=1$$</p>
<p>下面，二项分布转变为指数分布族的推导如下：</p>
<center><br><img src="http://img.blog.csdn.net/20151006131133675" alt="这里写图片描述"><br></center>

<p>其中，最后一步的各个变量如下：</p>
<center><br><img src="http://img.blog.csdn.net/20151006131218488" alt="这里写图片描述"><br></center>

<p>由$\eta$的表达式可知：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24%5Ceta_%7Bi%7D%3Dlog%5Cfrac%7B%5Cphi_%7Bi%7D%7D%7B%5Cphi_%7Bk%7D%7D%5CRightarrow%20%5Cphi_%7Bi%7D%3D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%24%24" alt="$$\eta_{i}=log\frac{\phi_{i}}{\phi_{k}}\Rightarrow \phi_{i}=\phi_{k}e^{\eta_{i}}$$"></p>
<p>为了方便，再定义：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24%5Ceta_%7Bk%7D%20%3D%20log%20%5Cfrac%7B%5Cphi_%7Bk%7D%7D%7B%5Cphi_%7Bk%7D%7D%3D0%24%24" alt="$$\eta_{k} = log \frac{\phi_{k}}{\phi_{k}}=0$$"></p>
<p>于是，可以得到：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%5Cphi_%7Bi%7D%3D%5Csum_%7Bj%3D1%7D%5E%7Bk%7D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%3D1%20%5CRightarrow%20%5Cphi_%7Bk%7D%3D%5Cfrac%7B1%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%24%24" alt="$$\sum_{j=1}^{k}\phi_{i}=\sum_{j=1}^{k}\phi_{k}e^{\eta_{i}}=1 \Rightarrow \phi_{k}=\frac{1}{\sum_{j=1}^{k}e^{\eta_{i}}}$$"></p>
<p>将上式代入到</p>
<p><img src="http://latex.codecogs.com/gif.latex?%5Ceta_%7Bi%7D%3Dlog%5Cfrac%7B%5Cphi_%7Bi%7D%7D%7B%5Cphi_%7Bk%7D%7D%5CRightarrow%20%5Cphi_%7Bi%7D%3D%5Cphi_%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D" alt="$$\eta_{i}=log\frac{\phi_{i}}{\phi_{k}}\Rightarrow \phi_{i}=\phi_{k}e^{\eta_{i}}$$">，得到：</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24%5Cphi_%7Bi%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B%5Csum_%7Bj%3D1%7D%5E%7Bk%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B1&plus;%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%24%24" alt="$$\phi_{i}=\frac{e^{\eta_{i}}}{\sum_{j=1}^{k}e^{\eta_{i}}}=\frac{e^{\eta_{i}}}{1+\sum_{j=1}^{k-1}e^{\eta_{i}}}$$"></p>
<p>从而，我们就得到了连接函数，有了连接函数后，就可以把多项式分布的概率表达出来:</p>
<p><img src="http://latex.codecogs.com/gif.latex?%24%24P%28y%3Di%29%3D%5Cphi_%7Bi%7D%3D%5Cfrac%7Be%5E%7B%5Ceta_%7Bi%7D%7D%7D%7B1&plus;%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ceta_%7Bi%7D%7D%7D%3D%5Cfrac%7Be%5E%7B%5Ctheta_%7Bi%7D%5E%7BT%7Dx%7D%7D%7B1&plus;%5Csum_%7Bj%3D1%7D%5E%7Bk-1%7De%5E%7B%5Ctheta_%7Bj%7D%5E%7BT%7Dx%7D%7D%24%24" alt="$$P(y=i)=\phi_{i}=\frac{e^{\eta_{i}}}{1+\sum_{j=1}^{k-1}e^{\eta_{i}}}=\frac{e^{\theta_{i}^{T}x}}{1+\sum_{j=1}^{k-1}e^{\theta_{j}^{T}x}}$$"></p>
<p>注意到，上式中的每个参数$\eta_i$都是一个可用线性向量$\theta_i^Tx$表示出来的，因而这里的$\theta$其实是一个二维矩阵。</p>
<p>于是，我们可以得到假设函数 h 如下：</p>
<center><br><img src="http://img.blog.csdn.net/20151006132537928" alt="这里写图片描述"><br></center>

<p>那么就建立了假设函数，最后就获得了最大似然估计 </p>
<center><br><img src="http://img.blog.csdn.net/20151006132628356" alt="这里写图片描述"><br></center>

<p>对该式子可以使用梯度下降算法或者牛顿方法求得参数$\theta$后，使用假设函数$h$对新的样例进行预测，即可完成多类分类任务。这种多种分类问题的解法被称为 softmax regression.</p>
<h2 id="References"><strong>References</strong></h2><ul>
<li><a href="http://www.phengkimving.com/calc_of_one_real_var/08_app_of_the_der_part_2/08_05_approx_of_roots_of_func_newtons_meth.htm" target="_blank" rel="external">Approximations Of Roots Of Functions – Newton’s Method</a></li>
<li>机器学习-Andrew Ng 斯坦福大学<a href="http://open.163.com/movie/2008/1/E/D/M6SGF6VB4_M6SGHKAED.html" target="_blank" rel="external">机器学习视频-第四讲</a></li>
</ul>
<hr>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a><br><br></strong></center></span>
      
    </div>

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/GLM/" rel="tag">#GLM</a>
          
            <a href="/tags/Machine-Learning/" rel="tag">#Machine Learning</a>
          
            <a href="/tags/指数分布族/" rel="tag">#指数分布族</a>
          
            <a href="/tags/牛顿方法/" rel="tag">#牛顿方法</a>
          
        </div>
      

      
        <div class="post-nav">
          <div class="post-nav-prev post-nav-item">
            
              <a href="/2016/01/22/2016-01-21-Python-notes/" rel="prev">
                <i class="fa fa-chevron-left"></i> Python笔记小记
              </a>
            
          </div>

          <div class="post-nav-next post-nav-item">
            
              <a href="/2016/01/02/2016-01-02-extracte-data-from-web-server-in-python/" rel="next">
                用python模拟网页数据提交 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
        <div class="ds-share flat" data-thread-key="2016/01/12/2016-01-12-Newton-Method/"
     data-title="机器学习-牛顿方法&指数分布族&GLM"
     data-content=""
     data-url="http://csuldw.github.io/2016/01/12/2016-01-12-Newton-Method/">
  <div class="ds-share-inline">
    <ul  class="ds-share-icons-16">

      <li data-toggle="ds-share-icons-more"><a class="ds-more" href="javascript:void(0);">分享到：</a></li>
      <li><a class="ds-weibo" href="javascript:void(0);" data-service="weibo">微博</a></li>
      <li><a class="ds-qzone" href="javascript:void(0);" data-service="qzone">QQ空间</a></li>
      <li><a class="ds-qqt" href="javascript:void(0);" data-service="qqt">腾讯微博</a></li>
      <li><a class="ds-wechat" href="javascript:void(0);" data-service="wechat">微信</a></li>

    </ul>
    <div class="ds-share-icons-more">
    </div>
  </div>
</div>
      
    </div>
  </div>

 </div>

        

        
          <div class="comments" id="comments">
            
              <div class="ds-thread" data-thread-key="2016/01/12/2016-01-12-Newton-Method/"
                   data-title="机器学习-牛顿方法&指数分布族&GLM" data-url="http://csuldw.github.io/2016/01/12/2016-01-12-Newton-Method/">
              </div>
            
          </div>
        
      

        
          
  
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" src="/images/default_avatar.jpg" alt="刘帝伟" itemprop="image"/>
          <p class="site-author-name" itemprop="name">刘帝伟</p>
        </div>
        <p class="site-description motion-element" itemprop="description">中南大学在读硕士，关注机器学习、数据挖掘和人工智能领域.</p>
        <nav class="site-state motion-element">
          <div class="site-state-item site-state-posts">
            <a href="/archives">
              <span class="site-state-item-count">57</span>
              <span class="site-state-item-name">日志</span>
            </a>
          </div>

          <div class="site-state-item site-state-categories">
            <a href="/categories">
              <span class="site-state-item-count">9</span>
              <span class="site-state-item-name">分类</span>
              </a>
          </div>

          <div class="site-state-item site-state-tags">
            <a href="/tags">
              <span class="site-state-item-count">73</span>
              <span class="site-state-item-name">标签</span>
              </a>
          </div>

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="menu-item-icon icon-next-feed"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
            
              <span class="links-of-author-item">
                <a href="https://github.com/csuldw" target="_blank">
                  <i class="fa fa-github"></i> GitHub
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://weibo.com/liudiwei210" target="_blank">
                  <i class="fa fa-weibo"></i> WeiBo
                </a>
              </span>
            
              <span class="links-of-author-item">
                <a href="http://blog.csdn.net/dream_angel_z" target="_blank">
                  <i class="fa fa-csdn"></i> CSDN
                </a>
              </span>
            
          
        </div>

        
        

        <div class="links-of-author motion-element">
          
        </div>

      </section>

      
        <section class="post-toc-wrap motion-element sidebar-panel-active">
          <div class="post-toc-indicator-top post-toc-indicator"></div>
          <div class="post-toc">
            
            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-牛顿方法"><span class="nav-number">1.</span> <span class="nav-text">1.牛顿方法</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-指数分布族（The_exponential_family）"><span class="nav-number">2.</span> <span class="nav-text">2.指数分布族（The exponential family）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#伯努利分布"><span class="nav-number">2.1.</span> <span class="nav-text">伯努利分布</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#高斯分布"><span class="nav-number">2.2.</span> <span class="nav-text">高斯分布</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-广义线性模型（Generalized_Linear_Model,_GLM）"><span class="nav-number">3.</span> <span class="nav-text">3.广义线性模型（Generalized Linear Model, GLM）</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#References"><span class="nav-number">4.</span> <span class="nav-text">References</span></a></li></ol></div>
            
          </div>
          <div class="post-toc-indicator-bottom post-toc-indicator"></div>
        </section>
      

    </div>
  </aside>


        
	  </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner"> <div class="copyright" >
  
  &copy;  2014 - 
  <span itemprop="copyrightYear">2016</span>
  <span class="with-love">
    <i class="icon-next-heart fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">刘帝伟</span>
</div>

<div class="powered-by">
  由 <a class="theme-link" href="http://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="#">
    FreeSky
  </a>(Reserved)

  <!--
  <span id="busuanzi_container_site_uv">
     &nbsp; | &nbsp;  访客<span id="busuanzi_value_site_uv"></span>人
  </span>
  <span id="busuanzi_container_site_pv">
    &nbsp; | &nbsp;  总访问量<span id="busuanzi_value_site_pv"></span>次
  </span>
  -->
  
</div>


<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
 </div>
    </footer>

    <div class="back-to-top"></div>
  </div>

  <script type="text/javascript" src="/vendors/jquery/index.js?v=2.1.3"></script>

  
  
  
    

  
    
  

  <script type="text/javascript">
    var duoshuoQuery = {short_name:"csuldw"};
    (function() {
      var ds = document.createElement('script');
      ds.type = 'text/javascript';ds.async = true;
      ds.src = (document.location.protocol == 'https:' ? 'https:' : 'http:') + '//static.duoshuo.com/embed.js';
      ds.charset = 'UTF-8';
      (document.getElementsByTagName('head')[0]
      || document.getElementsByTagName('body')[0]).appendChild(ds);
    })();
  </script>
    
     

    
  
  
  <script type="text/javascript" src="/vendors/fancybox/source/jquery.fancybox.pack.js"></script>
  <script type="text/javascript" src="/js/fancy-box.js?v=0.4.5.2"></script>


  <script type="text/javascript" src="/js/helpers.js?v=0.4.5.2"></script>
  

  <script type="text/javascript" src="/vendors/velocity/velocity.min.js"></script>
  <script type="text/javascript" src="/vendors/velocity/velocity.ui.min.js"></script>

  <script type="text/javascript" src="/js/motion_global.js?v=0.4.5.2" id="motion.global"></script>




  <script type="text/javascript" src="/js/nav-toggle.js?v=0.4.5.2"></script>
  <script type="text/javascript" src="/vendors/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  
<script type="text/javascript" src="/js/bootstrap.scrollspy.js?v=0.4.5.2" id="bootstrap.scrollspy.custom"></script>


<script type="text/javascript" id="sidebar.toc.highlight">
  $(document).ready(function () {
    var tocSelector = '.post-toc';
    var $tocSelector = $(tocSelector);
    var activeCurrentSelector = '.active-current';

    $tocSelector
      .on('activate.bs.scrollspy', function () {
        var $currentActiveElement = $(tocSelector + ' .active').last();

        removeCurrentActiveClass();
        $currentActiveElement.addClass('active-current');

        $tocSelector[0].scrollTop = $currentActiveElement.position().top;
      })
      .on('clear.bs.scrollspy', function () {
        removeCurrentActiveClass();
      });

    function removeCurrentActiveClass () {
      $(tocSelector + ' ' + activeCurrentSelector)
        .removeClass(activeCurrentSelector.substring(1));
    }

    function processTOC () {
      getTOCMaxHeight();
      toggleTOCOverflowIndicators();
    }

    function getTOCMaxHeight () {
      var height = $('.sidebar').height() -
                   $tocSelector.position().top -
                   $('.post-toc-indicator-bottom').height();

      $tocSelector.css('height', height);

      return height;
    }

    function toggleTOCOverflowIndicators () {
      tocOverflowIndicator(
        '.post-toc-indicator-top',
        $tocSelector.scrollTop() > 0 ? 'show' : 'hide'
      );

      tocOverflowIndicator(
        '.post-toc-indicator-bottom',
        $tocSelector.scrollTop() >= $tocSelector.find('ol').height() - $tocSelector.height() ? 'hide' : 'show'
      )
    }

    $(document).on('sidebar.motion.complete', function () {
      processTOC();
    });

    $('body').scrollspy({ target: tocSelector });
    $(window).on('resize', function () {
      if ( $('.sidebar').hasClass('sidebar-active') ) {
        processTOC();
      }
    });

    onScroll($tocSelector);

    function onScroll (element) {
      element.on('mousewheel DOMMouseScroll', function (event) {
          var oe = event.originalEvent;
          var delta = oe.wheelDelta || -oe.detail;

          this.scrollTop += ( delta < 0 ? 1 : -1 ) * 30;
          event.preventDefault();

          toggleTOCOverflowIndicators();
      });
    }

    function tocOverflowIndicator (indicator, action) {
      var $indicator = $(indicator);
      var opacity = action === 'show' ? 0.4 : 0;
      $indicator.velocity ?
        $indicator.velocity('stop').velocity({
          opacity: opacity
        }, { duration: 100 }) :
        $indicator.stop().animate({
          opacity: opacity
        }, 100);
    }

  });
</script>

<script type="text/javascript" id="sidebar.nav">
  $(document).ready(function () {
    var html = $('html');
    var TAB_ANIMATE_DURATION = 200;
    var hasVelocity = $.isFunction(html.velocity);

    $('.sidebar-nav li').on('click', function () {
      var item = $(this);
      var activeTabClassName = 'sidebar-nav-active';
      var activePanelClassName = 'sidebar-panel-active';
      if (item.hasClass(activeTabClassName)) {
        return;
      }

      var currentTarget = $('.' + activePanelClassName);
      var target = $('.' + item.data('target'));

      hasVelocity ?
        currentTarget.velocity('transition.slideUpOut', TAB_ANIMATE_DURATION, function () {
          target
            .velocity('stop')
            .velocity('transition.slideDownIn', TAB_ANIMATE_DURATION)
            .addClass(activePanelClassName);
        }) :
        currentTarget.animate({ opacity: 0 }, TAB_ANIMATE_DURATION, function () {
          currentTarget.hide();
          target
            .stop()
            .css({'opacity': 0, 'display': 'block'})
            .animate({ opacity: 1 }, TAB_ANIMATE_DURATION, function () {
              currentTarget.removeClass(activePanelClassName);
              target.addClass(activePanelClassName);
            });
        });

      item.siblings().removeClass(activeTabClassName);
      item.addClass(activeTabClassName);
    });

    $('.post-toc a').on('click', function (e) {
      e.preventDefault();
      var targetSelector = escapeSelector(this.getAttribute('href'));
      var offset = $(targetSelector).offset().top;
      hasVelocity ?
        html.velocity('stop').velocity('scroll', {
          offset: offset  + 'px',
          mobileHA: false
        }) :
        $('html, body').stop().animate({
          scrollTop: offset
        }, 500);
    });

    // Expand sidebar on post detail page by default, when post has a toc.
    var $tocContent = $('.post-toc-content');
    if (isDesktop() && CONFIG.sidebar === 'post') {
      if ($tocContent.length > 0 && $tocContent.html().trim().length > 0) {
        displaySidebar();
      }
    }
  });
</script>



  <script type="text/javascript">
    $(document).ready(function () {
      if (CONFIG.sidebar === 'always') {
        displaySidebar();
      }
      if (isMobile()) {
        FastClick.attach(document.body);
      }

      motionIntegrator.bootstrap();
    });
  </script>

  
  
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
      processEscapes: true,
      skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
  });
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for (i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


  
  

<script>window._bd_share_config={"common":{"bdSnsKey":{},"bdText":"","bdMini":"1","bdMiniList":false,"bdPic":"","bdStyle":"0","bdSize":"16"},"slide":{"type":"slide","bdImg":"3","bdPos":"left","bdTop":"250"}};with(document)0[(getElementsByTagName('head')[0]||body).appendChild(createElement('script')).src='http://bdimg.share.baidu.com/static/api/js/share.js?v=89860593.js?cdnversion='+~(-new Date()/36e5)];</script>



  
  <script type="text/javascript" src="/js/lazyload.js"></script>
  <script type="text/javascript">
    $(function () {
      $("#posts").find('img').lazyload({
        placeholder: "/images/loading.gif",
        effect: "fadeIn"
      });
    });
  </script>
</body>
</html>
