<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[D.W's Diaries]]></title>
  <subtitle><![CDATA[——悄悄是别离的笙箫，沉默是今晚的康桥。]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://csuldw.github.io//"/>
  <updated>2015-11-04T02:57:10.780Z</updated>
  <id>http://csuldw.github.io//</id>
  
  <author>
    <name><![CDATA[刘帝伟]]></name>
    <email><![CDATA[csu.ldw@csu.edu.cn]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Add header and footer to some file]]></title>
    <link href="http://csuldw.github.io/2015/11/03/2015-11-03%20Add%20header%20and%20footer%20to%20some%20file/"/>
    <id>http://csuldw.github.io/2015/11/03/2015-11-03 Add header and footer to some file/</id>
    <published>2015-11-03T08:24:00.000Z</published>
    <updated>2015-11-04T02:57:10.780Z</updated>
    <content type="html"><![CDATA[<p>今天整理资料的时候，发现要在很多文件中的头部和尾部添加相同的文本，于是自己使用Python做了一个简单的文件拼接功能，也可以说是文件追加功能，给一个文件批量追加头尾内容，达到省事的效果，顺便还可以练习下Python。下面来介绍下这个功能的代码：</p>
<p>现在有三个文件，如下：</p>
<ul>
<li>content.txt 位于一个叫path的文件中；</li>
<li>header.txt用于添加到content.txt头部的文件；</li>
<li>footer.txt用于添加到content.txt尾部的文件。</li>
</ul>
<p>现在要实现的功能就是，将header和footer分别添加到content的头部和尾部。 </p>
<a id="more"></a>
<hr>
<p>函数说明：</p>
<ul>
<li>add_footer(infile, outfile)：用于将footer内容添加到content中，第一个参数表示的添加到尾部的文件，如输入footer.txt，第二个为内容文件。如content.txt文件</li>
<li>add_header(infile, outfile, auto=True): 用于将一个文件放入好另一个文件的头部，如果auto=Ture，则不对内容做修改，auto为False的话，这里添加了部分需要的东西，如文件的创建时间、标题等信息。</li>
<li>addHeadAndFooter(path, header, footer, auto=False)：核心函数，调用头尾两个方法，此处的path为文件夹名称，该函数的功能是将path文件夹下的所有文件都添加头和尾的内容，auto默认为False，功能和上面的相同。</li>
<li>getStdTime(seconds):将时间戳格式的日期转换为标准格式，如：2015-11-03 10:24</li>
</ul>
<p>代码（AddHeader.py）：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">Created on Tue Nov 03 10:32:26 2015</span><br><span class="line"></span><br><span class="line">@author: liudiwei</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os,time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_footer</span><span class="params">(infile, outfile)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(infile,<span class="string">'r'</span>) <span class="keyword">as</span> inputfile:</span><br><span class="line">        <span class="keyword">with</span> open(outfile,<span class="string">'a'</span>) <span class="keyword">as</span> outfile:</span><br><span class="line">            outfile.write(<span class="string">"\n\n"</span>+<span class="string">''</span>.join(inputfile.readlines()))</span><br><span class="line"></span><br><span class="line"><span class="comment">#如果auto==True，直接将文件内容加入到当前文件</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">add_header</span><span class="params">(infile, outfile, auto=True)</span>:</span> </span><br><span class="line">    inf=open(infile,<span class="string">'r'</span>)</span><br><span class="line">    outf = open(outfile,<span class="string">'r'</span>)</span><br><span class="line">    header = inf.readlines()</span><br><span class="line">    content=outf.readlines()</span><br><span class="line">    <span class="keyword">if</span> auto==<span class="keyword">True</span>:</span><br><span class="line">        <span class="keyword">with</span> open(outfile,<span class="string">'w'</span>) <span class="keyword">as</span> output:</span><br><span class="line">            output.write(<span class="string">''</span>.join(header)+ <span class="string">"\n\n"</span> \</span><br><span class="line">                            +<span class="string">''</span>.join(content))  </span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        ctime=getStdTime(os.path.getctime(outfile))</span><br><span class="line">        title=<span class="string">"title: "</span> + outfile.split(<span class="string">'/'</span>)[<span class="number">1</span>].split(<span class="string">'.'</span>)[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">print</span> title</span><br><span class="line">        add_content=<span class="string">"---\n"</span></span><br><span class="line">        add_content=add_content+title+<span class="string">'\n'</span>  <span class="comment">#add title</span></span><br><span class="line">        add_content=add_content+ctime +<span class="string">'\n'</span> <span class="comment">#add date</span></span><br><span class="line">        add_content=add_content+<span class="string">''</span>.join(header)</span><br><span class="line">        <span class="keyword">with</span> open(outfile,<span class="string">'w'</span>) <span class="keyword">as</span> output:</span><br><span class="line">            output.write(<span class="string">''</span>.join(add_content)+ <span class="string">"\n\n"</span> \</span><br><span class="line">                        +<span class="string">''</span>.join(content))  </span><br><span class="line">    outf.close()</span><br><span class="line">    inf.close()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">addHeadAndFooter</span><span class="params">(path, header, footer, auto=False)</span>:</span></span><br><span class="line">    filelist=os.listdir(path)</span><br><span class="line">    <span class="keyword">for</span> eachfile <span class="keyword">in</span> filelist:</span><br><span class="line">        add_header(header,path + <span class="string">"/"</span> + eachfile, auto)</span><br><span class="line">        add_footer(footer,path + <span class="string">"/"</span> + eachfile)   </span><br><span class="line">        </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getStdTime</span><span class="params">(seconds)</span>:</span></span><br><span class="line">    x = time.localtime(seconds)</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"date: "</span>+ time.strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>,x)</span><br><span class="line">    </span><br><span class="line">        </span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    <span class="keyword">if</span> (len(os.sys.argv)&lt;<span class="number">4</span>):</span><br><span class="line">        <span class="keyword">raise</span> TypeError()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"os.sys.arg"</span></span><br><span class="line">    <span class="comment">#path="path"</span></span><br><span class="line">    <span class="comment">#header="head.md"</span></span><br><span class="line">    <span class="comment">#footer="footer.md"</span></span><br><span class="line">    os.chdir(<span class="string">"."</span>)</span><br><span class="line">    path=os.sys.argv[<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> path</span><br><span class="line">    header=os.sys.argv[<span class="number">2</span>]</span><br><span class="line">    footer=os.sys.argv[<span class="number">3</span>]</span><br><span class="line">    filelist=os.listdir(path)</span><br><span class="line">    addHeadAndFooter(path,header,footer)</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"Success added!"</span></span><br><span class="line">    </span><br><span class="line"><span class="comment">#----------------    </span></span><br><span class="line"><span class="comment"># command </span></span><br><span class="line"><span class="comment"># python AddHead.py "path" "header.txt" "footer.txt"</span></span><br><span class="line"><span class="comment">#----------------</span></span><br></pre></td></tr></table></figure>
<p>直接在console控制台上运行下列代码即可 </p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python AddHeader<span class="class">.py</span> <span class="string">"path"</span> <span class="string">"header.txt"</span> <span class="string">"footer.txt"</span></span><br></pre></td></tr></table></figure>
<hr>
<center><strong><br>此文乃博主即兴之作，如果你从中有所收获，欢迎前来赞助，为博主送上你的支持：<a href="http://csuldw.github.io/donation" target="_black"><font color="red">【赞助中心】</font></a>。<br>  博客主页： <a href="http://csuldw.github.io" target="_black">【D.W’s Diaries 】</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">【@拾毅者】</a><br><br></strong></center>


]]></content>
    <summary type="html">
    <![CDATA[<p>今天整理资料的时候，发现要在很多文件中的头部和尾部添加相同的文本，于是自己使用Python做了一个简单的文件拼接功能，也可以说是文件追加功能，给一个文件批量追加头尾内容，达到省事的效果，顺便还可以练习下Python。下面来介绍下这个功能的代码：</p>
<p>现在有三个文件，如下：</p>
<ul>
<li>content.txt 位于一个叫path的文件中；</li>
<li>header.txt用于添加到content.txt头部的文件；</li>
<li>footer.txt用于添加到content.txt尾部的文件。</li>
</ul>
<p>现在要实现的功能就是，将header和footer分别添加到content的头部和尾部。 </p>]]>
    
    </summary>
    
      <category term="Python" scheme="http://csuldw.github.io/tags/Python/"/>
    
      <category term="Python" scheme="http://csuldw.github.io/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python-RegEx（正则表达式）]]></title>
    <link href="http://csuldw.github.io/2015/10/29/2015-10-29%20Python%20RegEx/"/>
    <id>http://csuldw.github.io/2015/10/29/2015-10-29 Python RegEx/</id>
    <published>2015-10-29T12:24:00.000Z</published>
    <updated>2015-10-29T13:37:31.102Z</updated>
    <content type="html"><![CDATA[<p>关于Python的正则表达式，初步学习了下，感觉跟shell脚本的正则表达式大体相同，先来做个小结吧！</p>
<h2 id="正则表达式简介">正则表达式简介</h2><p>正则表达式在实际的文本文件处理中，经常用到，其实正则表达式并不是Python的一部分，其它语言中都有。正则表达式是用于处理字符串的强大工具，拥有自己独特的语法以及一个独立的处理引擎，效率上可能不如str自带的方法，但功能真的十分强大。得益于这一点，在提供了正则表达式的语言里，正则表达式的语法都是一样的，区别只在于不同的编程语言实现支持的语法数量不同；但不用担心，不被支持的语法通常是不常用的部分。如果已经在其他语言里使用过正则表达式，只需要简单看一看就可以上手了。</p>
<p>下图展示了使用正则表达式进行匹配的流程： </p>
<p><img src="http://ww4.sinaimg.cn/large/637f3c58gw1exic0q7k4ej20cj055t9e.jpg" alt=""></p>
<a id="more"></a>
<p>从上图我们可以看出，正则表达式的大致匹配过程是：依次拿出表达式和文本中的字符比较，如果每一个字符都能匹配，则匹配成功；一旦有匹配不成功的字符则匹配失败。如果表达式中有量词或边界，这个过程会稍微有一些不同，但也是很好理解的，来看看下面的这个正则表达式模式。</p>
<table>
<thead>
<tr>
<th>模式</th>
<th>描述</th>
</tr>
</thead>
<tbody>
<tr>
<td>^</td>
<td>匹配字符串的开头</td>
</tr>
<tr>
<td>$</td>
<td>匹配字符串的末尾。</td>
</tr>
<tr>
<td>.</td>
<td>匹配任意字符，除了换行符，当re.DOTALL标记被指定时，则可以匹配包括换行符的任意字符。</td>
</tr>
<tr>
<td>[…]</td>
<td>用来表示一组字符,单独列出：[amk] 匹配 ‘a’，’m’或’k’</td>
</tr>
<tr>
<td>[^…]</td>
<td>不在[]中的字符：[^abc] 匹配除了a,b,c之外的字符。</td>
</tr>
<tr>
<td>*</td>
<td>匹配0个或多个的表达式。</td>
</tr>
<tr>
<td>+</td>
<td>匹配1个或多个的表达式。</td>
</tr>
<tr>
<td>?</td>
<td>匹配0个或1个由前面的正则表达式定义的片段，非贪婪方式</td>
</tr>
<tr>
<td>{ n,}</td>
<td>精确匹配n个前面表达式。</td>
</tr>
<tr>
<td>{ n, m}</td>
<td>匹配 n 到 m 次由前面的正则表达式定义的片段，贪婪方式</td>
</tr>
<tr>
<td>(re)</td>
<td>G匹配括号内的表达式，也表示一个组</td>
</tr>
<tr>
<td>(?imx)</td>
<td>正则表达式包含三种可选标志：i, m, 或 x 。只影响括号中的区域。</td>
</tr>
<tr>
<td>(?-imx)</td>
<td>正则表达式关闭 i, m, 或 x 可选标志。只影响括号中的区域。</td>
</tr>
<tr>
<td>(?: re)</td>
<td>类似 (…), 但是不表示一个组</td>
</tr>
<tr>
<td>(?imx: re)</td>
<td>在括号中使用i, m, 或 x 可选标志</td>
</tr>
<tr>
<td>(?-imx: re)</td>
<td>在括号中不使用i, m, 或 x 可选标志</td>
</tr>
<tr>
<td>(?#…)</td>
<td>注释.</td>
</tr>
<tr>
<td>(?= re)</td>
<td>前向肯定界定符。如果所含正则表达式，以 … 表示，在当前位置成功匹配时成功，否则失败。但一旦所含表达式已经尝试，匹配引擎根本没有提高；模式的剩余部分还要尝试界定符的右边。</td>
</tr>
<tr>
<td>(?! re)</td>
<td>前向否定界定符。与肯定界定符相反；当所含表达式不能在字符串当前位置匹配时成功</td>
</tr>
<tr>
<td>(?&gt; re)</td>
<td>匹配的独立模式，省去回溯。</td>
</tr>
<tr>
<td>\w</td>
<td>匹配字母数字,等价于’[A-Za-z0-9_]’</td>
</tr>
<tr>
<td>\W</td>
<td>匹配非字母数字, [^A-Za-z0-9_]’</td>
</tr>
<tr>
<td>\s</td>
<td>匹配任意空白字符，等价于[\t\n\r\f].</td>
</tr>
<tr>
<td>\S</td>
<td>匹配任意非空字符,等价于[^ \f\n\r\t\v]</td>
</tr>
<tr>
<td>\d</td>
<td>匹配任意数字，等价于[0-9].</td>
</tr>
<tr>
<td>\D</td>
<td>匹配任意非数字,等价于[^0-9]。</td>
</tr>
<tr>
<td>\A</td>
<td>匹配字符串开始</td>
</tr>
<tr>
<td>\Z</td>
<td>匹配字符串结束，如果是存在换行，只匹配到换行前的结束字符串。c</td>
</tr>
<tr>
<td>\z</td>
<td>匹配字符串结束</td>
</tr>
<tr>
<td>\G</td>
<td>匹配最后匹配完成的位置。</td>
</tr>
<tr>
<td>\b</td>
<td>匹配一个单词边界，也就是指单词和空格间的位置。例如， ‘er\b’ 可以匹配”never” 中的 ‘er’，但不能匹配 “verb” 中的 ‘er’。</td>
</tr>
<tr>
<td>\B</td>
<td>匹配非单词边界。’er\B’ 能匹配 “verb” 中的 ‘er’，但不能匹配 “never” 中的 ‘er’。</td>
</tr>
<tr>
<td>\n, \t, 等.</td>
<td>匹配一个换行符。匹配一个制表符。等</td>
</tr>
<tr>
<td>\1…\9</td>
<td>匹配第n个分组的子表达式。</td>
</tr>
<tr>
<td>\10</td>
<td>匹配第n个分组的子表达式，如果它经匹配。否则指的是八进制字符码的表达式。</td>
</tr>
</tbody>
</table>
<p>下面从正则表达式的几个函数/方法来简单介绍下正则表达式的用法。</p>
<hr>
<h2 id="re-match函数">re.match函数</h2><p>re.match 尝试<strong>从字符串的开头匹配一个模式</strong>，如：下面的例子匹配第一个单词。 </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">"This is a very beautiful girl, I like her very much."</span></span><br><span class="line">m = re.match(<span class="string">r"(\w+)\s"</span>, text)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">	<span class="keyword">print</span> m.group(<span class="number">0</span>), <span class="string">'\n'</span>, m.group(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">print</span> <span class="string">'not match'</span></span><br></pre></td></tr></table></figure>
<p>输出:</p>
<pre><code class="markdown">
This
This
</code></pre>

<p>re.match的函数原型为：re.match(pattern, string, flags)</p>
<ul>
<li>第一个参数是正则表达式，这里为”(\w+)\s”，如果匹配成功，则返回一个Match，否则返回一个None；</li>
<li>第二个参数表示要匹配的字符串；</li>
<li>第三个参数是标致位，用于控制正则表达式的匹配方式，如：是否区分大小写，多行匹配等等。</li>
</ul>
<hr>
<h2 id="re-search函数">re.search函数</h2><p>re.search函数会在字符串内查找模式匹配,只到找到第一个匹配然后返回，如果字符串没有匹配，则返回None。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">"This is a very beautiful girl, I like her very much."</span></span><br><span class="line">m = re.search(<span class="string">r'\sbeaut(i)ful\s'</span>, text)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">	<span class="keyword">print</span> m.group(<span class="number">0</span>), m.group(<span class="number">1</span>)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">	<span class="keyword">print</span> <span class="string">'not search'</span></span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<pre><code class="markdown">
beautiful i
</code></pre>

<p>re.search的函数原型为： re.search(pattern, string, flags)</p>
<p>每个参数的含意与re.match一样。 </p>
<hr>
<h2 id="re-match与re-search的区别">re.match与re.search的区别</h2><p>re.match只匹配字符串的开始，如果字符串从一开始就不符合正则表达式，则匹配失败，函数返回None；而re.search匹配整个字符串，直到找到一个匹配。</p>
<p>请看下面这个实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">line = <span class="string">"This is a very beautiful girl, I like her very much."</span>;</span><br><span class="line">m = re.match( <span class="string">r'girl'</span>, line, re.M|re.I)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"match --&gt; m.group() : "</span>, m.group()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"No match!!"</span></span><br></pre></td></tr></table></figure>
<pre><code class="markdown">
search --> m.group() :  girl
search --> matchObj.group() :  dogs
</code></pre>

<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">m = re.search( <span class="string">r'girl'</span>, line, re.M|re.I)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"search --&gt; m.group() : "</span>, m.group()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">   <span class="keyword">print</span> <span class="string">"No match!"</span></span><br></pre></td></tr></table></figure>
<p>以上实例运行结果如下：</p>
<pre><code class="markdown">
search --> m.group() :  girl
</code></pre>

<hr>
<h2 id="re-sub函数">re.sub函数</h2><p>re.sub用于替换字符串中的匹配项。下面一个例子将字符串中的空格 ‘ ‘ 替换成 ‘-‘ : </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text = <span class="string">"I like Cats more than dogs!"</span></span><br><span class="line"><span class="keyword">print</span> re.sub(<span class="string">r'\s+'</span>, <span class="string">'-'</span>, text)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code class="markdown">
I-like-Cats-more-than-dogs!
</code></pre>


<p>re.sub的函数原型为：re.sub(pattern, repl, string, count)</p>
<p>其中第二个函数是替换后的字符串；本例中为’-‘</p>
<p>第四个参数指替换个数。默认为0，表示每个匹配项都替换。</p>
<p>re.sub还允许使用函数对匹配项的替换进行复杂的处理。如：re.sub(r’\s’, lambda m: ‘[‘ + m.group(0) + ‘]’, text, 0)；将字符串中的空格’ ‘替换为’[ ]’。</p>
<hr>
<h2 id="re-split函数">re.split函数</h2><p>可以使用re.split来分割字符串，如：re.split(r’-‘, text)；将字符串按’-‘符号分割成一个单词列表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text=<span class="string">"I-really-like-this-girl!"</span></span><br><span class="line">re.split(<span class="string">r'-'</span>,text)</span><br></pre></td></tr></table></figure>
<p>输出：</p>
<pre><code class="markdown">
['I', 'really', 'like', 'this', 'girl!']
</code></pre>


<hr>
<h2 id="re-findall函数">re.findall函数</h2><p>re.findall可以获取字符串中所有匹配的字符串。如：re.findall(r’\w<em>i\w</em>‘, text)；获取字符串中，包含’oo’的所有单词。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">text=<span class="string">"I-really-like-this-girl!"</span></span><br><span class="line">re.findall(<span class="string">r'girl'</span>,text)</span><br></pre></td></tr></table></figure>
<p>输出结果：</p>
<pre><code class="markdown">
['like', 'this', 'girl']
</code></pre>

<hr>
<h2 id="re-compile函数">re.compile函数</h2><p>可以把正则表达式编译成一个正则表达式对象。可以把那些经常使用的正则表达式编译成正则表达式对象，这样可以提高一定的效率。下面是一个正则表达式对象的一个例子：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> re</span><br><span class="line">regex = re.compile(<span class="string">r'\w*er\w*'</span>) <span class="comment"># 将正则表达式编译成Pattern对象</span></span><br><span class="line">text = <span class="string">"This is a very beautiful girl, I like her very much."</span></span><br><span class="line">m = regex.search(text) <span class="comment">#使用regex来匹配text字符串</span></span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">	<span class="keyword">print</span> m.group() <span class="comment"># 使用Match获得分组信息</span></span><br><span class="line"><span class="keyword">print</span> regex.findall(text)   <span class="comment">#查找所有包含'oo'的单词</span></span><br><span class="line"><span class="keyword">print</span> regex.sub(<span class="keyword">lambda</span> m: <span class="string">'['</span> + m.group(<span class="number">0</span>) + <span class="string">']'</span>, text) <span class="comment">#将字符串中含有'oo'的单词用[]括起来。</span></span><br></pre></td></tr></table></figure>
<p>分别输出下列信息：</p>
<pre><code class="markdown">
'very'
['very', 'her', 'very']
This is a [very] beautiful girl, I like [her] [very] much.
</code></pre>

<hr>
<h2 id="邮箱验证">邮箱验证</h2><p>使用Python写一个简单的邮箱验证的正则表达式：</p>
<p>根据csu.ldw@csu.edu.cn来填写规则</p>
<p>规则：</p>
<ul>
<li>@前面可以有’.’、‘_’,’-‘，但不能出现在头尾，而且不能连续出现</li>
<li>@后面到结尾之间，可以有多个子域名</li>
<li>邮箱的结尾为2~5个字母，比如cn、com、name等</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#-*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">Created on Thu Oct 29 20:28:57 2015</span><br><span class="line">@author: liudiwei</span><br><span class="line">"""</span></span><br><span class="line"><span class="keyword">import</span> re</span><br><span class="line">regex = re.compile(<span class="string">'^[A-Za-z0-9]+(([\.\_\-])?[A-Za-z0-9]+)+@([A-Za-z]+.)+[A-Za-z]&#123;2,5&#125;$'</span>)</span><br><span class="line">m = regex.match(<span class="string">"csu.ldw@csu.edu.cn"</span>)</span><br><span class="line"><span class="keyword">if</span> m:</span><br><span class="line">    <span class="keyword">print</span> m.group()</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"no match!"</span></span><br></pre></td></tr></table></figure>
<p>测试输出：</p>
<pre><code class="markdown">
csu.ldw@csu.edu.cn
</code></pre>

<p>当m = regex.match(“_csu.ldw@csu.edu.cn”)<br>当邮箱为：</p>
<pre><code class="markdown">
_csu.ldw@csu.edu.cn  
csu.ldw_@csu.edu.cn
csu.ldw@csu_.edu.cn
_csu.ldw@csu.edu.cn1
</code></pre>

<p>都不会匹配</p>
<p>提示：合法邮箱的规则可能不够完善，这里就简单的匹配这三个规则吧！</p>
<hr>
<p><br></p>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a> 个人博客： <a href="http://csuldw.github.io" target="_black">D.W’s Diary</a><br><br></strong></center>]]></content>
    <summary type="html">
    <![CDATA[<p>关于Python的正则表达式，初步学习了下，感觉跟shell脚本的正则表达式大体相同，先来做个小结吧！</p>
<h2 id="正则表达式简介">正则表达式简介</h2><p>正则表达式在实际的文本文件处理中，经常用到，其实正则表达式并不是Python的一部分，其它语言中都有。正则表达式是用于处理字符串的强大工具，拥有自己独特的语法以及一个独立的处理引擎，效率上可能不如str自带的方法，但功能真的十分强大。得益于这一点，在提供了正则表达式的语言里，正则表达式的语法都是一样的，区别只在于不同的编程语言实现支持的语法数量不同；但不用担心，不被支持的语法通常是不常用的部分。如果已经在其他语言里使用过正则表达式，只需要简单看一看就可以上手了。</p>
<p>下图展示了使用正则表达式进行匹配的流程： </p>
<p><img src="http://ww4.sinaimg.cn/large/637f3c58gw1exic0q7k4ej20cj055t9e.jpg" alt=""></p>]]>
    
    </summary>
    
      <category term="Python" scheme="http://csuldw.github.io/tags/Python/"/>
    
      <category term="正则表达式" scheme="http://csuldw.github.io/tags/%E6%AD%A3%E5%88%99%E8%A1%A8%E8%BE%BE%E5%BC%8F/"/>
    
      <category term="Python" scheme="http://csuldw.github.io/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title></title>
    <link href="http://csuldw.github.io/2015/10/25/2015-10-24%20image%20article/"/>
    <id>http://csuldw.github.io/2015/10/25/2015-10-24 image article/</id>
    <published>2015-10-25T12:27:51.133Z</published>
    <updated>2015-10-25T12:27:51.133Z</updated>
    <content type="html"><![CDATA[<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="width: 100%;"><img src="http://ww3.sinaimg.cn/large/637f3c58gw1ex7h85txfoj22s01kab29.jpg" alt=""></div></div><div class="group-picture-row"><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://ww2.sinaimg.cn/large/637f3c58gw1ex7h89fy8vj22gw1e0qv5.jpg" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://ww1.sinaimg.cn/large/637f3c58gw1ex7h8mjhd2j21hc0u0e81.jpg" alt=""></div><div class="group-picture-column" style="width: 33.333333333333336%;"><img src="http://ww2.sinaimg.cn/large/637f3c58gw1ex7h8hxw10j23341qgqv8.jpg" alt=""></div></div></div></div>]]></content>
    <summary type="html">
    <![CDATA[<div class="group-picture"><div class="group-picture-container"><div class="group-picture-row"><div class="group-picture-column" style="widt]]>
    </summary>
    
  </entry>
  
  <entry>
    <title><![CDATA[scikit-learn Preprocessing]]></title>
    <link href="http://csuldw.github.io/2015/10/25/2015-10-25%20scikit-learn%20preprocessing/"/>
    <id>http://csuldw.github.io/2015/10/25/2015-10-25 scikit-learn preprocessing/</id>
    <published>2015-10-25T02:24:00.000Z</published>
    <updated>2015-10-29T13:03:22.464Z</updated>
    <content type="html"><![CDATA[<p>本文主要是对照<a href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing" target="_blank" rel="external">scikit-learn的preprocessing</a>章节结合代码简单的回顾下预处理技术的几种方法，主要包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理。内容比较简单，仅供参考！</p>
<p>首先来回顾一下下面要用到的基本知识。</p>
<a id="more"></a>
<h2 id="一、知识回顾"><strong>一、知识回顾</strong></h2><p>均值公式：</p>
<p>$$\bar{x}=\frac{1}{n}\sum_{i=1}^{n}x_{i}$$</p>
<p>方差公式：</p>
<p>$$s^{2}=\frac{1}{n}\sum_{i=1}^{n}(x_{i}-\bar{x})^{2}$$</p>
<p>0-范数，向量中非零元素的个数。</p>
<p>1-范数：</p>
<p>$$||X||= \sum_{i=1}^{n} |x_{i}|$$</p>
<p>2-范数：</p>
<p>$$||X||_{2} =  (\sum_{i=1}^{n} x_{i}^{2})^{\frac{1}{2}}$$</p>
<p>p-范数的计算公式：</p>
<p>$$||X||_{p}=(|x1|^{p}+|x2|^{p}+…+|xn|^{p})^{\frac{1}{p}}$$</p>
<hr>
<p>数据标准化：当单个特征的样本取值相差甚大或明显不遵从高斯正态分布时，标准化表现的效果较差。实际操作中，经常忽略特征数据的分布形状，移除每个特征均值，划分离散特征的标准差，从而等级化，进而实现数据中心化。</p>
<h2 id="二、标准化(Standardization)，或者去除均值和方差进行缩放"><strong>二、标准化(Standardization)，或者去除均值和方差进行缩放</strong></h2><p>公式为：(X-X_mean)/X_std 计算时对每个属性/每列分别进行.</p>
<p>将数据按其属性(按列进行)减去其均值，然后除以其方差。最后得到的结果是，对每个属性/每列来说所有数据都聚集在0附近，方差值为1。</p>
<p>首先说明下sklearn中preprocessing库里面的scale函数使用方法：</p>
<figure class="highlight gradle"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sklearn.preprocessing.scale(X, axis=<span class="number">0</span>, with_mean=<span class="keyword">True</span>,with_std=<span class="keyword">True</span>,<span class="keyword">copy</span>=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure>
<p>根据参数的不同，可以沿任意轴标准化数据集。</p>
<p>参数解释：</p>
<ul>
<li>X：数组或者矩阵</li>
<li>axis：int类型，初始值为0，axis用来计算均值 means 和标准方差 standard deviations. 如果是0，则单独的标准化每个特征（列），如果是1，则标准化每个观测样本（行）。</li>
<li>with_mean: boolean类型，默认为True，表示将数据均值规范到0</li>
<li>with_std: boolean类型，默认为True，表示将数据方差规范到1</li>
</ul>
<p><strong>一个简单的例子</strong></p>
<p>假设现在我构造一个数据集X，然后想要将其标准化。下面使用不同的方法来标准化X：</p>
<p><strong>方法一：使用sklearn.preprocessing.scale()函数</strong></p>
<p><strong>方法说明：</strong></p>
<ul>
<li>X.mean(axis=0)用来计算数据X每个特征的均值；</li>
<li>X.std(axis=0)用来计算数据X每个特征的方差；</li>
<li>preprocessing.scale(X)直接标准化数据X。</li>
</ul>
<p>将代码整理到一个文件中：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">from sklearn <span class="built_in">import</span> preprocessing </span><br><span class="line"><span class="built_in">import</span> numpy as np</span><br><span class="line"><span class="variable">X =</span> np.array([[ <span class="number">1</span>., -<span class="number">1</span>.,  <span class="number">2</span>.],</span><br><span class="line">              [ <span class="number">2</span>.,  <span class="number">0</span>.,  <span class="number">0</span>.],</span><br><span class="line">              [ <span class="number">0</span>.,  <span class="number">1</span>., -<span class="number">1</span>.]])</span><br><span class="line"><span class="comment"># calculate mean</span></span><br><span class="line"><span class="variable">X_mean =</span> X.mean(<span class="variable">axis=</span><span class="number">0</span>)</span><br><span class="line"><span class="comment"># calculate variance </span></span><br><span class="line"><span class="variable">X_std =</span> X.std(<span class="variable">axis=</span><span class="number">0</span>)</span><br><span class="line"><span class="comment"># standardize X</span></span><br><span class="line"><span class="variable">X1 =</span> (X-X_mean)/X_std</span><br><span class="line"><span class="comment"># use function preprocessing.scale to standardize X</span></span><br><span class="line"><span class="variable">X_scale =</span> preprocessing.scale(X)</span><br></pre></td></tr></table></figure>
<p>最后X_scale的值和X1的值是一样的，前面是单独的使用数学公式来计算，主要是为了形成一个对比，能够更好的理解scale()方法。</p>
<p><strong>方法2：sklearn.preprocessing.StandardScaler类</strong></p>
<p>该方法也可以对数据X进行标准化处理，实例如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing </span><br><span class="line">import numpy as np</span><br><span class="line">X = np.array([[ <span class="number">1</span>., -<span class="number">1</span>.,  <span class="number">2</span>.],</span><br><span class="line">              [ <span class="number">2</span>.,  <span class="number">0</span>.,  <span class="number">0</span>.],</span><br><span class="line">              [ <span class="number">0</span>.,  <span class="number">1</span>., -<span class="number">1</span>.]])</span><br><span class="line">scaler = preprocessing.<span class="function"><span class="title">StandardScaler</span><span class="params">()</span></span></span><br><span class="line">X_scaled = scaler.<span class="function"><span class="title">fit_transform</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure>
<p>这两个方法得到最后的结果都是一样的。</p>
<hr>
<h2 id="三、将特征的取值缩小到一个范围（如0到1）"><strong>三、将特征的取值缩小到一个范围（如0到1）</strong></h2><p>除了上述介绍的方法之外，另一种常用的方法是将属性缩放到一个指定的最大值和最小值(通常是1-0)之间，这可以通过preprocessing.MinMaxScaler类来实现。</p>
<p>使用这种方法的目的包括：</p>
<ul>
<li>1、对于方差非常小的属性可以增强其稳定性；</li>
<li>2、维持稀疏矩阵中为0的条目。</li>
</ul>
<p>下面将数据缩至0-1之间，采用MinMaxScaler函数</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">from sklearn import preprocessing </span><br><span class="line">import numpy as np</span><br><span class="line">X = np.array([[ <span class="number">1</span>., -<span class="number">1</span>.,  <span class="number">2</span>.],</span><br><span class="line">              [ <span class="number">2</span>.,  <span class="number">0</span>.,  <span class="number">0</span>.],</span><br><span class="line">              [ <span class="number">0</span>.,  <span class="number">1</span>., -<span class="number">1</span>.]])</span><br><span class="line">min_max_scaler = preprocessing.<span class="function"><span class="title">MinMaxScaler</span><span class="params">()</span></span></span><br><span class="line">X_minMax = min_max_scaler.<span class="function"><span class="title">fit_transform</span><span class="params">(X)</span></span></span><br></pre></td></tr></table></figure>
<p>最后输出：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">array(<span class="string">[[ 0.5       ,  0.        ,  1.        ],</span><br><span class="line">       [ 1.        ,  0.5       ,  0.33333333],</span><br><span class="line">       [ 0.        ,  1.        ,  0.        ]]</span>)</span><br></pre></td></tr></table></figure>
<p>测试用例：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_test = np.array(<span class="string">[[ -3., -1.,  4.]]</span>)</span><br><span class="line">&gt;&gt;&gt; X_test_minmax = min_max_scaler.transform(X_test)</span><br><span class="line">&gt;&gt;&gt; X_test_minmax</span><br><span class="line">array(<span class="string">[[-1.5       ,  0.        ,  1.66666667]]</span>)</span><br></pre></td></tr></table></figure>
<p>注意：这些变换都是对列进行处理。</p>
<p>当然，在构造类对象的时候也可以直接指定最大最小值的范围：feature_range=(min, max)，此时应用的公式变为：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X_std=(X-X.<span class="function"><span class="title">min</span><span class="params">(axis=<span class="number">0</span>)</span></span>)/(X.<span class="function"><span class="title">max</span><span class="params">(axis=<span class="number">0</span>)</span></span>-X.<span class="function"><span class="title">min</span><span class="params">(axis=<span class="number">0</span>)</span></span>)</span><br><span class="line">X_minmax=X_std/(X.<span class="function"><span class="title">max</span><span class="params">(axis=<span class="number">0</span>)</span></span>-X.<span class="function"><span class="title">min</span><span class="params">(axis=<span class="number">0</span>)</span></span>)+X.<span class="function"><span class="title">min</span><span class="params">(axis=<span class="number">0</span>)</span></span>)</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="四、正则化(Normalization)"><strong>四、正则化(Normalization)</strong></h2><p>正则化的过程是将每个样本缩放到单位范数(每个样本的范数为1)，如果要使用如二次型(点积)或者其它核方法计算两个样本之间的相似性这个方法会很有用。</p>
<p>该方法是文本分类和聚类分析中经常使用的向量空间模型（Vector Space Model)的基础.</p>
<p>Normalization主要思想是对每个样本计算其p-范数，然后对该样本中每个元素除以该范数，这样处理的结果是使得每个处理后样本的p-范数(l1-norm,l2-norm)等于1。</p>
<p><strong>方法1：使用sklearn.preprocessing.normalize()函数</strong></p>
<figure class="highlight r"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = [[ <span class="number">1.</span>, -<span class="number">1.</span>,  <span class="number">2.</span>],</span><br><span class="line"><span class="keyword">...</span>      [ <span class="number">2.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>],</span><br><span class="line"><span class="keyword">...</span>      [ <span class="number">0.</span>,  <span class="number">1.</span>, -<span class="number">1.</span>]]</span><br><span class="line">&gt;&gt;&gt; X_normalized = preprocessing.normalize(X, norm=<span class="string">'l2'</span>)</span><br><span class="line">&gt;&gt;&gt; X_normalized                                      </span><br><span class="line">array([[ <span class="number">0.40</span><span class="keyword">...</span>, -<span class="number">0.40</span><span class="keyword">...</span>,  <span class="number">0.81</span><span class="keyword">...</span>],</span><br><span class="line">       [ <span class="number">1.</span>  <span class="keyword">...</span>,  <span class="number">0.</span>  <span class="keyword">...</span>,  <span class="number">0.</span>  <span class="keyword">...</span>],</span><br><span class="line">       [ <span class="number">0.</span>  <span class="keyword">...</span>,  <span class="number">0.70</span><span class="keyword">...</span>, -<span class="number">0.70</span><span class="keyword">...</span>]])</span><br></pre></td></tr></table></figure>
<p><strong>方法2：sklearn.preprocessing.StandardScaler类</strong></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; normalizer = preprocessing.<span class="constant">Normalizer</span>().fit(<span class="constant">X</span>)  <span class="comment"># fit does nothing</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; normalizer</span><br><span class="line"><span class="constant">Normalizer</span>(copy=<span class="constant">True</span>, norm=<span class="string">'l2'</span>)</span><br></pre></td></tr></table></figure>
<p>然后使用正则化实例来转换样本向量：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; normalizer.transform(X)                            </span><br><span class="line">array(<span class="string">[[ 0.40..., -0.40...,  0.81...],</span><br><span class="line">       [ 1.  ...,  0.  ...,  0.  ...],</span><br><span class="line">       [ 0.  ...,  0.70..., -0.70...]]</span>)</span><br><span class="line">&gt;&gt;&gt; normalizer.transform(<span class="string">[[-1.,  1., 0.]]</span>)             </span><br><span class="line">array(<span class="string">[[-0.70...,  0.70...,  0.  ...]]</span>)</span><br></pre></td></tr></table></figure>
<p>两种方法都可以，效果是一样的。</p>
<hr>
<h2 id="五、二值化(Binarization)"><strong>五、二值化(Binarization)</strong></h2><p>特征的二值化主要是为了将数据特征转变成boolean变量。在sklearn中，sklearn.preprocessing.Binarizer函数可以实现这一功能。实例如下：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X = <span class="string">[[ 1., -1.,  2.],</span><br><span class="line">...      [ 2.,  0.,  0.],</span><br><span class="line">...      [ 0.,  1., -1.]]</span></span><br><span class="line">&gt;&gt;&gt; binarizer = preprocessing.Binarizer().fit(X)  # fit does nothing</span><br><span class="line">&gt;&gt;&gt; binarizer</span><br><span class="line">Binarizer(copy=True, threshold=<span class="number">0.0</span>)</span><br><span class="line">&gt;&gt;&gt; binarizer.transform(X)</span><br><span class="line">array(<span class="string">[[ 1.,  0.,  1.],</span><br><span class="line">       [ 1.,  0.,  0.],</span><br><span class="line">       [ 0.,  1.,  0.]]</span>)</span><br></pre></td></tr></table></figure>
<p>Binarizer函数也可以设定一个阈值，结果数据值大于阈值的为1，小于阈值的为0，实例代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; binarizer = preprocessing.<span class="function"><span class="title">Binarizer</span><span class="params">(threshold=<span class="number">1.1</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; binarizer.<span class="function"><span class="title">transform</span><span class="params">(X)</span></span></span><br><span class="line">array([[ <span class="number">0</span>.,  <span class="number">0</span>.,  <span class="number">1</span>.],</span><br><span class="line">       [ <span class="number">1</span>.,  <span class="number">0</span>.,  <span class="number">0</span>.],</span><br><span class="line">       [ <span class="number">0</span>.,  <span class="number">0</span>.,  <span class="number">0</span>.]])</span><br></pre></td></tr></table></figure>
<hr>
<h2 id="六、缺失值处理"><strong>六、缺失值处理</strong></h2><p>由于不同的原因，许多现实中的数据集都包含有缺失值，要么是空白的，要么使用NaNs或者其它的符号替代。这些数据无法直接使用scikit-learn分类器直接训练，所以需要进行处理。幸运地是，sklearn中的<strong>Imputer</strong>类提供了一些基本的方法来处理缺失值，如使用均值、中位值或者缺失值所在列中频繁出现的值来替换。</p>
<p>下面是使用均值来处理的实例：</p>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn.preprocessing import Imputer</span><br><span class="line">&gt;&gt;&gt; imp = Imputer(missing_values=<span class="string">'NaN'</span>, strategy=<span class="string">'mean'</span>, axis=<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt; imp.fit(<span class="string">[[1, 2], [np.nan, 3], [7, 6]]</span>)</span><br><span class="line">Imputer(axis=<span class="number">0</span>, copy=True, missing_values=<span class="string">'NaN'</span>, strategy=<span class="string">'mean'</span>, verbose=<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt; X = <span class="string">[[np.nan, 2], [6, np.nan], [7, 6]]</span></span><br><span class="line">&gt;&gt;&gt; <span class="built_in">print</span>(imp.transform(X))                           </span><br><span class="line"><span class="string">[[ 4.          2.        ]</span><br><span class="line"> [ 6.          3.666...]</span><br><span class="line"> [ 7.          6.        ]]</span></span><br></pre></td></tr></table></figure>
<p>Imputer类同样支持稀疏矩阵：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import scipy<span class="class">.sparse</span> as sp</span><br><span class="line">&gt;&gt;&gt; X = sp.<span class="function"><span class="title">csc_matrix</span><span class="params">([[<span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">3</span>], [<span class="number">7</span>, <span class="number">6</span>]])</span></span></span><br><span class="line">&gt;&gt;&gt; imp = <span class="function"><span class="title">Imputer</span><span class="params">(missing_values=<span class="number">0</span>, strategy=<span class="string">'mean'</span>, axis=<span class="number">0</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; imp.<span class="function"><span class="title">fit</span><span class="params">(X)</span></span></span><br><span class="line"><span class="function"><span class="title">Imputer</span><span class="params">(axis=<span class="number">0</span>, copy=True, missing_values=<span class="number">0</span>, strategy=<span class="string">'mean'</span>, verbose=<span class="number">0</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; X_test = sp.<span class="function"><span class="title">csc_matrix</span><span class="params">([[<span class="number">0</span>, <span class="number">2</span>], [<span class="number">6</span>, <span class="number">0</span>], [<span class="number">7</span>, <span class="number">6</span>]])</span></span></span><br><span class="line">&gt;&gt;&gt; <span class="function"><span class="title">print</span><span class="params">(imp.transform(X_test)</span></span>)                      </span><br><span class="line">[[ <span class="number">4</span>.          <span class="number">2</span>.        ]</span><br><span class="line"> [ <span class="number">6</span>.          <span class="number">3.666</span>...]</span><br><span class="line"> [ <span class="number">7</span>.          <span class="number">6</span>.        ]]</span><br></pre></td></tr></table></figure>
<p>本文讲解的比较接单，如果对这些不是很理解的话，请到scikit-learn的官网中查看英文版本：<a href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing" target="_blank" rel="external">preprocessing</a>.</p>
<h2 id="References"><strong>References</strong></h2><ul>
<li><a href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing" target="_blank" rel="external">Scikit-learn preprocessing</a>.</li>
</ul>
<hr>
<p><br></p>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a> 个人博客： <a href="http://csuldw.github.io" target="_black">D.W’s Diary</a><br><br></strong></center>




]]></content>
    <summary type="html">
    <![CDATA[<p>本文主要是对照<a href="http://scikit-learn.org/stable/modules/preprocessing.html#preprocessing">scikit-learn的preprocessing</a>章节结合代码简单的回顾下预处理技术的几种方法，主要包括标准化、数据最大最小缩放处理、正则化、特征二值化和数据缺失值处理。内容比较简单，仅供参考！</p>
<p>首先来回顾一下下面要用到的基本知识。</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="preprocessing" scheme="http://csuldw.github.io/tags/preprocessing/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习之特征工程]]></title>
    <link href="http://csuldw.github.io/2015/10/24/2015-10-24%20feature%20engineering/"/>
    <id>http://csuldw.github.io/2015/10/24/2015-10-24 feature engineering/</id>
    <published>2015-10-24T02:24:00.000Z</published>
    <updated>2015-10-29T13:03:36.052Z</updated>
    <content type="html"><![CDATA[<p>在这个振奋人心的程序员节日里，我决定认真地写一篇文章来纪念一下自己这长达六年程序员史。o(╯□╰)o</p>
<p>本文是一篇关于特征工程的总结类文章，如有不足之处或理解有偏差的地方，还望多多指教。</p>
<p>首先，给一张特征工程的思维导图吧：</p>
<p><img src="http://ww1.sinaimg.cn/large/637f3c58gw1exd7mcjk7yj28k33uwaoe.jpg" alt="特征工程"></p>
<a id="more"></a>
<center><br><font color="green"><strong>【如果要浏览图片，建议将其下载到本地，使用图片浏览软件查看】</strong></font><br></center>

<p>关于特征工程（Feature Engineering），已经是很古老很常见的话题了，坊间常说：“数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已”。由此可见，特征工程在机器学习中占有相当重要的地位。在实际应用当中，可以说特征工程是机器学习成功的关键。纵观Kaggle、KDD等国内外大大小小的比赛，每个竞赛的冠军其实并没有用到很高深的算法，大多数都是在特征工程这个环节做出了出色的工作，然后使用一些常见的算法，比如LR，就能得到出色的性能。遗憾的是，在很多的书籍中并没有直接提到Feature Engineering，更多的是Feature selection。这也并不，很多ML书籍都是以讲解算法为主，他们的目的是从理论到实践来理解算法，所以用到的数据要么是使用代码生成的，要么是已经处理好的数据，并没有提到特征工程。在这篇文章，我打算自我总结下特征工程，让自己对特征工程有个全面的认识。在这我要说明一下，我并不是说那些书写的不好，其实都很有不错，主要是因为它们的目的是理解算法，所以直接给出数据相对而言对于学习和理解算法效果更佳。</p>
<p>这篇文章主要从以下三个问题出发来理解特征工程：</p>
<ul>
<li>特征工程是什么？</li>
<li>为什么要做特征工程？</li>
<li>应该如何做特征工程？</li>
</ul>
<p>对于第一个问题，我会通过特征工程的目的来解释什么是特征工程。对于第二个问题，主要从特征工程的重要性来阐述。对于第三个问题，我会从特征工程的子问题以及简单的处理方法来进一步说明。下面来看看详细内容！</p>
<hr>
<h2 id="1、特征工程是什么"><strong>1、特征工程是什么</strong></h2><p>首先来解释下什么是特征工程？</p>
<p>当你想要你的预测模型性能达到最佳时，你要做的不仅是要选取最好的算法，还要尽可能的从原始数据中获取更多的信息。那么问题来了，<font color="red">你应该如何为你的预测模型得到更好的数据呢？</font></p>
<p>想必到了这里你也应该猜到了，是的，这就是特征工程要做的事，它的目的就是<font color="red">获取更好的训练数据</font>。</p>
<p>关于特征工程的定义，Wikipedia上是这样说的：</p>
<pre><code>Feature engineering is <span class="operator">the</span> <span class="built_in">process</span> <span class="operator">of</span> <span class="keyword">using</span> domain knowledge <span class="operator">of</span> <span class="operator">the</span> data <span class="built_in">to</span> <span class="built_in">create</span> features that make machine learning algorithms work. ”
</code></pre><p>我的理解：</p>
<pre><code>特征工程是利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。
</code></pre><p>简而言之，特征工程就是一个把原始数据转变成特征的过程，这些特征可以很好的描述这些数据，并且利用它们建立的模型在未知数据上的表现性能可以达到最优（或者接近最佳性能）。从数学的角度来看，特征工程就是人工地去设计输入变量X。</p>
<p>特征工程更是一门艺术，跟编程一样。导致许多机器学习项目成功和失败的主要因素就是使用了不同的特征。说了这么多，想必你也大概知道了为什么要做特征工程，下面来说说特征工程的重要性。</p>
<hr>
<h2 id="2、特征工程的重要性"><strong>2、特征工程的重要性</strong></h2><p>OK！知道了特征工程是什么，那么我们必须要来了解下特征工程的重要性，为什么在实际工作中都要有特征工程这个过程，下面不同的角度来分析一下。</p>
<p>首先，我们大家都知道，数据特征会直接影响我们模型的预测性能。你可以这么说：“选择的特征越好，最终得到的性能也就越好”。这句话说得没错，但也会给我们造成误解。事实上，<font color="green">你得到的实验结果取决于你选择的模型、获取的数据以及使用的特征，甚至你问题的形式和你用来评估精度的客观方法也扮演了一部分</font>。此外，你的实验结果还受到许多相互依赖的属性的影响，你需要的是能够很好地描述你数据内部结构的好特征。</p>
<p><strong>（1）特征越好，灵活性越强</strong></p>
<p>只要特征选得好，即使是一般的模型（或算法）也能获得很好的性能，因为大多数模型（或算法）在好的数据特征下表现的性能都还不错。<font color="red">好特征的灵活性在于它允许你选择不复杂的模型，同时运行速度也更快，也更容易理解和维护</font>。</p>
<p><strong>（2）特征越好，构建的模型越简单</strong></p>
<p>有了好的特征，即便你的参数不是最优的，你的模型性能也能仍然会表现的很nice，所以你就不需要花太多的时间去寻找最有参数，这大大的降低了模型的复杂度，使模型趋于简单。</p>
<p><strong>（3）特征越好，模型的性能越出色</strong></p>
<p>显然，这一点是毫无争议的，我们进行特征工程的最终目的就是提升模型的性能。</p>
<p>下面从特征的子问题来分析下特征工程。</p>
<hr>
<h2 id="3、特征工程子问题"><strong>3、特征工程子问题</strong></h2><p>大家通常会把特征工程看做是一个问题。事实上，在特征工程下面，还有许多的子问题，主要包括：Feature Selection（特征选择）、Feature Extraction（特征提取）和Feature construction（特征构造）.下面从这三个子问题来详细介绍。</p>
<h3 id="3-1_特征选择Feature_Selection"><strong>3.1 特征选择Feature Selection</strong></h3><p>首先，从特征开始说起，假设你现在有一个标准的Excel表格数据，它的每一行表示的是一个观测样本数据，表格数据中的每一列就是一个特征。在这些特征中，有的特征携带的信息量丰富，有的（或许很少）则属于无关数据（irrelevant data），我们可以通过特征项和类别项之间的相关性（特征重要性）来衡量。比如，在实际应用中，常用的方法就是使用一些评价指标单独地计算出单个特征跟类别变量之间的关系。如Pearson相关系数，Gini-index（基尼指数），IG（信息增益）等，下面举Pearson指数为例，它的计算方式如下：</p>
<p>$$r_{xy}^2=(\frac{con(x,y)}{\sqrt{var(x)var(y)}})$$</p>
<p>其中，x属于X，X表一个特征的多个观测值，y表示这个特征观测值对应的类别列表。</p>
<p>Pearson相关系数的取值在0到1之间，如果你使用这个评价指标来计算所有特征和类别标号的相关性，那么得到这些相关性之后，你可以将它们从高到低进行排名，然后选择一个子集作为特征子集（比如top 10%），接着用这些特征进行训练，看看性能如何。此外，你还可以画出不同子集的一个精度图，根据绘制的图形来找出性能最好的一组特征。</p>
<p>这就是特征工程的子问题之一——特征选择，它的目的是<font color="red"><strong>从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果</strong></font>。</p>
<p>做特征选择的原因是因为这些特征对于目标类别的作用并不是相等的，一些无关的数据需要删掉。做特征选择的方法有多种，上面提到的这种特征子集选择的方法属于filter（刷选器）方法，它主要侧重于单个特征跟目标变量的相关性。优点是计算时间上较高效,对于过拟合问题也具有较高的鲁棒性。缺点就是倾向于选择冗余的特征,因为他们不考虑特征之间的相关性,有可能某一个特征的分类能力很差，但是它和某些其它特征组合起来会得到不错的效果。另外做特征子集选取的方法还有wrapper（封装器）和Embeded(集成方法)。wrapper方法实质上是一个分类器，封装器用选取的特征子集对样本集进行分类，分类的精度作为衡量特征子集好坏的标准,经过比较选出最好的特征子集。常用的有逐步回归（Stepwise regression）、向前选择（Forward selection）和向后选择（Backward selection）。它的优点是考虑了特征与特征之间的关联性，缺点是：当观测数据较少时容易过拟合，而当特征数量较多时,计算时间又会增长。对于Embeded集成方法，它是学习器自身自主选择特征，如使用Regularization做特征选择，或者使用决策树思想，细节这里就不做介绍了。这里还提一下，在做实验的时候，我们有时候会用Random Forest和Gradient boosting做特征选择，本质上都是基于决策树来做的特征选择，只是细节上有些区别。</p>
<p>综上所述，特征选择过程一般包括产生过程，评价函数，停止准则，验证过程，这4个部分。如下图所示：</p>
<center><br><img src="/assets/images/feature selection.png" alt="feature selection"><br></center>


<p>(1) <strong>产生过程( Generation Procedure )</strong>：产生过程是搜索特征子集的过程，负责为评价函数提供特征子集。搜索特征子集的过程有多种，将在2.2小节展开介绍。<br>(2) <strong>评价函数( Evaluation Function )</strong>：评价函数是评价一个特征子集好坏程度的一个准则。评价函数将在2.3小节展开介绍。<br>(3) <strong>停止准则( Stopping Criterion )</strong>：停止准则是与评价函数相关的，一般是一个阈值，当评价函数值达到这个阈值后就可停止搜索。<br>(4) <strong>验证过程( Validation Procedure )</strong> ：在验证数据集上验证选出来的特征子集的有效性。</p>
<h3 id="3-2_特征提取"><strong>3.2 特征提取</strong></h3><p>特征提取的子问题之二——特征提取。</p>
<p>原则上来讲，特征提取应该在特征选择之前。特征提取的对象是原始数据（raw data），它的目的是<font color="red"><strong>自动地构建新的特征，将原始特征转换为一组具有明显物理意义（Gabor、几何特征[角点、不变量]、纹理[LBP HOG]）或者统计意义或核的特征</strong></font>。比如通过变换特征取值来减少原始数据中某个特征的取值个数等。对于表格数据，你可以在你设计的特征矩阵上使用主要成分分析（Principal Component Analysis，PCA)来进行特征提取从而创建新的特征。对于图像数据，可能还包括了线或边缘检测。</p>
<p>常用的方法有：</p>
<ul>
<li>PCA (Principal component analysis，主成分分析)</li>
<li>ICA (Independent component analysis，独立成分分析)</li>
<li>LDA （Linear Discriminant Analysis，线性判别分析）</li>
</ul>
<p>对于图像识别中，还有SIFT方法。</p>
<h3 id="3-3_特征构建_Feature_Construction"><strong>3.3 特征构建 Feature Construction</strong></h3><p>特征提取的子问题之二——特征构建。</p>
<p>在上面的特征选择部分，我们提到了对特征重要性进行排名。那么，这些特征是如何得到的呢？在实际应用中，显然是不可能凭空而来的，需要我们手工去构建特征。关于特征构建的定义，可以这么说：<font color="green"><strong>特征构建指的是从原始数据中人工的构建新的特征</strong></font>。我们需要人工的创建它们。这需要我们花大量的时间去研究真实的数据样本，思考问题的潜在形式和数据结构，同时能够更好地应用到预测模型中。</p>
<p>特征构建需要很强的洞察力和分析能力，要求我们能够从原始数据中找出一些具有物理意义的特征。假设原始数据是表格数据，一般你可以使用混合属性或者组合属性来创建新的特征，或是分解或切分原有的特征来创建新的特征。</p>
<hr>
<h2 id="4、特征工程处理过程"><strong>4、特征工程处理过程</strong></h2><p>那么问题来了，特征工程具体是在哪个步骤做呢？</p>
<p>具体的机器学习过程是这样的一个过程：</p>
<ul>
<li>1.（Task before here）</li>
<li>2.选择数据(Select Data): 整合数据，将数据规范化成一个数据集，收集起来.</li>
<li>3.数据预处理（Preprocess Data）: 数据格式化，数据清理，采样等.</li>
<li>4.数据转换（Transform Data）: <font color="red"><strong>这个阶段做特征工程</strong></font>.</li>
<li>5.数据建模（Model Data）: 建立模型，评估模型并逐步优化.</li>
<li>(Tasks after here…)</li>
</ul>
<p>我们发现，特征工程和数据转换其实是等价的。<font color="red"><strong>事实上，特征工程是一个迭代过程，我们需要不断的设计特征、选择特征、建立模型、评估模型，然后才能得到最终的model</strong></font>。下面是特征工程的一个迭代过程：</p>
<ul>
<li>1.头脑风暴式特征：意思就是进你可能的从原始数据中提取特征，暂时不考虑其重要性，对应于特征构建；</li>
<li>2.设计特征：根据你的问题，你可以使用自动地特征提取，或者是手工构造特征，或者两者混合使用；</li>
<li>3.选择特征：使用不同的特征重要性评分和特征选择方法进行特征选择；</li>
<li>4.评估模型：使用你选择的特征进行建模，同时使用未知的数据来评估你的模型精度。</li>
</ul>
<p>By the way, 在做feature selection的时候，会涉及到特征学习（Feature Learning），这里说下特征学习的概念，一般而言，特征学习（Feature Learning）是指学习输入特征和一个训练实例真是类别之间的关系。</p>
<p>下面举个例子来简单了解下特征工程的处理。</p>
<p>首先是来说下特征提取，假设你的数据里现在有一个颜色类别的属性，比如是“item_Color”,它的取值有三个，分别是：<em>red，blue，unknown</em>。从特征提取的角度来看，你可以将其转化成一个二值特征“<em>has_color</em>”，取值为1或0。其中1表示有颜色，0表示没颜色。你还可以将其转换成三个二值属性：<em>Is_Red, Is_Blue and Is_Unknown</em>。这样构建特征之后，你就可以使用简单的线性模型进行训练了。</p>
<p>另外再举一个例子，假设你有一个日期时间 (i.e. 2014-09-20T20:45:40Z)，这个该如何转换呢？</p>
<p>对于这种时间的数据，我们可以根据需求提取出多种属性。比如，如果你想知道某一天的时间段跟其它属性的关系，你可以创建一个数字特征“<strong>Hour_Of_Day</strong>”来帮你建立一个回归模型，或者你可以建立一个序数特征，“Part_Of_Day”,取值“<em>Morning,Midday,Afternoon,Night</em>”来关联你的数据。</p>
<p>此外，你还可以按星期或季度来构建属性，等等等等……</p>
<p>关于特征构建，主要是尽可能的从原始数据中构建特征，而特征选择，经过上面的分析，想必大家也知道了，其实就是达到一个降维的效果。</p>
<p>只要分析能力和实践能力够强，那么特征构建和特征提取对你而言就会显得相对比较简单，所以抓紧时间好好实践吧！</p>
<hr>
<h2 id="Conclusion"><strong>Conclusion</strong></h2><p>恩。说了这么多，大家可能对特征工程、特征选择、特征提取和特征构建有点混乱了，下面来简单的做个总结：</p>
<p>首先来说说这几个术语：</p>
<ul>
<li>特征工程：利用数据领域的相关知识来创建能够使机器学习算法达到最佳性能的特征的过程。</li>
<li>特征构建：是原始数据中人工的构建新的特征。</li>
<li>特征提取：自动地构建新的特征，将原始特征转换为一组具有明显物理意义或者统计意义或核的特征。</li>
<li>特征选择：从特征集合中挑选一组最具统计意义的特征子集，从而达到降维的效果</li>
</ul>
<p>了解这几个术语的意思后，我们来看看他们之间的关系。</p>
<p>在Quora中有人这么说：</p>
<p>Feature engineering is a super-set of  activities which include feature extraction, feature construction and feature selection. Each of the three are important steps and none should be ignored. We could make a generalization of the importance though, from my experience the relative importance of the steps would be feature construction &gt; feature extraction &gt; feature selection.</p>
<p>用中文来说就是：<font color="green"><strong>特征工程是一个超集，它包括特征提取、特征构建和特征选择这三个子模块。在实践当中，每一个子模块都非常重要，忽略不得。根据答主的经验，他将这三个子模块的重要性进行了一个排名，即：特征构建&gt;特征提取&gt;特征选择。</strong></font></p>
<p>事实上，真的是这样，<font color="red"><strong>如果特征构建做的不好，那么它会直接影响特征提取，进而影响了特征选择，最终影响模型的性能</strong></font>。</p>
<p>OK！关于特征工程就到此为止吧，如果有纰漏的地方，还望多多指导！作为一枚行走在ML界的程序员，就让我们快乐的建模，快乐的做特征工程吧^_^！Happy coding, happy modeling！</p>
<h2 id="References"><strong>References</strong></h2><ul>
<li><a href="https://www.quora.com/What-are-some-general-tips-on-feature-selection-and-engineering-that-every-data-scientist-should-know" target="_blank" rel="external">Neglected machine learning ideas</a></li>
<li><a href="http://blog.kaggle.com/2013/04/10/qa-with-xavier-conort/" target="_blank" rel="external">Q&amp;A with Xavier Conort</a></li>
<li><a href="https://www.quora.com/What-is-feature-engineering" target="_blank" rel="external">https://www.quora.com/What-is-feature-engineering</a></li>
<li><a href="https://en.wikipedia.org/wiki/Feature_engineering" target="_blank" rel="external">Feature_engineering-wikipedia</a></li>
<li><a href="http://machinelearningmastery.com/an-introduction-to-feature-selection/" target="_blank" rel="external">An Introduction to Feature Selection</a></li>
<li><a href="http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/" target="_blank" rel="external">Discover Feature Engineering, How to Engineer Features and How to Get Good at It</a></li>
<li><a href="https://www.quora.com/How-valuable-do-you-think-feature-selection-is-in-machine-learning-Which-do-you-think-improves-accuracy-more-feature-selection-or-feature-engineering" target="_blank" rel="external">How valuable do you think feature selection is in machine learning? Which do you think improves accuracy more, feature selection or feature engineering?</a></li>
</ul>
<hr>
<p><br></p>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a> 个人博客： <a href="http://csuldw.github.io" target="_black">D.W’s Diary</a><br><br></strong></center>



]]></content>
    <summary type="html">
    <![CDATA[<p>在这个振奋人心的程序员节日里，我决定认真地写一篇文章来纪念一下自己这长达六年程序员史。o(╯□╰)o</p>
<p>本文是一篇关于特征工程的总结类文章，如有不足之处或理解有偏差的地方，还望多多指教。</p>
<p>首先，给一张特征工程的思维导图吧：</p>
<p><img src="http://ww1.sinaimg.cn/large/637f3c58gw1exd7mcjk7yj28k33uwaoe.jpg" alt="特征工程"></p>]]>
    
    </summary>
    
      <category term="Feature Engineering" scheme="http://csuldw.github.io/tags/Feature-Engineering/"/>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Windows下使用 git push 命令的无密码设置]]></title>
    <link href="http://csuldw.github.io/2015/10/21/2015-10-21%20Windows%20git%20push%20no%20password/"/>
    <id>http://csuldw.github.io/2015/10/21/2015-10-21 Windows git push no password/</id>
    <published>2015-10-21T08:45:44.000Z</published>
    <updated>2015-10-24T00:27:19.831Z</updated>
    <content type="html"><![CDATA[<p>在使用git时，每次进行git push时都需要输入用户名和密码，简直让人抓狂呀。下面介绍一种方法，可以避免用户名和密码输入，节省大量时间。</p>
<h2 id="1-添加环境变量">1.添加环境变量</h2><p>首先在系统变量中添加一个环境变量HOME，内容为</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HOME<span class="preprocessor">%</span>USERPROFILE<span class="preprocessor">%</span></span><br></pre></td></tr></table></figure>
<center><br><img src="http://ww4.sinaimg.cn/large/637f3c58gw1exbx3roqvcj20bo0cadgy.jpg" alt="配置环境变量"><br></center>

<a id="more"></a>
<h2 id="2-新建配置文件">2.新建配置文件</h2><p>由于使用的是Windows，所以进入%HOME%目录（如我的:C:\Users\username），新建一个名为”_netrc”的文件，文件中内容格式如下：</p>
<figure class="highlight puppet"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">machine github.com</span><br><span class="line">login your-username</span><br><span class="line"><span class="literal">password</span> your-<span class="literal">password</span></span><br></pre></td></tr></table></figure>
<p>接着，打开git bash后，输入git push 命令就无需再输入用户名和密码了。</p>
<p>爽歪歪啦~</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>在使用git时，每次进行git push时都需要输入用户名和密码，简直让人抓狂呀。下面介绍一种方法，可以避免用户名和密码输入，节省大量时间。</p>
<h2 id="1-添加环境变量">1.添加环境变量</h2><p>首先在系统变量中添加一个环境变量HOME，内容为</p>
<figure class="highlight gcode"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">HOME<span class="preprocessor">%</span>USERPROFILE<span class="preprocessor">%</span></span><br></pre></td></tr></table></figure>
<center><br><img src="http://ww4.sinaimg.cn/large/637f3c58gw1exbx3roqvcj20bo0cadgy.jpg" alt="配置环境变量"><br></center>]]>
    
    </summary>
    
      <category term="GitHub" scheme="http://csuldw.github.io/tags/GitHub/"/>
    
      <category term="GitHub" scheme="http://csuldw.github.io/categories/GitHub/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习-个人资料整理]]></title>
    <link href="http://csuldw.github.io/2015/09/23/2015-09-23%20Machine%20learning%20materials/"/>
    <id>http://csuldw.github.io/2015/09/23/2015-09-23 Machine learning materials/</id>
    <published>2015-09-23T14:22:22.000Z</published>
    <updated>2015-11-01T00:52:36.871Z</updated>
    <content type="html"><![CDATA[<p>学习Machine Learning也有很长一段时间了，前段时间在paper中应用了GTB（Gradient Tree Boosting）算法。在我的数据集上GTB的performance比Random Forest要稍微强一点，整个experiment做完之后，有许多东西都来不及及时整理，很多都遗忘了。打算接下来的时间里，好好整理下自己的学习资料，这份资料绝对不是一时半会就整理得完的，先开个头吧，以后会间断性更新该blog的。</p>
<p>下面来做个资料整理吧。</p>
<a id="more"></a>
<h2 id="书籍推荐"><strong>书籍推荐</strong></h2><p>机器学习的书籍很多，下面推荐几本本人用过而且觉得还不错的书籍。优于机器学习是一门跨领域的学科，所以在书籍上并非全是机器学习的书籍:</p>
<ul>
<li>1.《机器学习实战》<strong>Machine Learning in Action [美] Peter Harington 著</strong>。该书贯穿了10个最受欢迎的机器学习算法，提供了案例研究问题并用Python代码实例来解决。我本人比较喜欢这本书，因为里面的代码给了我很大的帮助，自己在学习机器学习算法的时候，理论上很多东西不太理解透，通过该书实践之后，在算法层面又有了进一步的提高。</li>
<li>2.《统计学习方法》 李航著。该书比较详细地介绍了算法的原理，只从理论层面来研究算法。通过这本书和《机器学习实战》两本书相结合，一本讲理论，一本着手实践，加在一起会有事半功倍的效果。</li>
<li>3.《数据挖掘概念与技术》 韩家炜著。该书介绍了数据挖掘的常用技术，比较详实，但本人觉得不太适合初学者，当时自己初学的时候看的就是这本书，结果最后很多地方理解的不是很好，后来通过《统计学习方法》和算法实践之后，再回头看《数据挖掘概念与技术》，感觉就轻松多了。</li>
<li>4.《数学之美》 吴军著。本书可以当做业余书籍来看，可以在无聊的时候看看，不过里面讲的东西还是挺有用的。</li>
<li>5.《Python科学计算》该书可以当做Python编程参考书籍，但前提是你喜欢使用Python，并爱上了它，不然这本书还是蛮贵的，我自己也是通过“研究生自由探索项目”才买的这本书，因为可以报销嘛。</li>
</ul>
<h2 id="学习工具"><strong>学习工具</strong></h2><p>机器学习的tools很多，这里只列出几个参考工具。</p>
<ul>
<li><a href="http://scikit-learn.org/stable/user_guide.html" target="_blank" rel="external">Scikit-learn</a>.基于Python语言的<a href="http://scikit-learn.org/stable/user_guide.html" target="_blank" rel="external">scikit-learn</a>库，里面涵盖了分类、聚类、回归的大部分算法，并且有常用的评估指标以及预处理数据的方法，是一个不错的学习库，强力推荐。附一篇博文：<a href="http://www.erogol.com/broad-view-machine-learning-libraries/" target="_blank" rel="external">SOME USEFUL MACHINE LEARNING LIBRARIES</a>.</li>
<li><a href="http://www.r-project.org/" target="_blank" rel="external">R</a>语言，语言就是一门工具，R语言现在在商业界是用的最多的，在统计方面功能强大，而且也有封装好的算法库可以直接使用。附：<a href="https://cran.r-project.org/doc/contrib/Liu-R-refcard.pdf" target="_blank" rel="external">R语言参考卡片</a>.</li>
<li><a href="http://www.cs.waikato.ac.nz/ml/weka/" target="_blank" rel="external">Weka</a>，是一个基于java开发的数据挖掘工具，可以尝试一下。它为用户提供了一系列据挖掘API、命令行和图形化用户接口。你可以准备数据、可视化、建立分类、进行回归分析、建立聚类模型，同时可以通过第三方插件执行其他算法。除了WEKA之外， <a href="http://mahout.apache.org/" target="_blank" rel="external">Mahout</a>是Hadoop中为机器学习提供的一个很好的JAVA框架，你可以自行学习。如果你是机器学习和大数据学习的新手，那么坚持学习WEKA，并且全心全意地学习一个库。</li>
<li>Matlab，里面有很多的工具包，不过本人不怎么用过。参考：<a href="http://www.cad.zju.edu.cn/home/dengcai/Data/data.html" target="_blank" rel="external">Matlab Codes and Datasets for Feature Learning</a>和<a href="http://cn.mathworks.com/products/statistics/" target="_blank" rel="external">Statistics and Machine Learning Toolbox</a>。此外matlab中的<a href="http://www.gnu.org/software/octave/" target="_blank" rel="external">Octave</a>可以很方便地解决线性和非线性问题，比如机器学习算法底层涉及的问题。如果你有工程背景，那么你可以由此入手。</li>
<li><a href="https://bigml.com/" target="_blank" rel="external">BigML</a>:可能你并不想进行编程工作。你完全可以不通过代码，来使用 WEKA那样的工具。你通过使用BigMLS的服务来进行更加深入的工作。BigML通过Web页面，提供了机器学习的接口，因此你可以通过浏览器来建立模型。</li>
<li>如果你使用Python，这里推荐一个IDE，<a href="http://sourceforge.net/projects/winpython/files/WinPython_2.7/2.7.10.1/" target="_blank" rel="external">WinPython</a>,IDE版本就是Python的版本，自行选择！</li>
</ul>
<p>下面给出一个比较图，具体想要学什么，还需自己抉择。</p>
<center><br><img src="http://img.blog.csdn.net/20150918075645450" alt="这里写图片描述"><br></center>


<h2 id="学习视频"><strong>学习视频</strong></h2><p>由于本人比较崇拜Andrew Ng，所以关于视频，首先推荐的便是Andrew Ng的斯坦福大学的机器学习课程。这套视频在网上有两个网址，国外和国内的都有，全程英语教学，内容很好，有时间建议你去听听：</p>
<ul>
<li>一个是国外的Coursera公开课，该课程在机器学习领域很火，是很多入门学者的首选。地址：<a href="https://www.coursera.org/；讲义地址：[Stanford" target="_blank" rel="external">https://www.coursera.org/；讲义地址：[Stanford</a> CS229 course下载讲义和笔记](<a href="http://cs229.stanford.edu/)；" target="_blank" rel="external">http://cs229.stanford.edu/)；</a></li>
<li>一个是国内的网易公开课，链接地址：<a href="http://open.163.com/movie/2008/1/U/O/M6SGF6VB4_M6SGJURUO.html" target="_blank" rel="external">http://open.163.com/movie/2008/1/U/O/M6SGF6VB4_M6SGJURUO.html</a></li>
</ul>
<p>下面是一个机器学习视频库，由加州理工学院（Caltech）出品。</p>
<ul>
<li>机器学习视频库，地址：<a href="http://work.caltech.edu/library/" target="_blank" rel="external">http://work.caltech.edu/library/</a></li>
</ul>
<p>其它的视频库</p>
<ul>
<li><a href="http://videolectures.net/Top/Computer_Science/Machine_Learning/" target="_blank" rel="external">Machine Learning Category on VideoLectures</a>，这个网站的视频比较多。你可以找出比较感兴趣的资源，然后深入学习。</li>
</ul>
<font color="#008B00">机器学习最近在国内比较火，许多培训机构都相应的开了该门课程，如果想要听中文教程的，可以去网上搜索下，这里就不给培训机构打广告了。</font>

<h2 id="博客和文章推荐"><strong>博客和文章推荐</strong></h2><p>大牛们的博客，会让你感到兴奋，让你觉得你不是一个人在奋斗，让你时刻记住你的前方已经有很多的学者正在等着你，你要加油。他们的经验会让我们少走些冤枉路，能让我们在他们的基础上进一步理解。下面推荐几个我所知道的或者说我了解到的几位牛人博客和几篇文章：</p>
<ul>
<li><strong>pluskid</strong>，真名张弛原，一位技术大牛，毕业于浙江大学，后来出国深造。他的博文质量非常高，深入浅出，其SVM三层境界的讲解让人茅塞顿开，应该给了很多人启发吧，很值得学习。现在的博客网址：<a href="http://pluskid.org/about.html" target="_blank" rel="external">Chiyuan Zhang</a>，原博客网址：<a href="http://blog.pluskid.org/" target="_blank" rel="external">Chiyuan Zhang</a></li>
<li><strong>Rachel Zhang</strong>，真名张睿卿，很有气质的一位软妹纸，目前是百度深度学习实验室研发工程师，在CSDN中的博客人气绝对屈指可数，算是IT界的一位女中豪杰。博客网址：<a href="http://blog.csdn.net/abcjennifer" target="_blank" rel="external">CSDN博客-Rachel Zhang</a></li>
<li><strong>July</strong>，对算法研究独具一格，目前是七月在线科技创始人兼CEO。博客网址：<a href="http://blog.csdn.net/v_JULY_v" target="_blank" rel="external">July</a></li>
<li><strong>Jason</strong>，一位国外机器学习爱好者，其博客内容详实，多篇文章被国内机器学习者翻译。博客网址：<a href="http://machinelearningmastery.com/blog/" target="_blank" rel="external">http://machinelearningmastery.com/blog/</a></li>
<li>一个国外很好的机器学习博客，里面介绍了详细的算法知识，很全面，从感知机、神经网络、决策树、SVM、Adaboost到随机森林、Deep Learning.网址：<a href="http://www.erogol.com/machine-learning/" target="_blank" rel="external">A Blog From a Human-engineer-being</a></li>
<li>一篇涵盖许多机器学习资料的文章：<a href="http://www.open-open.com/lib/view/open1428112201271.html" target="_blank" rel="external">机器学习(Machine Learning)&amp;深度学习(Deep Learning)资料</a></li>
<li><strong>Edwin Chen</strong>    ，机器学习爱好者，博客内容涵盖数学、机器学习和数据科学。分享其中一篇博文：<a href="http://blog.echen.me/2011/04/27/choosing-a-machine-learning-classifier/" target="_blank" rel="external">Choosing a Machine Learning Classifier</a>    </li>
<li>一篇以前的博文：<a href="http://conductrics.com/data-science-resources/" target="_blank" rel="external">A List of Data Science and Machine Learning Resources</a>，有时间好好阅读阅读，对你绝对有帮助。</li>
<li><a href="http://homes.cs.washington.edu/~pedrod/papers/cacm12.pdf" target="_blank" rel="external">A Few Useful Things to Know about Machine Learning</a>,一篇很有帮助的机器学习文章，里面包括了特征选择与模型的简化。</li>
<li><a href="http://www.cs.cmu.edu/~tom/pubs/MachineLearning.pdf" target="_blank" rel="external">The Discipline of Machine Learning</a>机器学习规则。该文章比较老，2006年发布的，作者是Tom Mitchell，但很有参考价值，其中定义了机器学习的规则。Mitchell在说服CMU总裁为一个百年内都存在的问题建立一个独立的机器学习部门时，也用到了这本书中的观点。希望能对你也有所帮助。</li>
<li>分享一个网站：<a href="http://www.jianshu.com/" target="_blank" rel="external">简书</a>。</li>
</ul>
<h2 id="国外网站"><strong>国外网站</strong></h2><p>如果你想搜索比较新颖的机器学习资料或是文章，可以到以下网站中搜索，里面不仅包括了机器学习的内容，还有许多其它相关领域内容，如数据科学和云计算等。</p>
<ul>
<li>InfoWord：<a href="http://www.infoworld.com/reviews/" target="_blank" rel="external">http://www.infoworld.com/reviews/</a></li>
<li>Kdnuggets：<a href="http://www.kdnuggets.com" target="_blank" rel="external">http://www.kdnuggets.com</a></li>
<li>Datasciencecentral：<a href="http://www.datasciencecentral.com/" target="_blank" rel="external">http://www.datasciencecentral.com/</a></li>
<li>Datascienceplus：<a href="http://datascienceplus.com" target="_blank" rel="external">http://datascienceplus.com</a></li>
</ul>
<h2 id="数据科学竞赛"><strong>数据科学竞赛</strong></h2><p>关于数据分析的竞赛，国内国外都有，下面推荐几个比较火的竞赛网站 ：</p>
<ul>
<li>Kaggle比赛，网址：<a href="https://www.kaggle.com/" target="_blank" rel="external">https://www.kaggle.com/</a></li>
<li>DataCastle比赛，网站：<a href="http://www.pkbigdata.com/" target="_blank" rel="external">http://www.pkbigdata.com/</a></li>
<li>阿里大数据竞赛，目前没有消息了，2015年有个【2015天池大数据竞赛】</li>
</ul>
<h2 id="ML相关算法参考"><strong>ML相关算法参考</strong></h2><ul>
<li>决策树-参考：<a href="http://blog.csdn.net/dream_angel_z/article/details/45965463" target="_blank" rel="external">decision Tree（Python实现）</a></li>
<li>SVM支持向量机-参考：<a href="http://blog.pluskid.org/?page_id=683" target="_blank" rel="external">pluskid支持向量机三重境界</a></li>
<li>Adaboost-参考：<a href="http://www.csuldw.com/2015/07/05/2015-07-12-Adaboost/" target="_blank" rel="external">组合算法-Adaboost</a></li>
<li>Random Forest-参考：<a href="http://www.cnblogs.com/wentingtu/archive/2011/12/22/2297405.html" target="_blank" rel="external">随机森林算法</a></li>
<li>朴素贝叶斯算法-参考：<a href="http://blog.csdn.net/dream_angel_z/article/details/46120867" target="_blank" rel="external">Naive Bayes算法实现</a></li>
<li>人工神经网络-参考：<a href="http://www.cnblogs.com/luxiaoxun/archive/2012/12/10/2811309.html" target="_blank" rel="external">http://www.cnblogs.com/luxiaoxun/archive/2012/12/10/2811309.html</a></li>
<li>Apriori算法-参考地址：<a href="http://www.csuldw.com/2015/06/04/2015-06-04-Apriori/" target="_blank" rel="external">Apriori关联分析</a></li>
<li>K最近邻算法-参考：<a href="http://blog.csdn.net/dream_angel_z/article/details/45896449" target="_blank" rel="external">KNN从原理到实现</a></li>
<li>梯度树提升GTB算法-参考：<a href="http://blog.csdn.net/dream_angel_z/article/details/48085889" target="_blank" rel="external">Gradient Tree Boosting（或GBRT）</a></li>
<li>K-means聚类-参考：<a href="http://blog.csdn.net/dream_angel_z/article/details/46343597" target="_blank" rel="external">K-means cluster</a></li>
<li>组合算法总结-参考：<a href="http://www.csuldw.com/2015/07/22/2015-07-22%20%20ensemble/" target="_blank" rel="external">Ensemble算法总结</a></li>
<li>EM期望最大算法-参考：<a href="http://blog.csdn.net/zouxy09/article/details/8537620" target="_blank" rel="external">EM算法</a></li>
<li>Logistic回归-参考：<a href="http://blog.csdn.net/wangran51/article/details/8892923" target="_blank" rel="external">逻辑回归</a></li>
<li>HMM隐马尔可夫模型，参考:<a href="http://blog.csdn.net/likelet/article/details/7056068" target="_blank" rel="external">HMM</a></li>
<li>条件随机场，参考：<a href="http://www.tanghuangwhu.com/archives/162" target="_blank" rel="external">CRF</a></li>
<li>随机森林和GBDT，参考：<a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/1976562.html" target="_blank" rel="external">决策树模型组合之随机森林与GBDT</a></li>
<li>特征选择和特征提取，参考：<a href="http://blog.csdn.net/lanbing510/article/details/40488787" target="_blank" rel="external">特征提取与特征选择</a></li>
<li>梯度下降法，参考:<a href="http://blog.csdn.net/woxincd/article/details/7040944" target="_blank" rel="external">gradient descent</a></li>
<li>牛顿法，参考：<a href="http://blog.csdn.net/luoleicn/article/details/6527049" target="_blank" rel="external">牛顿法</a></li>
<li>线性判别分析，参考：<a href="http://www.cnblogs.com/jerrylead/archive/2011/04/21/2024384.html" target="_blank" rel="external">线性判别</a></li>
<li>深度学习-<a href="http://www.cnblogs.com/xiaowanyer/p/3701944.html" target="_blank" rel="external">深度学习概述：从感知机到深度网络</a></li>
</ul>
<h2 id="个人译文"><strong>个人译文</strong></h2><p>下面是本人在CSDN云计算栏目发布的翻译文章，如有翻译不准确的地方，还望多多包涵，希望能给大家带来点帮助，译文列表如下：</p>
<ul>
<li>2015-09-14 <a href="http://www.csdn.net/article/2015-09-14/2825693" target="_blank" rel="external">LSTM实现详解</a></li>
<li>2015-09-10 <a href="http://www.csdn.net/article/2015-09-08/2825646" target="_blank" rel="external">从零实现来理解机器学习算法：书籍推荐及障碍的克服</a></li>
<li>2015-08-31  <a href="http://www.csdn.net/article/2015-08-27/2825551" target="_blank" rel="external">机器学习开发者的现代化路径：不需要从统计学微积分开始</a></li>
<li>2015-08-27 <a href="http://www.csdn.net/article/2015-08-27/2825549" target="_blank" rel="external">基于Python的卷积神经网络和特征提取</a></li>
<li>2015-08-20 <a href="http://www.csdn.net/article/2015-08-19/2825492" target="_blank" rel="external">你应该掌握的七种回归技术</a></li>
<li>2015-08-11 <a href="http://www.csdn.net/article/2015-08-10/2825430" target="_blank" rel="external">机器学习API Top 10：AT&amp;T Speech、IBM Watson和Google Prediction</a></li>
<li>2015-08-03 <a href="http://www.csdn.net/article/2015-08-01/2825362" target="_blank" rel="external">从Theano到Lasagne：基于Python的深度学习的框架和库</a></li>
<li>2015-07-15 <a href="http://www.csdn.net/article/2015-07-13/2825188" target="_blank" rel="external">Airbnb欺诈预测机器学习模型设计：准确率和召回率的故事</a></li>
<li>2015-07-13 <a href="http://www.csdn.net/article/2015-07-13/2825187" target="_blank" rel="external">开发者成功使用机器学习的十大诀窍</a></li>
</ul>
<p>下面是相关译者的译文，仅供参考：</p>
<ul>
<li>2015-09-16 <a href="http://www.csdn.net/article/2015-09-15/2825714" target="_blank" rel="external">各种编程语言的深度学习库整理</a></li>
<li>2015-09-11 <a href="http://www.csdn.net/article/2015-09-08/2825647" target="_blank" rel="external">机器学习温和指南</a></li>
<li>2015-09-10 <a href="http://www.csdn.net/article/2015-09-10/2825668" target="_blank" rel="external">关于数据科学，书上不曾提及的三点经验</a></li>
</ul>
<hr>
<font color="#CD3333">从这些牛人的博客中，你能学到很多。慢慢地你会体会到，不是你一个人在战斗，还有很多人，所以你不用害怕孤独。</font>

<p>最后，关于机器学习资料的整理，先到此为止吧，如果你有什么好的资料，欢迎在评论中给出推荐或网址链接。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>学习Machine Learning也有很长一段时间了，前段时间在paper中应用了GTB（Gradient Tree Boosting）算法。在我的数据集上GTB的performance比Random Forest要稍微强一点，整个experiment做完之后，有许多东西都来不及及时整理，很多都遗忘了。打算接下来的时间里，好好整理下自己的学习资料，这份资料绝对不是一时半会就整理得完的，先开个头吧，以后会间断性更新该blog的。</p>
<p>下面来做个资料整理吧。</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[2015年8月总结]]></title>
    <link href="http://csuldw.github.io/2015/08/29/2015-08-29%20Summary/"/>
    <id>http://csuldw.github.io/2015/08/29/2015-08-29 Summary/</id>
    <published>2015-08-29T02:54:00.000Z</published>
    <updated>2015-10-22T01:44:54.356Z</updated>
    <content type="html"><![CDATA[<p>很多时候，都想写点什么，可又不知道从何开始，总会有一种语尽词穷的感觉，道不出那种振奋人心的话。“书到用时方恨少”大概就是这个意思吧。有时间，再回头学学语文，看看美文，涨涨知识吧。</p>
<p>下面来总结一下自已这几个月的学习成长吧。<br><a id="more"></a><br>以前总认为别人了不起，总感觉别人的知识面怎么会如此的宽泛，然后敬仰之心便犹如滔滔江水，连绵不绝。每次都会想着以后我也要好好充实自己的业余爱好，为生活增添一份乐趣，可事后往往又不了了之了。其实，学东西就好比爬山，起初精力充沛，步伐之神速令人刮目，很快就到了半山。可过了一阵子之后，精力值会达到了一个饱和点（或者说大脑的知识储藏量有限），会让你感觉怎么奋进都进步缓慢，看不到成功的灯塔，但当你再努力向前，到达山顶的时候，回首经历的一切，你会感觉，曾经的一切确实不过如此，一种“一览众山小”的感受油然而生，而这种感觉会让你心情舒畅，神清气爽，整个人都好了。</p>
<p>从某种角度上说，我应该算是个“技术宅”吧。能够连续待在实验室拼个好几个月，这大概算是我最自豪的事情了，没有之一。</p>
<p>从今年四月中旬开始，看paper，学习ML（DM）算法，然后跑实验，到现在大概整整四个月时间吧。能够写出一篇论文，确实不容易。期间也遇到了许多问题，比如自己做的数据没别人的好，数据正负样本不均衡、实验效果不理想等等，不过从中确实学到了很多的东西。现在，自己独立跑实验基本没有多大问题，至少自己能用Python实现常见的算法以及对文件的一些操作，唯一感觉会出问题的当属数据集这块。因为自己构造数据集，有些奇异点处理的不好，然后直接影响了最后模型的好坏。</p>
<p>五月上旬基本上玩过去了，五月下旬各类机器学习书籍接连到手，然后开始学习《机器学习实践》这本书，全书是用Python实现的代码，因为有源码参考，所以很容易理解，很适合算法学习入门。由于之前学习过数据挖掘算法的理论知识，所以这本书不到半个月就把相应的知识点理解的差不多了。关于算法理论，清华大学出版社李航的《统计学习方法》确实不错，全书纯理论知识，分析独特，通俗易懂，很全面，适合想深入研究算法的人。这本书我还没完全看完，接下来可以好好学习下。</p>
<p>六月，上旬的时候，在Leetcode上面刷了一个星期的算法题，做了60几个题目，后来感觉还是做实验重要，然后就接着去做数据了。数据处理期间，碰到了一些专业性问题，尤其是在计算结合位点的时候，导师当时也不在，而我对于生物上的一些专业知识不是很了解，所以进度卡住了一个星期左右。还有就是在计算一个特征的时候，停留了好几天。不过还好，六月底，数据的每个特征算是做出来了，但当时喉咙开始发炎了，然后又引起感冒，整个日子都不好过，整个人也都不好了。</p>
<p>到了七月，开始整理自己的数据集，跑实验，然后从分析结果。这段时间碰到的问题也让人抓狂。首先是数据集不均衡，跑出来的效果很差，很不理想。不过这个问题还是比较容易解决，最常用的就是抽样了，下抽样还是上抽样就看结果好不好了。最后我选择的是下抽样，构造新的平衡数据，但是最后跑出来的效果不理想，使用别人的方法，跑出来的效果比别人差了将近8个百分点，这显然不合逻辑。到了七月下旬的时候，没办法了，还是使用别人的数据集吧。然后接着在别人数据集的基础上，加特征，构造新的数据集，然后使用不同算法来跑实验，比较效果等。最后得到的效果和别人的差不多，但是相对于另外一篇论文，似乎要差点。因为该论文没有说明自己的结果是在独立测试上的结果还是交叉验证得到的结果。所以，继续跑实验吧。</p>
<p>八月的第一个星期，实验总算做出来了，最后比较了下，效果提升不多，因为本来原来的实验就做的比较好了，AUC达到了0.923，确实难以超越。接下来就开始赶paper，没日没夜的赶呀，终于在8月中旬将paper赶出来了。对于APBC2016能不能中，30%的接受率，就看运气了，因为整篇论文的效果确实不是很明显。</p>
<p>总的来说，整个实验下来，学到了不少东西。以前是看中文论文容易，看英文paper难，现在感觉是看英文paper明显简单了，而写paper困难了。接下来好好努力，加油学英语、学算法、学理论、做实验吧，其他的事情都是浮云。</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>很多时候，都想写点什么，可又不知道从何开始，总会有一种语尽词穷的感觉，道不出那种振奋人心的话。“书到用时方恨少”大概就是这个意思吧。有时间，再回头学学语文，看看美文，涨涨知识吧。</p>
<p>下面来总结一下自已这几个月的学习成长吧。<br>]]>
    
    </summary>
    
      <category term="总结" scheme="http://csuldw.github.io/tags/%E6%80%BB%E7%BB%93/"/>
    
      <category term="总结" scheme="http://csuldw.github.io/categories/%E6%80%BB%E7%BB%93/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Gradient Tree Boosting]]></title>
    <link href="http://csuldw.github.io/2015/08/19/2015-08-19%20GBDT/"/>
    <id>http://csuldw.github.io/2015/08/19/2015-08-19 GBDT/</id>
    <published>2015-08-19T02:54:00.000Z</published>
    <updated>2015-10-22T01:34:04.812Z</updated>
    <content type="html"><![CDATA[<h2 id="Introduction"><strong>Introduction</strong></h2><p>决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型容易展示（容易将得到的决策树做成图片展示出来）等。但是同时，单决策树又有一些不好的地方，比如说容易over-fitting，虽然有一些方法，如剪枝可以减少这种情况，但是还是不太理想。</p>
<p>模型组合（比如说有Boosting，Bagging等）与决策树相关的算法比较多，如randomForest、Adaboost、GBRT等，这些算法最终的结果是生成N(可能会有几百棵以上）棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠赛过一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单（相对于C4.5这种单决策树而言），但是他们组合起来确是很强大。虽然这些算法都是通过决策树演变过来的，但在处理的过程上有着一些差异，我会在后面对此做一个本质上的比较。下面先来介绍下本文的梯度提升算法。<br><a id="more"></a></p>
<h2 id="Gradient_Tree_Boosting"><strong>Gradient Tree Boosting</strong></h2><p>梯度树提升（Gradient Tree Boosting）是一种组合算法，也叫做梯度提升回归树（gradient boosting regression tree），它的基分类器是决策树，既可以用来回归，也可以用作分类。在分类性能上，能够和随机森林媲美，甚至在有的数据集上表现的有过之而无不及。如今，Gradient Tree Boosting模型已经广泛的运用在Web搜索排行榜以及生态学上。在阿里内部也用的比较多，所以值得我们去花点时间认真学习。</p>
<p>根据scikit-learn官网的介绍，GBRT的优势有：</p>
<ul>
<li>自然而然地处理混合类型的数据</li>
<li>预测能力强</li>
<li>在输出空间对于异常值的鲁棒性强（通过强大的损失函数）</li>
</ul>
<p>然而，GBRT也有劣势：</p>
<ul>
<li>可扩展性方面，由于提升的时序性，不能进行并行处理</li>
</ul>
<p>尽管如此，由于GTB的表现性能很好，所以它仍然受广大业界人士的青睐。下面来介绍下梯度提升树的算法原理。</p>
<h3 id="GTB算法"><strong>GTB算法</strong></h3><p>梯度提升（gradient boosting）算法最初是FreidMan在2000年提出来的，其核心就在于，每棵树是从先前所有树的残差中来学习。利用的是当前模型中损失函数的负梯度值</p>
<p>$$ r<em>{mi} = - \Bigg [ \frac {\partial L(y_i, f (x_i))}{\partial f (x_i)}\Bigg ] </em>{f (x) = f _{m-1}(x)}$$</p>
<p>作为提升树算法中的残差的近似值，进而拟合一棵回归（分类）树。</p>
<p>梯度提升属于Boost算法的一种，也可以说是Boost算法的一种改进，原始的Boost算法是在算法开始时，为每一个样本赋上一个相等的权重值，也就是说，最开始的时候，大家都是一样重要的。在每一次训练中得到的模型，会使得数据点的估计有所差异，所以在每一步结束后，我们需要对权重值进行处理，而处理的方式就是通过增加错分类点的权重，同时减少错分类点的权重，这样使得某些点如果老是被分错，那么就会被“严重关注”，也就被赋上一个很高的权重。然后等进行了N次迭代（由用户指定），将会得到N个简单的基分类器（basic learner），最后将它们组合起来，可以对它们进行加权（错误率越大的基分类器权重值越小，错误率越小的基分类器权重值越大）、或者让它们进行投票等得到一个最终的模型。</p>
<p>Gradient Boost与传统的Boost有着很大的区别，它的每一次计算都是为了减少上一次的残差(residual)，而为了减少这些残差，可以在残差减少的梯度(Gradient)方向上建立一个新模型。所以说，在Gradient Boost中，每个新模型的建立是为了使得先前模型残差往梯度方向减少，与传统的Boost算法对正确、错误的样本进行加权有着极大的区别。</p>
<h4 id="梯度提升算法（以回归为例）"><strong>梯度提升算法（以回归为例）</strong></h4><p>对于给定的输入：训练数据集T={(x1,y1),(x2,y2),…,(xn,yn)},损失函数L(y,f(x));<br>输出结果：一棵回归树$\tilde{f}(x)$</p>
<hr>
<p>（1）首先初始化$$f<em>0(x)=arg min_c \sum</em>{i=1}^{N}L(y_i, c)$$</p>
<p>估计一个使损失函数极小化的常数值，此时它只有一个节点的树；<br>（2）迭代的建立M棵提升树<br>for m=1 to M:（第一层循环）<br>for i=1 to N：（第二层循环） 计算损失函数的负梯度在当前模型的值，并将它作为残差的估计值。</p>
<p>$$ r<em>{mi} = - \Bigg [ \frac {\partial L(y_i, f (x_i))}{\partial f (x_i)}\Bigg ] </em>{f (x) = f _{m-1}(x)}$$</p>
<p>对于$r<em>{mi}$拟合一棵回归树，得到第m棵树的叶节点区域$R</em>{mj}$,j=1,2,…,J</p>
<p>for j=1 to J：（第二层循环）,计算：</p>
<p>$$c<em>{mj} = arg min_c \sum</em>{x<em>i\epsilon R</em>{mj}}L(y<em>i,f</em>{m-1}(x_i)+c)$$</p>
<p>利用线性搜索估计叶节点区域的值，使损失函数极小化；</p>
<p>然后，更新$f<em>{m}(x) = f</em>{m-1}(x) + \sum<em>{j=1}^Jc</em>{mj}I(x \epsilon R_{mj})$</p>
<p>（3）最后得到的$f_{m}(x)$就是我们最终的模型</p>
<p>$$\tilde{f}(x)=f<em>M(x)=\sum</em>{m=1}^M\sum<em>{j=1}^Jc</em>{mj}I(x \epsilon R_{mj})$$</p>
<hr>
<h4 id="使用scikit-learn中的GTB"><strong>使用scikit-learn中的GTB</strong></h4><p>在scikit-learn中对GTB算法有了很好的封装，对于分类可以选择的损失函数有逻辑回归和指数函数，对于回归的损失函数相对比较多，有最小二乘法、最小绝对偏差函数、huber以及分位数等。具体描述参考下面的图片：<br><img src="file:///C:/Users/liudiwei/Desktop/QQ截图20150829104337.png" alt=""></p>
<p>下面是sklearn中的一个分类原例：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn.datasets import make_hastie_10_2</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn.ensemble import <span class="constant">GradientBoostingClassifier</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X</span>, y = make_hastie_10_2(random_state=<span class="number">0</span>)</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X_train</span>, <span class="constant">X_test</span> = <span class="constant">X</span>[<span class="symbol">:</span><span class="number">2000</span>], <span class="constant">X</span>[<span class="number">2000</span><span class="symbol">:</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; y_train, y_test = y[<span class="symbol">:</span><span class="number">2000</span>], y[<span class="number">2000</span><span class="symbol">:</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = <span class="constant">GradientBoostingClassifier</span>(n_estimators=<span class="number">100</span>, learning_rate=<span class="number">1.0</span>,</span><br><span class="line">...     max_depth=<span class="number">1</span>, random_state=<span class="number">0</span>).fit(<span class="constant">X_train</span>, y_train)</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf.score(<span class="constant">X_test</span>, y_test)                 </span><br><span class="line"><span class="number">0</span>.<span class="number">913</span>...</span><br></pre></td></tr></table></figure>
<p>其中n_estimators表示弱分类器的个数，learning_rate表示学习率，max_depth表示最大的深度等。GTB的参数比较多，在实际应用中需要自己去调整合适的参数。</p>
<h2 id="基于决策树的组合算法比较"><strong>基于决策树的组合算法比较</strong></h2><p>基于决策树的组合算法常用的有三个，分别是Adaboost、RandomFrest以及本文的GBRT。</p>
<p>Adaboost是通过迭代的学习每一个基分类器，每次迭代中，把上一次错分类的数据权值增大，正确分类的数据权值减小，然后将基分类器的线性组合作为一个强分类器，同时给分类误差率较小的基本分类器以大的权值，给分类误差率较大的基分类器以小的权重值。Adaboost使用的是自适应的方法，其中概率分布式变化的，关注的是难分类的样本。详细内容请参考我之前的文章：<a href="http://blog.csdn.net/dream_angel_z/article/details/46764845" target="_blank" rel="external">机器学习算法-Adaboost</a>。</p>
<p>随机森林RandomForest算法，与adaboost有错区别，可以说一种改进的装袋组合算法。随机森林则(randomForest)，不仅对样本进行抽样，还对变量进行抽样。它通过随机的方式建立一个森林，森林里面有许多棵决策树，并且每一棵树之间是没有联系的。在得到森林之后，当有一个新的输入样本进来的时候，就让森林中的每一棵决策树分别对其进行判断，看这个样本应该属于哪一类（就分类算法而言），然后看看哪一类选择最多，就预测这个样本为该类。在建立每一棵决策树的过程中，有两点需要注意，即<strong>采样</strong>与<strong>完全分裂</strong>。首先是两个随机采样的过程，RF对输入的数据要进行行采样和列采样。对于行采样，是采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting过拟合。然后进行列采样，从M个feature特征中，选择m个(m &lt;&lt; M)。之后就是对采样之后的数据使用<strong>完全分裂</strong>的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个类别。一般很多的决策树算法都一个重要的步骤-<strong>剪枝</strong>，但是这里不这样干，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。按照这种算法得到的随机森林中的每一棵决策树都是非常弱的，但当它们组合在一起的时候，就相当厉害了。随机森林就好比是：每一棵决策树就是一个精通于某一领域的专家（因为我们从M个feature中选择m个让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。随机森林的分类准确率可以与adaboost媲美。它对噪声数据更加鲁棒，运行速度比adaboost也快得多。</p>
<p>对于梯度提升树，它的每一次计算都是为了减少上一次的残差(residual)，而为了减少这些残差，可以在残差减少的梯度(Gradient)方向上建立一个新模型。这与adaboost和随机森林有很大的区别。</p>
<h3 id="References">References</h3><p>[1] Introduction to Data Mining 数据挖掘概论. Pang-Ning Tan Michael Steinbach Vipin Kumar著<br>[2] 统计学习方法 李航 著<br>[3] scikit-learn官网组合算法 <a href="http://scikit-learn.org/stable/modules/ensemble.html#gradient-boosting" target="_blank" rel="external">点击这里</a></p>
<p>参考文章： <a href="http://www.cnblogs.com/LeftNotEasy/archive/2011/03/07/1976562.html" target="_blank" rel="external">随机森林与GBDT</a></p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="Introduction"><strong>Introduction</strong></h2><p>决策树这种算法有着很多良好的特性，比如说训练时间复杂度较低，预测的过程比较快速，模型容易展示（容易将得到的决策树做成图片展示出来）等。但是同时，单决策树又有一些不好的地方，比如说容易over-fitting，虽然有一些方法，如剪枝可以减少这种情况，但是还是不太理想。</p>
<p>模型组合（比如说有Boosting，Bagging等）与决策树相关的算法比较多，如randomForest、Adaboost、GBRT等，这些算法最终的结果是生成N(可能会有几百棵以上）棵树，这样可以大大的减少单决策树带来的毛病，有点类似于三个臭皮匠赛过一个诸葛亮的做法，虽然这几百棵决策树中的每一棵都很简单（相对于C4.5这种单决策树而言），但是他们组合起来确是很强大。虽然这些算法都是通过决策树演变过来的，但在处理的过程上有着一些差异，我会在后面对此做一个本质上的比较。下面先来介绍下本文的梯度提升算法。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习-CrossValidation交叉验证Python实现]]></title>
    <link href="http://csuldw.github.io/2015/07/28/2015-07-28%20crossvalidation/"/>
    <id>http://csuldw.github.io/2015/07/28/2015-07-28 crossvalidation/</id>
    <published>2015-07-28T07:40:00.000Z</published>
    <updated>2015-10-29T13:08:28.478Z</updated>
    <content type="html"><![CDATA[<p>版权声明：本文为原创文章，转载请注明来源。</p>
<h2 id="1-原理"><strong>1.原理</strong></h2><h3 id="1-1_概念"><strong>1.1 概念</strong></h3><p>交叉验证(Cross-validation)主要用于模型训练或建模应用中，如分类预测、PCR、PLS回归建模等。在给定的样本空间中，拿出大部分样本作为训练集来训练模型，剩余的小部分样本使用刚建立的模型进行预测，并求这小部分样本的预测误差或者预测精度，同时记录它们的加和平均值。这个过程迭代K次，即K折交叉。其中，把每个样本的预测误差平方加和，称为PRESS(predicted Error Sum of Squares)。<br><a id="more"></a></p>
<h3 id="1-2_目的"><strong>1.2 目的</strong></h3><p>用交叉验证的目的是为了得到可靠稳定的模型。在分类，建立PC 或PLS模型时，一个很重要的因素是取多少个主成分的问题。用cross validation校验每个主成分下的PRESS值，选择PRESS值小的主成分数。或PRESS值不再变小时的主成分数。</p>
<p>常用的精度测试方法主要是交叉验证，例如10折交叉验证(10-fold cross validation)，将数据集分成十份，轮流将其中9份做训练1份做验证，10次的结果的均值作为对算法精度的估计，一般还需要进行多次10折交叉验证求均值，例如：10次10折交叉验证，以求更精确一点。<br>交叉验证有时也称为交叉比对，如：10折交叉比对</p>
<h3 id="1-3_常见的交叉验证形式："><strong>1.3 常见的交叉验证形式</strong>：</h3><p><strong>Holdout 验证</strong></p>
<blockquote>
<p>方法：将原始数据随机分为两组,一组做为训练集,一组做为验证集,利用训练集训练分类器,然后利用验证集验证模型,记录最后的分类准确率为此Hold-OutMethod下分类器的性能指标.。Hold-OutMethod相对于K-fold Cross Validation 又称Double cross-validation ，或相对K-CV称 2-fold cross-validation(2-CV)</p>
<p>一般来说，Holdout 验证并非一种交叉验证，因为数据并没有交叉使用。 随机从最初的样本中选出部分，形成交叉验证数据，而剩余的就当做训练数据。 一般来说，少于原本样本三分之一的数据被选做验证数据。</p>
</blockquote>
<ul>
<li>优点：好处的处理简单,只需随机把原始数据分为两组即可</li>
<li>缺点：严格意义来说Hold-Out Method并不能算是CV,因为这种方法没有达到交叉的思想,由于是随机的将原始数据分组,所以最后验证集分类准确率的高低与原始数据的分组有很大的关系,所以这种方法得到的结果其实并不具有说服性.(主要原因是 训练集样本数太少，通常不足以代表母体样本的分布，导致 test 阶段辨识率容易出现明显落差。此外，2-CV 中一分为二的分子集方法的变异度大，往往无法达到「实验过程必须可以被复制」的要求。)</li>
</ul>
<p><strong>K-fold cross-validation</strong></p>
<blockquote>
<p>K折交叉验证，初始采样分割成K个子样本，一个单独的子样本被保留作为验证模型的数据，其他K-1个样本用来训练。交叉验证重复K次，每个子样本验证一次，平均K次的结果或者使用其它结合方式，最终得到一个单一估测。这个方法的优势在于，同时重复运用随机产生的子样本进行训练和验证，每次的结果验证一次，10折交叉验证是最常用的。</p>
</blockquote>
<ul>
<li>优点：K-CV可以有效的避免过学习以及欠学习状态的发生,最后得到的结果也比较具有说服性.  </li>
<li>缺点：K值选取上</li>
</ul>
<p><strong>留一验证</strong></p>
<blockquote>
<p>正如名称所建议， 留一验证（LOOCV）意指只使用原本样本中的一项来当做验证资料， 而剩余的则留下来当做训练资料。 这个步骤一直持续到每个样本都被当做一次验证资料。 事实上，这等同于 K-fold 交叉验证是一样的，其中K为原本样本个数。 在某些情况下是存在有效率的演算法，如使用kernel regression 和Tikhonov regularization。</p>
</blockquote>
<h2 id="2-深入"><strong>2.深入</strong></h2><p>使用交叉验证方法的目的主要有3个： </p>
<ul>
<li>（1）从有限的学习数据中获取尽可能多的有效信息； </li>
<li>（2）交叉验证从多个方向开始学习样本的，可以有效的避免陷入局部最小值； </li>
<li>（3）可以在一定程度上避免过拟合问题。</li>
</ul>
<p>采用交叉验证方法时需要将学习数据样本分为两部分：训练数据样本和验证数据样本。并且为了得到更好的学习效果，无论训练样本还是验证样本都要尽可能参与学习。一般选取10重交叉验证即可达到好的学习效果。下面在上述原则基础上设计算法，主要描述下算法步骤，如下所示。</p>
<h2 id="Algorithm">Algorithm  </h2><figure class="highlight matlab"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">Step1: 	将学习样本空间 C 分为大小相等的 K 份  </span><br><span class="line">Step2: 	<span class="keyword">for</span> <span class="built_in">i</span> = <span class="number">1</span> to K ：</span><br><span class="line">			取第<span class="built_in">i</span>份作为测试集</span><br><span class="line">			<span class="keyword">for</span> <span class="built_in">j</span> = <span class="number">1</span> to K:</span><br><span class="line">				<span class="keyword">if</span> <span class="built_in">i</span> != <span class="built_in">j</span>:</span><br><span class="line">					将第<span class="built_in">j</span>份加到训练集中，作为训练集的一部分</span><br><span class="line">				<span class="keyword">end</span> <span class="keyword">if</span></span><br><span class="line">			<span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">		<span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">Step3: 	<span class="keyword">for</span> <span class="built_in">i</span> in (K-<span class="number">1</span>训练集)：</span><br><span class="line">			训练第<span class="built_in">i</span>个训练集，得到一个分类模型</span><br><span class="line">			使用该模型在第N个数据集上测试，计算并保存模型评估指标</span><br><span class="line">		<span class="keyword">end</span> <span class="keyword">for</span></span><br><span class="line">Step4: 	计算模型的平均性能</span><br><span class="line">Step5: 	用这K个模型在最终验证集的分类准确率平均值作为此K-CV下分类器的性能指标.</span><br></pre></td></tr></table></figure>
<h2 id="3-实现"><strong>3.实现</strong></h2><h3 id="3-1_scikit-learn交叉验证"><strong>3.1 scikit-learn交叉验证</strong></h3><p>在scikit-learn中有CrossValidation的实现代码，地址： <a href="http://scikit-learn.org/dev/modules/cross_validation.html#cross-validation" target="_blank" rel="external">scikit-learn官网crossvalidation文档</a></p>
<p>使用方法：</p>
<p>首先加载数据集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> cross_validation</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>iris = datasets.load_iris()</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>iris.data.shape, iris.target.shape</span><br><span class="line">((<span class="number">150</span>, <span class="number">4</span>), (<span class="number">150</span>,))</span><br></pre></td></tr></table></figure>
<p>通过上面代码，数据集特征和类标签分别为iris.data, iris.target，接着进行交叉验证</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; X_train, X_test, y_train, y_test = cross_validation.train_test_split(</span><br><span class="line">...     iris<span class="class">.data</span>, iris<span class="class">.target</span>, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt; X_train<span class="class">.shape</span>, y_train<span class="class">.shape</span></span><br><span class="line">((<span class="number">90</span>, <span class="number">4</span>), (<span class="number">90</span>,))</span><br><span class="line">&gt;&gt;&gt; X_test<span class="class">.shape</span>, y_test<span class="class">.shape</span></span><br><span class="line">((<span class="number">60</span>, <span class="number">4</span>), (<span class="number">60</span>,))</span><br><span class="line">&gt;&gt;&gt; clf = svm.<span class="function"><span class="title">SVC</span><span class="params">(kernel=<span class="string">'linear'</span>, C=<span class="number">1</span>)</span></span>.<span class="function"><span class="title">fit</span><span class="params">(X_train, y_train)</span></span></span><br><span class="line">&gt;&gt;&gt; clf.<span class="function"><span class="title">score</span><span class="params">(X_test, y_test)</span></span>                           </span><br><span class="line"><span class="number">0.96</span>...</span><br></pre></td></tr></table></figure>
<p>上面的clf是分类器，可以自己替换，比如我可以使用RandomForest</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf = <span class="function"><span class="title">RandomForestClassifier</span><span class="params">(n_estimators=<span class="number">400</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>一个比较有用的函数是train_test_split。功能是从样本中随机的按比例选取train data和test data。形式为</p>
<figure class="highlight cpp"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X<span class="keyword">_t</span>rain, X<span class="keyword">_t</span>est, <span class="keyword">y_t</span>rain, <span class="keyword">y_t</span>est = cross_validation.<span class="keyword">train_t</span>est_split(train_data,<span class="keyword">train_t</span>arget, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<p>test_size是样本占比。如果是整数的话就是样本的数量。random_state是随机数的种子。</p>
<p>当然，也可以换成别的，具体算法可以参考 <a href="http://scikit-learn.org/dev/supervised_learning.html#supervised-learning" target="_blank" rel="external">scikit-learn官方文档</a></p>
<hr>
<h3 id="3-2_抽样与CV结合"><strong>3.2 抽样与CV结合</strong></h3><blockquote>
<p>由于我跑的实验，数据是非均衡数据，不能直接套用，所以这里自己写了一个交叉验证的代码，仅供参考，如有问题，欢迎交流。</p>
</blockquote>
<p>首先有一个自适应的数据加载函数，主要用于加载本地文本数据，同时文本每行数据以”\t”隔开，最后一列为类标号，数据样例如下：</p>
<figure class="highlight dns"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">A</span><span class="number">1001	708</span>	K	-4	-3	6	2	-13	0	2	-4	-4	-10	-9	1</span><br><span class="line"><span class="keyword">A</span><span class="number">1002	709</span>	L	-4	-4	-1	-2	-11	-1	0	-12	-7	-5	-1	-1</span><br><span class="line"><span class="keyword">A</span><span class="number">1003	710</span>	G	0	-6	-2	-6	-8	-4	-6	-6	-9	-4	0	-1</span><br><span class="line"><span class="keyword">A</span><span class="number">1004	711</span>	R	0	0	1	-3	-10	-1	-3	-4	-6	-9	-6	1</span><br></pre></td></tr></table></figure>
<p><strong>说明</strong>：前面三个不是特征，所以在加载数据集的时候，特征部分起始位置修改了下，loadDataSet函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    fr = open(fileName)</span><br><span class="line">    dataMat = []; labelMat = []</span><br><span class="line">    <span class="keyword">for</span> eachline <span class="keyword">in</span> fr:</span><br><span class="line">        lineArr = []</span><br><span class="line">        curLine = eachline.strip().split(<span class="string">'\t'</span>) <span class="comment">#remove '\n'</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">3</span>, len(curLine)-<span class="number">1</span>):</span><br><span class="line">            lineArr.append(float(curLine[i])) <span class="comment">#get all feature from inpurfile</span></span><br><span class="line">        dataMat.append(lineArr)</span><br><span class="line">        labelMat.append(int(curLine[-<span class="number">1</span>])) <span class="comment">#last one is class lable</span></span><br><span class="line">    fr.close()</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat</span><br></pre></td></tr></table></figure>
<p>返回的dataMat为纯特征矩阵，labelMat为类别标号。</p>
<p>下面的<strong>splitDataSet</strong>用来切分数据集，如果是十折交叉，则split_size取10，filename为整个数据集文件，outdir则是切分的数据集的存放路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">splitDataSet</span><span class="params">(fileName, split_size,outdir)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(outdir): <span class="comment">#if not outdir,makrdir</span></span><br><span class="line">        os.makedirs(outdir)</span><br><span class="line">    fr = open(fileName,<span class="string">'r'</span>)<span class="comment">#open fileName to read</span></span><br><span class="line">    num_line = <span class="number">0</span></span><br><span class="line">    onefile = fr.readlines()</span><br><span class="line">    num_line = len(onefile)        </span><br><span class="line">    arr = np.arange(num_line) <span class="comment">#get a seq and set len=numLine</span></span><br><span class="line">    np.random.shuffle(arr) <span class="comment">#generate a random seq from arr</span></span><br><span class="line">    list_all = arr.tolist()</span><br><span class="line">    each_size = (num_line+<span class="number">1</span>) / split_size <span class="comment">#size of each split sets</span></span><br><span class="line">    split_all = []; each_split = []</span><br><span class="line">    count_num = <span class="number">0</span>; count_split = <span class="number">0</span>  <span class="comment">#count_num 统计每次遍历的当前个数</span></span><br><span class="line">                                    <span class="comment">#count_split 统计切分次数</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(list_all)): <span class="comment">#遍历整个数字序列</span></span><br><span class="line">        each_split.append(onefile[int(list_all[i])].strip()) </span><br><span class="line">        count_num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> count_num == each_size:</span><br><span class="line">            count_split += <span class="number">1</span> </span><br><span class="line">            array_ = np.array(each_split)</span><br><span class="line">            np.savetxt(outdir + <span class="string">"/split_"</span> + str(count_split) + <span class="string">'.txt'</span>,\</span><br><span class="line">                        array_,fmt=<span class="string">"%s"</span>, delimiter=<span class="string">'\t'</span>)  <span class="comment">#输出每一份数据</span></span><br><span class="line">            split_all.append(each_split) <span class="comment">#将每一份数据加入到一个list中</span></span><br><span class="line">            each_split = []</span><br><span class="line">            count_num = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> split_all</span><br></pre></td></tr></table></figure>
<p>underSample(datafile)方法为抽样函数，强正负样本比例固定为1:1，返回的是一个正负样本比例均等的数据集合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">underSample</span><span class="params">(datafile)</span>:</span> <span class="comment">#只针对一个数据集的下采样</span></span><br><span class="line">    dataMat,labelMat = loadDataSet(datafile) <span class="comment">#加载数据</span></span><br><span class="line">    pos_num = <span class="number">0</span>; pos_indexs = []; neg_indexs = []   </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labelMat)):<span class="comment">#统计正负样本的下标    </span></span><br><span class="line">        <span class="keyword">if</span> labelMat[i] == <span class="number">1</span>:</span><br><span class="line">            pos_num +=<span class="number">1</span></span><br><span class="line">            pos_indexs.append(i)</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        neg_indexs.append(i)</span><br><span class="line">    np.random.shuffle(neg_indexs)</span><br><span class="line">    neg_indexs = neg_indexs[<span class="number">0</span>:pos_num]</span><br><span class="line">    fr = open(datafile, <span class="string">'r'</span>)</span><br><span class="line">    onefile = fr.readlines()</span><br><span class="line">    outfile = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(pos_num):</span><br><span class="line">        pos_line = onefile[pos_indexs[i]]    </span><br><span class="line">        outfile.append(pos_line)</span><br><span class="line">        neg_line= onefile[neg_indexs[i]]      </span><br><span class="line">        outfile.append(neg_line)</span><br><span class="line">    <span class="keyword">return</span> outfile <span class="comment">#输出单个数据集采样结果</span></span><br></pre></td></tr></table></figure>
<p>下面的generateDataset(datadir,outdir)方法是从切分的数据集中留出一份作为测试集（无需抽样），对其余的进行抽样然后合并为一个作为训练集，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateDataset</span><span class="params">(datadir,outdir)</span>:</span> <span class="comment">#从切分的数据集中，对其中九份抽样汇成一个,\</span></span><br><span class="line">    <span class="comment">#剩余一个做为测试集,将最后的结果按照训练集和测试集输出到outdir中</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(outdir): <span class="comment">#if not outdir,makrdir</span></span><br><span class="line">        os.makedirs(outdir)</span><br><span class="line">    listfile = os.listdir(datadir)</span><br><span class="line">    train_all = []; test_all = [];cross_now = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> eachfile1 <span class="keyword">in</span> listfile:</span><br><span class="line">        train_sets = []; test_sets = []; </span><br><span class="line">        cross_now += <span class="number">1</span> <span class="comment">#记录当前的交叉次数</span></span><br><span class="line">        <span class="keyword">for</span> eachfile2 <span class="keyword">in</span> listfile:</span><br><span class="line">            <span class="keyword">if</span> eachfile2 != eachfile1:<span class="comment">#对其余九份欠抽样构成训练集</span></span><br><span class="line">                one_sample = underSample(datadir + <span class="string">'/'</span> + eachfile2)</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(len(one_sample)):</span><br><span class="line">                    train_sets.append(one_sample[i])</span><br><span class="line">        <span class="comment">#将训练集和测试集文件单独保存起来</span></span><br><span class="line">        <span class="keyword">with</span> open(outdir +<span class="string">"/test_"</span>+str(cross_now)+<span class="string">".datasets"</span>,<span class="string">'w'</span>) <span class="keyword">as</span> fw_test:</span><br><span class="line">            <span class="keyword">with</span> open(datadir + <span class="string">'/'</span> + eachfile1, <span class="string">'r'</span>) <span class="keyword">as</span> fr_testsets:</span><br><span class="line">                <span class="keyword">for</span> each_testline <span class="keyword">in</span> fr_testsets:                </span><br><span class="line">                    test_sets.append(each_testline) </span><br><span class="line">            <span class="keyword">for</span> oneline_test <span class="keyword">in</span> test_sets:</span><br><span class="line">                fw_test.write(oneline_test) <span class="comment">#输出测试集</span></span><br><span class="line">            test_all.append(test_sets)<span class="comment">#保存训练集</span></span><br><span class="line">        <span class="keyword">with</span> open(outdir+<span class="string">"/train_"</span>+str(cross_now)+<span class="string">".datasets"</span>,<span class="string">'w'</span>) <span class="keyword">as</span> fw_train:</span><br><span class="line">            <span class="keyword">for</span> oneline_train <span class="keyword">in</span> train_sets:   </span><br><span class="line">                oneline_train = oneline_train</span><br><span class="line">                fw_train.write(oneline_train)<span class="comment">#输出训练集</span></span><br><span class="line">            train_all.append(train_sets)<span class="comment">#保存训练集</span></span><br><span class="line">    <span class="keyword">return</span> train_all,test_all</span><br></pre></td></tr></table></figure>
<p>因为需要评估交叉验证，所以我写了一个performance方法根据真实类标签纸和预测值来计算SN和SP，当然如果需要其他的评估标准，继续添加即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">performance</span><span class="params">(labelArr, predictArr)</span>:</span><span class="comment">#类标签为int类型</span></span><br><span class="line">    <span class="comment">#labelArr[i] is actual value,predictArr[i] is predict value</span></span><br><span class="line">    TP = <span class="number">0.</span>; TN = <span class="number">0.</span>; FP = <span class="number">0.</span>; FN = <span class="number">0.</span>   </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(labelArr)):</span><br><span class="line">        <span class="keyword">if</span> labelArr[i] == <span class="number">1</span> <span class="keyword">and</span> predictArr[i] == <span class="number">1</span>:</span><br><span class="line">            TP += <span class="number">1.</span></span><br><span class="line">        <span class="keyword">if</span> labelArr[i] == <span class="number">1</span> <span class="keyword">and</span> predictArr[i] == -<span class="number">1</span>:</span><br><span class="line">            FN += <span class="number">1.</span></span><br><span class="line">        <span class="keyword">if</span> labelArr[i] == -<span class="number">1</span> <span class="keyword">and</span> predictArr[i] == <span class="number">1</span>:</span><br><span class="line">            FP += <span class="number">1.</span></span><br><span class="line">        <span class="keyword">if</span> labelArr[i] == -<span class="number">1</span> <span class="keyword">and</span> predictArr[i] == -<span class="number">1</span>:</span><br><span class="line">            TN += <span class="number">1.</span></span><br><span class="line">    SN = TP/(TP + FN) <span class="comment">#Sensitivity = TP/P  and P = TP + FN </span></span><br><span class="line">    SP = TN/(FP + TN) <span class="comment">#Specificity = TN/N  and N = TN + FP</span></span><br><span class="line">    <span class="comment">#MCC = (TP*TN-FP*FN)/math.sqrt((TP+FP)*(TP+FN)*(TN+FP)*(TN+FN))</span></span><br><span class="line">    <span class="keyword">return</span> SN,SP</span><br></pre></td></tr></table></figure>
<p> classifier(clf,train_X, train_y, test_X, test_y)方法是交叉验证中每次用的分类器训练过程以及测试过程，里面使用的分类器是scikit-learn自带的。该方法会将一些训练结果写入到文件中并保存到本地，同时在最后会返回ACC,SP,SN。</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">def classifier(clf,train_X, train_y, <span class="built_in">test</span>_X, <span class="built_in">test</span>_y):<span class="comment">#X:训练特征，y:训练标号</span></span><br><span class="line">    <span class="comment"># train with randomForest </span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">" training begin..."</span></span><br><span class="line">    clf = clf.fit(train_X,train_y)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">" training end."</span></span><br><span class="line">    <span class="comment">#==========================================================================</span></span><br><span class="line">    <span class="comment"># test randomForestClassifier with testsets</span></span><br><span class="line">    <span class="built_in">print</span> <span class="string">" test begin."</span></span><br><span class="line">    predict_ = clf.predict(<span class="built_in">test</span>_X) <span class="comment">#return type is float64</span></span><br><span class="line">    proba = clf.predict_proba(<span class="built_in">test</span>_X) <span class="comment">#return type is float64</span></span><br><span class="line">    score_ = clf.score(<span class="built_in">test</span>_X,<span class="built_in">test</span>_y)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">" test end."</span></span><br><span class="line">    <span class="comment">#==========================================================================</span></span><br><span class="line">    <span class="comment"># Modeal Evaluation</span></span><br><span class="line">    ACC = accuracy_score(<span class="built_in">test</span>_y, predict_)</span><br><span class="line">    SN,SP = performance(<span class="built_in">test</span>_y, predict_)</span><br><span class="line">    MCC = matthews_corrcoef(<span class="built_in">test</span>_y, predict_)</span><br><span class="line">    <span class="comment">#AUC = roc_auc_score(test_labelMat, proba)</span></span><br><span class="line">    <span class="comment">#==========================================================================</span></span><br><span class="line">    <span class="comment">#save output </span></span><br><span class="line">    <span class="built_in">eval</span>_output = []</span><br><span class="line">    <span class="built_in">eval</span>_output.append(ACC);<span class="built_in">eval</span>_output.append(SN)  <span class="comment">#eval_output.append(AUC)</span></span><br><span class="line">    <span class="built_in">eval</span>_output.append(SP);<span class="built_in">eval</span>_output.append(MCC)</span><br><span class="line">    <span class="built_in">eval</span>_output.append(score_)</span><br><span class="line">    <span class="built_in">eval</span>_output = np.array(<span class="built_in">eval</span>_output,dtype=<span class="built_in">float</span>)</span><br><span class="line">    np.savetxt(<span class="string">"proba.data"</span>,proba,fmt=<span class="string">"%f"</span>,delimiter=<span class="string">"\t"</span>)</span><br><span class="line">    np.savetxt(<span class="string">"test_y.data"</span>,<span class="built_in">test</span>_y,fmt=<span class="string">"%f"</span>,delimiter=<span class="string">"\t"</span>)</span><br><span class="line">    np.savetxt(<span class="string">"predict.data"</span>,predict_,fmt=<span class="string">"%f"</span>,delimiter=<span class="string">"\t"</span>) </span><br><span class="line">    np.savetxt(<span class="string">"eval_output.data"</span>,<span class="built_in">eval</span>_output,fmt=<span class="string">"%f"</span>,delimiter=<span class="string">"\t"</span>)</span><br><span class="line">    <span class="built_in">print</span> <span class="string">"Wrote results to output.data...EOF..."</span></span><br><span class="line">    <span class="built_in">return</span> ACC,SN,SP</span><br></pre></td></tr></table></figure>
<p>下面的mean_fun用于求列表list中数值的平均值，主要是求ACC_mean,SP_mean,SN_mean，用来评估模型好坏。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_fun</span><span class="params">(onelist)</span>:</span></span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> onelist:</span><br><span class="line">        count += i</span><br><span class="line">    <span class="keyword">return</span> float(count/len(onelist))</span><br></pre></td></tr></table></figure>
<p>交叉验证代码</p>
<figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">def crossValidation<span class="params">(clf, clfname, curdir,train_all, test_all)</span>:</span><br><span class="line">    os.chdir<span class="params">(curdir)</span></span><br><span class="line">    <span class="built_in">#</span>构造出纯数据型样本集</span><br><span class="line">    cur_path = curdir</span><br><span class="line">    ACCs = [];SNs = []; SPs =[]</span><br><span class="line">    <span class="keyword">for</span> i in range<span class="params">(len<span class="params">(train_all)</span>)</span>:</span><br><span class="line">        os.chdir<span class="params">(cur_path)</span></span><br><span class="line">        train_data = train_all[i];train_X = [];train_y = []</span><br><span class="line">        test_data = test_all[i];test_X = [];test_y = []</span><br><span class="line">        <span class="keyword">for</span> eachline_train in train_data:</span><br><span class="line">            one_train = eachline_train.split<span class="params">('\t')</span> </span><br><span class="line">            one_train_format = []</span><br><span class="line">            <span class="keyword">for</span> index in range<span class="params">(<span class="number">3</span>,len<span class="params">(one_train)</span>-<span class="number">1</span>)</span>:</span><br><span class="line">                one_train_format.append<span class="params">(float<span class="params">(one_train[index])</span>)</span></span><br><span class="line">            train_X.append<span class="params">(one_train_format)</span></span><br><span class="line">            train_y.append<span class="params">(int<span class="params">(one_train[-<span class="number">1</span>].strip<span class="params">()</span>)</span>)</span></span><br><span class="line">        <span class="keyword">for</span> eachline_test in test_data:</span><br><span class="line">            one_test = eachline_test.split<span class="params">('\t')</span></span><br><span class="line">            one_test_format = []</span><br><span class="line">            <span class="keyword">for</span> index in range<span class="params">(<span class="number">3</span>,len<span class="params">(one_test)</span>-<span class="number">1</span>)</span>:</span><br><span class="line">                one_test_format.append<span class="params">(float<span class="params">(one_test[index])</span>)</span></span><br><span class="line">            test_X.append<span class="params">(one_test_format)</span></span><br><span class="line">            test_y.append<span class="params">(int<span class="params">(one_test[-<span class="number">1</span>].strip<span class="params">()</span>)</span>)</span></span><br><span class="line">        <span class="built_in">#</span>======================================================================</span><br><span class="line">        <span class="built_in">#</span>classifier start here</span><br><span class="line">        <span class="keyword">if</span> not os.path.exists<span class="params">(clfname)</span>:<span class="built_in">#</span>使用的分类器</span><br><span class="line">            os.mkdir<span class="params">(clfname)</span></span><br><span class="line">        out_path = clfname + <span class="string">"/"</span> + clfname + <span class="string">"_00"</span> + <span class="built_in">str</span><span class="params">(i)</span><span class="built_in">#</span>计算结果文件夹</span><br><span class="line">        <span class="keyword">if</span> not os.path.exists<span class="params">(out_path)</span>:</span><br><span class="line">            os.mkdir<span class="params">(out_path)</span></span><br><span class="line">        os.chdir<span class="params">(out_path)</span></span><br><span class="line">        ACC, SN, SP = classifier<span class="params">(clf, train_X, train_y, test_X, test_y)</span></span><br><span class="line">        ACCs.append<span class="params">(ACC)</span>;SNs.append<span class="params">(SN)</span>;SPs.append<span class="params">(SP)</span></span><br><span class="line">        <span class="built_in">#</span>======================================================================</span><br><span class="line">    ACC_mean = mean_fun<span class="params">(ACCs)</span></span><br><span class="line">    SN_mean = mean_fun<span class="params">(SNs)</span></span><br><span class="line">    SP_mean = mean_fun<span class="params">(SPs)</span></span><br><span class="line">    <span class="built_in">#</span>==========================================================================</span><br><span class="line">    <span class="built_in">#</span>output experiment result</span><br><span class="line">    os.chdir<span class="params">(<span class="string">"../"</span>)</span></span><br><span class="line">    os.system<span class="params">(<span class="string">"echo `date` '"</span> + str<span class="params">(clf)</span> + <span class="string">"' &gt;&gt; log.out"</span>)</span></span><br><span class="line">    os.system<span class="params">(<span class="string">"echo ACC_mean="</span> + str<span class="params">(ACC_mean)</span> + <span class="string">" &gt;&gt; log.out"</span>)</span></span><br><span class="line">    os.system<span class="params">(<span class="string">"echo SN_mean="</span> + str<span class="params">(SN_mean)</span> + <span class="string">" &gt;&gt; log.out"</span>)</span></span><br><span class="line">    os.system<span class="params">(<span class="string">"echo SP_mean="</span> + str<span class="params">(SP_mean)</span> + <span class="string">" &gt;&gt; log.out"</span>)</span></span><br><span class="line">    return ACC_mean, SN_mean, SP_mean</span><br></pre></td></tr></table></figure>
<p><strong>测试：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">	os.chdir(<span class="string">"your workhome"</span>) <span class="comment">#你的数据存放目录</span></span><br><span class="line">    datadir = <span class="string">"split10_1"</span> <span class="comment">#切分后的文件输出目录</span></span><br><span class="line">    splitDataSet(<span class="string">'datasets'</span>,<span class="number">10</span>,datadir)<span class="comment">#将数据集datasets切为十个保存到datadir目录中</span></span><br><span class="line">	<span class="comment">#==========================================================================</span></span><br><span class="line">    outdir = <span class="string">"sample_data1"</span>	<span class="comment">#抽样的数据集存放目录</span></span><br><span class="line">    train_all,test_all = generateDataset(datadir,outdir) <span class="comment">#抽样后返回训练集和测试集</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">"generateDataset end and cross validation start"</span></span><br><span class="line">    <span class="comment">#==========================================================================</span></span><br><span class="line">    <span class="comment">#分类器部分</span></span><br><span class="line">    <span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line">    clf = RandomForestClassifier(n_estimators=<span class="number">500</span>) <span class="comment">#使用随机森林分类器来训练</span></span><br><span class="line">    clfname = <span class="string">"RF_1"</span></span><br><span class="line">    <span class="comment">#==========================================================================</span></span><br><span class="line">    curdir = <span class="string">"experimentdir"</span> <span class="comment">#工作目录</span></span><br><span class="line">	<span class="comment">#clf:分类器,clfname:分类器名称,curdir:当前路径,train_all:训练集,test_all:测试集</span></span><br><span class="line">    ACC_mean, SN_mean, SP_mean = crossValidation(clf, clfname, curdir, train_all,test_all)</span><br><span class="line">    <span class="keyword">print</span> ACC_mean,SN_mean,SP_mean	<span class="comment">#将ACC均值，SP均值，SN均值都输出到控制台</span></span><br></pre></td></tr></table></figure>
<p>上面的代码主要用于抽样后的十倍交叉验证，该怎么设置参数，还得具体分析。</p>
<p>总之，交叉验证在一定程度上能够避免陷入局部最小值。一般实际操作中使用的是十折交叉验证，单具体情况还得具体分析，并没有一个统一的标准固定十倍交叉的参数或者是算法的选择以及算法参数的选择。不同的数据使用不同的算法往往会的得到不同的最优分类器。So,just try it!Happy coding!</p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注CSDN博客：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">dream_angel_z</a><br></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p>版权声明：本文为原创文章，转载请注明来源。</p>
<h2 id="1-原理"><strong>1.原理</strong></h2><h3 id="1-1_概念"><strong>1.1 概念</strong></h3><p>交叉验证(Cross-validation)主要用于模型训练或建模应用中，如分类预测、PCR、PLS回归建模等。在给定的样本空间中，拿出大部分样本作为训练集来训练模型，剩余的小部分样本使用刚建立的模型进行预测，并求这小部分样本的预测误差或者预测精度，同时记录它们的加和平均值。这个过程迭代K次，即K折交叉。其中，把每个样本的预测误差平方加和，称为PRESS(predicted Error Sum of Squares)。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[scikit-klean交叉验证]]></title>
    <link href="http://csuldw.github.io/2015/07/23/2015-07-23%20machine%20learning%20tips/"/>
    <id>http://csuldw.github.io/2015/07/23/2015-07-23 machine learning tips/</id>
    <published>2015-07-23T04:53:00.000Z</published>
    <updated>2015-10-29T13:06:50.291Z</updated>
    <content type="html"><![CDATA[<p><strong>一个Windows操作系统能够使用的pythonIDE</strong></p>
<blockquote>
<p>winPython下载地址：<a href="http://sourceforge.net/projects/winpython/files/WinPython_2.7/2.7.10.1/" target="_blank" rel="external">WinPython_2.7</a></p>
</blockquote>
<p>传统的F-measure或平衡的F-score (F1 score)是精度和召回的调和平均值：</p>
<p>$$F_1 = 2\dfrac{precision * recall}{precision + recall}$$</p>
<h3 id="1-Cross_Validation_（交叉验证）"><strong>1.Cross Validation （交叉验证）</strong></h3><p>cross validation大概的意思是：对于原始数据我们要将其一部分分为train_data，一部分分为test_data。train_data用于训练，test_data用于测试准确率。在test_data上测试的结果叫做validation_error。将一个算法作用于一个原始数据，我们不可能只做出随机的划分一次train和test_data，然后得到一个validation_error，就作为衡量这个算法好坏的标准。因为这样存在偶然性。我们必须好多次的随机的划分train_data和test_data，分别在其上面算出各自的validation_error。这样就有一组validation_error，根据这一组validation_error，就可以较好的准确的衡量算法的好坏。<br><a id="more"></a><br>cross validation是在数据量有限的情况下的非常好的一个evaluate performance的方法。而对原始数据划分出train data和test data的方法有很多种，这也就造成了cross validation的方法有很多种。</p>
<p>sklearn中的cross validation模块，最主要的函数是如下函数：<br>sklearn.cross_validation.cross_val_score:他的调用形式是scores = cross_validation.cross_val_score(clf, raw_data, raw_target, cv=5, score_func=None)</p>
<p><strong>参数解释：</strong></p>
<p><strong>clf</strong>:表示的是不同的分类器，可以是任何的分类器。比如支持向量机分类器。clf = svm.SVC(kernel=’linear’, C=1)；<br><strong>raw_data</strong>：原始数据；<br><strong>raw_target</strong>:原始类别标号；<br><strong>cv</strong>：代表的就是不同的cross validation的方法了。引用scikit-learn上的一句话（When the cv argument is an integer, cross<em>val<em>score uses the KFold or StratifiedKFold strategies by default, the latter being used if the estimator derives from ClassifierMixin.）如果cv是一个int数字的话，那么默认使用的是KFold或者StratifiedKFold交叉，如果如果指定了类别标签则使用的是StratifiedKFold。<br>__cross_val_score</em></em>:这个函数的返回值就是对于每次不同的的划分raw_data时，在test_data上得到的分类的<strong>准确率</strong>。至于准确率的算法可以通过score_func参数指定，如果不指定的话，是用clf默认自带的准确率算法。  </p>
<p>scikit-learn的cross-validation交叉验证代码：</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn import cross_validation</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn import svm</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = svm.<span class="constant">SVC</span>(kernel=<span class="string">'linear'</span>, <span class="constant">C</span>=<span class="number">1</span>)</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; scores = cross_validation.cross_val_score(clf, iris.data, iris.target, cv=<span class="number">5</span>)<span class="comment">#5-fold cv</span></span><br><span class="line"><span class="comment"># change metrics</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn import metrics</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; cross_validation.cross_val_score(clf, iris.data, iris.target, cv=<span class="number">5</span>, score_func=metrics.f1_score)</span><br><span class="line"><span class="comment">#f1 score: http://en.wikipedia.org/wiki/F1_score</span></span><br></pre></td></tr></table></figure>
<p>Note: if using LR, clf = LogisticRegression().</p>
<p><strong>生成一个数据集做为交叉验证</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.cross_validation</span> import train_test_split</span><br><span class="line">&gt;&gt;&gt; X, y = np.<span class="function"><span class="title">arange</span><span class="params">(<span class="number">10</span>)</span></span>.<span class="function"><span class="title">reshape</span><span class="params">((<span class="number">5</span>, <span class="number">2</span>)</span></span>), <span class="function"><span class="title">range</span><span class="params">(<span class="number">5</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; X</span><br><span class="line">array([[<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>],</span><br><span class="line">       [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line">&gt;&gt;&gt; <span class="function"><span class="title">list</span><span class="params">(y)</span></span></span><br><span class="line">[<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<p><strong>将数据切分为训练集和测试集</strong></p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X_train</span>, <span class="constant">X_test</span>, y_train, y_test = train_test_split(</span><br><span class="line">...     <span class="constant">X</span>, y, test_size=<span class="number">0</span>.<span class="number">33</span>, random_state=<span class="number">42</span>)</span><br><span class="line">...</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X_train</span></span><br><span class="line">array([[<span class="number">4</span>, <span class="number">5</span>],</span><br><span class="line">       [<span class="number">0</span>, <span class="number">1</span>],</span><br><span class="line">       [<span class="number">6</span>, <span class="number">7</span>]])</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; y_train</span><br><span class="line">[<span class="number">2</span>, <span class="number">0</span>, <span class="number">3</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X_test</span></span><br><span class="line">array([[<span class="number">2</span>, <span class="number">3</span>],</span><br><span class="line">       [<span class="number">8</span>, <span class="number">9</span>]])</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; y_test</span><br><span class="line">[<span class="number">1</span>, <span class="number">4</span>]</span><br></pre></td></tr></table></figure>
<p><strong>交叉验证的使用</strong></p>
<p>下面是手动划分训练集和测试集，控制台中输入下列代码进行测试：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import cross_validation</span><br><span class="line">&gt;&gt;&gt; from sklearn import datasets</span><br><span class="line">&gt;&gt;&gt; from sklearn import svm</span><br><span class="line">&gt;&gt;&gt; iris = datasets.<span class="function"><span class="title">load_iris</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; iris<span class="class">.data</span><span class="class">.shape</span>, iris<span class="class">.target</span><span class="class">.shape</span></span><br><span class="line">((<span class="number">150</span>, <span class="number">4</span>), (<span class="number">150</span>,))</span><br><span class="line">&gt;&gt;&gt; X_train, X_test, y_train, y_test = cross_validation.train_test_split(</span><br><span class="line">...     iris<span class="class">.data</span>, iris<span class="class">.target</span>, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt; X_train<span class="class">.shape</span>, y_train<span class="class">.shape</span></span><br><span class="line">((<span class="number">90</span>, <span class="number">4</span>), (<span class="number">90</span>,))</span><br><span class="line">&gt;&gt;&gt; X_test<span class="class">.shape</span>, y_test<span class="class">.shape</span></span><br><span class="line">((<span class="number">60</span>, <span class="number">4</span>), (<span class="number">60</span>,))</span><br><span class="line">&gt;&gt;&gt; clf = svm.<span class="function"><span class="title">SVC</span><span class="params">(kernel=<span class="string">'linear'</span>, C=<span class="number">1</span>)</span></span>.<span class="function"><span class="title">fit</span><span class="params">(X_train, y_train)</span></span></span><br><span class="line">&gt;&gt;&gt; clf.<span class="function"><span class="title">score</span><span class="params">(X_test, y_test)</span></span>                           </span><br><span class="line"><span class="number">0.96</span>...</span><br></pre></td></tr></table></figure>
<p>下面是交叉验证的实例：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; clf = svm.<span class="function"><span class="title">SVC</span><span class="params">(kernel=<span class="string">'linear'</span>, C=<span class="number">1</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; scores = cross_validation.cross_val_score(</span><br><span class="line">...    clf, iris<span class="class">.data</span>, iris<span class="class">.target</span>, cv=<span class="number">5</span>)</span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt; scores                                              </span><br><span class="line"><span class="function"><span class="title">array</span><span class="params">([ <span class="number">0.96</span>...,  <span class="number">1</span>.  ...,  <span class="number">0.96</span>...,  <span class="number">0.96</span>...,  <span class="number">1</span>.        ])</span></span></span><br></pre></td></tr></table></figure>
<p>通过cross_validation，设置cv=5，进行5倍交叉验证，最后得到一个scores的预测准确率数组，表示每次交叉验证得到的准确率。</p>
<figure class="highlight openscad"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; print<span class="params">(<span class="string">"Accuracy: %0.2f (+/- %0.2f)"</span> % <span class="params">(scores.mean<span class="params">()</span>, scores.std<span class="params">()</span> * <span class="number">2</span>)</span>)</span></span><br><span class="line">Accuracy: <span class="number">0.98</span> <span class="params">(+/- <span class="number">0.03</span>)</span></span><br></pre></td></tr></table></figure>
<p>通过scores.mean()求出平均值，得到平均精度。还可以通过指定scoring来设置准确率算法</p>
<figure class="highlight lasso"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="subst">&gt;&gt;&gt;</span> from sklearn <span class="keyword">import</span> metrics</span><br><span class="line"><span class="subst">&gt;&gt;&gt;</span> scores <span class="subst">=</span> cross_validation<span class="built_in">.</span>cross_val_score(clf, iris<span class="built_in">.</span><span class="built_in">data</span>, iris<span class="built_in">.</span>target,</span><br><span class="line"><span class="attribute">...</span>     cv<span class="subst">=</span><span class="number">5</span>, scoring<span class="subst">=</span><span class="string">'f1_weighted'</span>)</span><br><span class="line"><span class="subst">&gt;&gt;&gt;</span> scores                                              </span><br><span class="line"><span class="built_in">array</span>(<span class="preprocessor">[</span> <span class="number">0.96</span><span class="attribute">...</span>,  <span class="number">1.</span>  <span class="attribute">...</span>,  <span class="number">0.96</span><span class="attribute">...</span>,  <span class="number">0.96</span><span class="attribute">...</span>,  <span class="number">1.</span>        <span class="preprocessor">]</span><span class="markup">)</span></span><br></pre></td></tr></table></figure>
<p><strong>libsvm格式的数据导入：</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.datasets</span> import load_svmlight_file</span><br><span class="line">&gt;&gt;&gt; X_train, y_train = <span class="function"><span class="title">load_svmlight_file</span><span class="params">(<span class="string">"/path/to/train_dataset.txt"</span>)</span></span></span><br><span class="line">...</span><br><span class="line">&gt;&gt;&gt;X_train.<span class="function"><span class="title">todense</span><span class="params">()</span></span>#将稀疏矩阵转化为完整特征矩阵</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="2-处理非均衡问题"><strong>2.处理非均衡问题</strong></h3><p>对于正负样本比例相差较大的非均衡问题，一种调节分类器的方法就是对分类器的训练数据进行改造。一种是<strong>欠抽样</strong>，一种是<strong>过抽样</strong>。过抽样意味着赋值样例，而欠抽样意味着删除样例。对于过抽样，最后可能导致过拟合问题；而对于欠抽样，则删掉的样本中可能包含某些重要的信息，会导致欠拟合。对于正例样本较少的情况下，通常采取的方式是<strong>使用反例类别的欠抽样和正例类别的过抽样相混合的方法</strong></p>
<hr>
<h3 id="3-scikit-learn学习SVM"><strong>3.scikit-learn学习SVM</strong></h3><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import datasets</span><br><span class="line">&gt;&gt;&gt; iris = datasets.<span class="function"><span class="title">load_iris</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; digits = datasets.<span class="function"><span class="title">load_digits</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; print digits<span class="class">.data</span></span><br><span class="line">[[  <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">5</span>. ...,   <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">0</span>.]</span><br><span class="line"> [  <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">0</span>. ...,  <span class="number">10</span>.   <span class="number">0</span>.   <span class="number">0</span>.]</span><br><span class="line"> [  <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">0</span>. ...,  <span class="number">16</span>.   <span class="number">9</span>.   <span class="number">0</span>.]</span><br><span class="line"> ..., </span><br><span class="line"> [  <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">1</span>. ...,   <span class="number">6</span>.   <span class="number">0</span>.   <span class="number">0</span>.]</span><br><span class="line"> [  <span class="number">0</span>.   <span class="number">0</span>.   <span class="number">2</span>. ...,  <span class="number">12</span>.   <span class="number">0</span>.   <span class="number">0</span>.]</span><br><span class="line"> [  <span class="number">0</span>.   <span class="number">0</span>.  <span class="number">10</span>. ...,  <span class="number">12</span>.   <span class="number">1</span>.   <span class="number">0</span>.]]</span><br><span class="line">&gt;&gt;&gt; digits<span class="class">.target</span></span><br><span class="line"><span class="function"><span class="title">array</span><span class="params">([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, ..., <span class="number">8</span>, <span class="number">9</span>, <span class="number">8</span>])</span></span></span><br><span class="line">&gt;&gt;&gt; digits<span class="class">.images</span>[<span class="number">0</span>]</span><br><span class="line">array([[  <span class="number">0</span>.,   <span class="number">0</span>.,   <span class="number">5</span>.,  <span class="number">13</span>.,   <span class="number">9</span>.,   <span class="number">1</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">0</span>.,  <span class="number">13</span>.,  <span class="number">15</span>.,  <span class="number">10</span>.,  <span class="number">15</span>.,   <span class="number">5</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">3</span>.,  <span class="number">15</span>.,   <span class="number">2</span>.,   <span class="number">0</span>.,  <span class="number">11</span>.,   <span class="number">8</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">4</span>.,  <span class="number">12</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.,   <span class="number">8</span>.,   <span class="number">8</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">5</span>.,   <span class="number">8</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.,   <span class="number">9</span>.,   <span class="number">8</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">4</span>.,  <span class="number">11</span>.,   <span class="number">0</span>.,   <span class="number">1</span>.,  <span class="number">12</span>.,   <span class="number">7</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">2</span>.,  <span class="number">14</span>.,   <span class="number">5</span>.,  <span class="number">10</span>.,  <span class="number">12</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.],</span><br><span class="line">       [  <span class="number">0</span>.,   <span class="number">0</span>.,   <span class="number">6</span>.,  <span class="number">13</span>.,  <span class="number">10</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.,   <span class="number">0</span>.]])</span><br><span class="line">&gt;&gt;&gt; from sklearn import svm</span><br><span class="line">&gt;&gt;&gt; clf = svm.<span class="function"><span class="title">SVC</span><span class="params">(gamma=<span class="number">0.001</span>, C=<span class="number">100</span>.)</span></span></span><br><span class="line">&gt;&gt;&gt; clf.<span class="function"><span class="title">fit</span><span class="params">(digits.data[:-<span class="number">1</span>],digits.target[:-<span class="number">1</span>])</span></span></span><br><span class="line">SVC(C=<span class="number">100.0</span>, cache_size=<span class="number">200</span>, class_weight=None, coef0=<span class="number">0.0</span>, degree=<span class="number">3</span>,</span><br><span class="line">  gamma=<span class="number">0.001</span>, kernel=<span class="string">'rbf'</span>, max_iter=-<span class="number">1</span>, probability=False,</span><br><span class="line">  random_state=None, shrinking=True, tol=<span class="number">0.001</span>, verbose=False)</span><br><span class="line">&gt;&gt;&gt; clf.<span class="function"><span class="title">predict</span><span class="params">(digits.data[-<span class="number">1</span>])</span></span></span><br><span class="line"><span class="function"><span class="title">array</span><span class="params">([<span class="number">8</span>])</span></span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4-scikit-learn学习RandomForest"><strong>4.scikit-learn学习RandomForest</strong></h3><p>使用例子</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn.ensemble import <span class="constant">RandomForestClassifier</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X</span> = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">Y</span> = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = <span class="constant">RandomForestClassifier</span>(n_estimators=<span class="number">10</span>)</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = clf.fit(<span class="constant">X</span>, <span class="constant">Y</span>)</span><br></pre></td></tr></table></figure>
<p><strong>Method</strong></p>
<p><img src="/assets/articleImg/2015-07-21 randomForest分类器的方法png.png" alt=""></p>
<p>randomForestClassifier分类器的初始值</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def __init__(self,</span><br><span class="line">	 <span class="variable">n_estimators=</span><span class="number">10</span>,</span><br><span class="line">	 <span class="variable">criterion=</span><span class="string">"gini"</span>,</span><br><span class="line">	 <span class="variable">max_depth=</span>None,</span><br><span class="line">	 <span class="variable">min_samples_split=</span><span class="number">2</span>,</span><br><span class="line">	 <span class="variable">min_samples_leaf=</span><span class="number">1</span>,</span><br><span class="line">	 <span class="variable">min_weight_fraction_leaf=</span><span class="number">0</span>.,</span><br><span class="line">	 <span class="variable">max_features=</span><span class="string">"auto"</span>,</span><br><span class="line">	 <span class="variable">max_leaf_nodes=</span>None,</span><br><span class="line">	 <span class="variable">bootstrap=</span>True,</span><br><span class="line">	 <span class="variable">oob_score=</span>False,</span><br><span class="line">	 <span class="variable">n_jobs=</span><span class="number">1</span>,</span><br><span class="line">	 <span class="variable">random_state=</span>None,</span><br><span class="line">	 <span class="variable">verbose=</span><span class="number">0</span>,</span><br><span class="line">	 <span class="variable">warm_start=</span>False,</span><br><span class="line">	 <span class="variable">class_weight=</span>None):</span><br></pre></td></tr></table></figure>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>一个Windows操作系统能够使用的pythonIDE</strong></p>
<blockquote>
<p>winPython下载地址：<a href="http://sourceforge.net/projects/winpython/files/WinPython_2.7/2.7.10.1/">WinPython_2.7</a></p>
</blockquote>
<p>传统的F-measure或平衡的F-score (F1 score)是精度和召回的调和平均值：</p>
<p>$$F_1 = 2\dfrac{precision * recall}{precision + recall}$$</p>
<h3 id="1-Cross_Validation_（交叉验证）"><strong>1.Cross Validation （交叉验证）</strong></h3><p>cross validation大概的意思是：对于原始数据我们要将其一部分分为train_data，一部分分为test_data。train_data用于训练，test_data用于测试准确率。在test_data上测试的结果叫做validation_error。将一个算法作用于一个原始数据，我们不可能只做出随机的划分一次train和test_data，然后得到一个validation_error，就作为衡量这个算法好坏的标准。因为这样存在偶然性。我们必须好多次的随机的划分train_data和test_data，分别在其上面算出各自的validation_error。这样就有一组validation_error，根据这一组validation_error，就可以较好的准确的衡量算法的好坏。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习-组合算法总结]]></title>
    <link href="http://csuldw.github.io/2015/07/22/2015-07-22%20%20ensemble/"/>
    <id>http://csuldw.github.io/2015/07/22/2015-07-22  ensemble/</id>
    <published>2015-07-21T22:53:00.000Z</published>
    <updated>2015-10-29T13:06:43.722Z</updated>
    <content type="html"><![CDATA[<h2 id="组合模型"><strong>组合模型</strong></h2><p>下面简单的介绍下Bootstraping, Bagging, Boosting, AdaBoost, RandomForest 和Gradient boosting这些组合型算法.</p>
<h3 id="1-Bootstraping"><strong>1.Bootstraping</strong></h3><p><strong>Bootstraping</strong>: 名字来自成语“pull up by your own bootstraps”，意思就是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下：<br> <a id="more"></a></p>
<blockquote>
<p>（1）采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。<br>（2）根据抽出的样本计算给定的统计量T。<br>（3）重复上述N次（一般大于1000），得到N个统计量T。<br>（4）计算上述N个统计量T的样本方差，得到统计量的方差。 </p>
</blockquote>
<p>应该说Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。</p>
<hr>
<h3 id="2-装袋bagging"><strong>2.装袋bagging</strong></h3><p>装袋算法相当于多个专家投票表决，对于多次测试，每个样本返回的是多次预测结果较多的那个。</p>
<p>装袋算法描述</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">模型生成</span><br><span class="line">	令<span class="keyword">n</span>为训练数据的实例数量</span><br><span class="line">	对于t次循环中的每一次</span><br><span class="line">		从训练数据中采样<span class="keyword">n</span>个实例</span><br><span class="line">		将学习应用于所采样本</span><br><span class="line">		保存结果模型</span><br><span class="line">分类</span><br><span class="line">	对于t个模型的每一个</span><br><span class="line">		使用模型对实例进行预测</span><br><span class="line">	返回被预测次数最多的一个</span><br></pre></td></tr></table></figure>
<p>bagging：bootstrap aggregating的缩写。让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后可得到一个预测函数序列</p>
<p>$$h_1，⋯ ⋯h_n$$ </p>
<p>最终的预测函数H对分类问题采用<strong>投票方式</strong>，对回归问题采用<strong>简单平均方法</strong>对新示例进行判别。</p>
<p>[训练R个分类器f_i，分类器之间其他相同就是参数不同。其中f_i是通过从训练集合中(N篇文档)随机取(取后放回)N次文档构成的训练集合训练得到的。对于新文档d，用这R个分类器去分类，得到的最多的那个类别作为d的最终类别。]</p>
<p>使用scikit-learn测试bagging方法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>bagging = BaggingClassifier(KNeighborsClassifier(),</span><br><span class="line"><span class="prompt">... </span>                            max_samples=<span class="number">0.5</span>, max_features=<span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="3-提升Boosting与Adaboost"><strong>3.提升Boosting与Adaboost</strong></h3><p><strong>提升算法描述</strong></p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">模型生成</span><br><span class="line">	赋予每个训练实例相同的权值</span><br><span class="line">	t次循环中的每一次：</span><br><span class="line">		将学习算法应用于加了权的数据集上并保存结果模型</span><br><span class="line">		计算模型在加了权的数据上的误差<span class="keyword">e</span>并保存这个误差</span><br><span class="line">		结果<span class="keyword">e</span>等于0或者大于等于0.5：</span><br><span class="line">			终止模型</span><br><span class="line">		对于数据集中的每个实例：</span><br><span class="line">			如果模型将实例正确分类</span><br><span class="line">				将实例的权值乘以<span class="keyword">e</span>/(1-<span class="keyword">e</span>)</span><br><span class="line">		将所有的实例权重进行正常化</span><br><span class="line">分类</span><br><span class="line">	赋予所有类权重为0</span><br><span class="line">	对于t（或小于t）个模型中的每一个：</span><br><span class="line">		给模型预测的类加权 -<span class="literal">log</span>(<span class="keyword">e</span>/(1-<span class="keyword">e</span>))</span><br><span class="line">	返回权重最高的类</span><br></pre></td></tr></table></figure>
<p>这个模型提供了一种巧妙的方法生成一系列互补型的专家。</p>
<p><strong>boosting</strong>: 其中主要的是<strong>AdaBoost</strong>（Adaptive boosting，自适应boosting）。初始化时对每一个训练例赋相等的权重1／N，然后用该学算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在后续的学习中集中对比较难的训练例进行学习，从而得到一个预测函数序列$h_1,⋯, h_m$ , 其中h_i也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方法对新示例进行判别。</p>
<p>提升算法理想状态是这些模型对于其他模型来说是一个补充，每个模型是这个领域的一个专家，而其他模型在这部分却不能表现很好，就像执行官一样要寻觅那些技能和经验互补的顾问，而不是重复的。这与装袋算法有所区分。</p>
<p>Adaboost算法描述</p>
<figure class="highlight stata"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">模型生成</span><br><span class="line">	训练数据中的每个样本，并赋予一个权重，构成权重向量<span class="keyword">D</span>，初始值为1/<span class="keyword">N</span></span><br><span class="line">	t次循环中的每一次：</span><br><span class="line">		在训练数据上训练弱分类器并计算分类器的错误率<span class="literal">e</span></span><br><span class="line">		如果<span class="keyword">e</span>等于0或者大于等于用户指定的阈值：</span><br><span class="line">			终止模型，<span class="keyword">break</span></span><br><span class="line">		重新调整每个样本的权重，其中<span class="keyword">alpha</span>=0.5*<span class="literal">ln</span>((1-<span class="keyword">e</span>)/<span class="keyword">e</span>)</span><br><span class="line">		对权重向量<span class="keyword">D</span>进行更新，正确分类的样本的权重降低而错误分类的样本权重值升高</span><br><span class="line">		对于数据集中的每个样例：</span><br><span class="line">			如果某个样本正确分类：</span><br><span class="line">				权重改为<span class="keyword">D</span>^(t+1)_i = <span class="keyword">D</span>^(t)_i * <span class="keyword">e</span>^(-a)/<span class="literal">Sum</span>(<span class="keyword">D</span>)</span><br><span class="line">			如果某个样本错误分类：</span><br><span class="line">				权重改为<span class="keyword">D</span>^(t+1)_i = <span class="keyword">D</span>^(t)_i * <span class="keyword">e</span>^(a)/<span class="literal">Sum</span>(<span class="keyword">D</span>)</span><br><span class="line">分类</span><br><span class="line">	赋予所有类权重为0</span><br><span class="line">	对于t（或小于t）个模型（基分类器）中的每一个：</span><br><span class="line">		给模型预测的类加权 -<span class="literal">log</span>(<span class="keyword">e</span>/(1-<span class="keyword">e</span>))</span><br><span class="line">	返回权重最高的类</span><br></pre></td></tr></table></figure>
<p>（类似Bagging方法，但是训练是串行进行的，第k个分类器训练时关注对前k-1分类器中错分的文档，即不是随机取，而是加大取这些文档的概率。)</p>
<p><strong>bagging与boosting的区别</strong>：</p>
<p>二者的主要区别是<strong>取样方式不同</strong>。bagging采用<strong>均匀取样</strong>，而Boosting根据<strong>错误率来取样</strong>，因此boosting的分类精度要优于Bagging。bagging的训练集的选择是随机的，各轮训练集之间相互独立，而boostlng的各轮训练集的选择与前面各轮的学习结果有关；bagging的各个预测函数没有权重，而boosting是有权重的；bagging的各个预测函数可以并行生成，而boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。bagging可通过并行训练节省大量时间开销。</p>
<p>bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化—- Overfit。  </p>
<p>Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。 </p>
<p><strong>Gradient boosting（又叫Mart, Treenet)</strong>：Boosting是一种思想，Gradient Boosting是一种实现Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型<strong>损失函数的梯度下降方向</strong>。<strong>损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。</strong>如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是<strong>让损失函数在其梯度（Gradient)的方向上下降</strong>。  </p>
<p>使用scikit-learn测试adaboost算法</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.cross_validation</span> import cross_val_score</span><br><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.datasets</span> import load_iris</span><br><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.ensemble</span> import AdaBoostClassifier</span><br><span class="line">&gt;&gt;&gt; iris = <span class="function"><span class="title">load_iris</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; clf = <span class="function"><span class="title">AdaBoostClassifier</span><span class="params">(n_estimators=<span class="number">100</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; scores = <span class="function"><span class="title">cross_val_score</span><span class="params">(clf, iris.data, iris.target)</span></span></span><br><span class="line">&gt;&gt;&gt; scores.<span class="function"><span class="title">mean</span><span class="params">()</span></span>                             </span><br><span class="line"><span class="number">0.9</span>...</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="4-Random_Forest"><strong>4.Random Forest</strong></h3><p><strong>Random Forest</strong>： 随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 在建立每一棵决策树的过程中，有两点需要注意——<strong>采样</strong>与<strong>完全分裂</strong>。首先是两个随机采样的过程，random forest对输入的数据要进行行和列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m &lt;&lt; M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。<strong>一般很多的决策树算法都一个重要的步骤——剪枝，但随机森林不这样做，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。</strong> 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。  </p>
<p><strong>Random forest与bagging的区别</strong>：</p>
<p>(1)Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；<br>(2)bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； <strong>一般Random forest效果比bagging效果好！</strong></p>
<p>使用scikit-learn测试随机森林算法</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn.ensemble import <span class="constant">RandomForestClassifier</span></span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">X</span> = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; <span class="constant">Y</span> = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = <span class="constant">RandomForestClassifier</span>(n_estimators=<span class="number">10</span>)</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = clf.fit(<span class="constant">X</span>, <span class="constant">Y</span>)</span><br></pre></td></tr></table></figure>
<h3 id="5-Gradient_boosting"><strong>5.Gradient boosting</strong></h3><p>梯度提升树或者梯度提升回归树(GBRT)是任意一个不同损失函数的泛化。GBRT是一个灵敏的并且高效程序，可以用在回归和分类中。梯度提升树模型在许多领域中都有使用，如web搜索排行榜和社会生态学中。它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。这句话有一点拗口，损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错（其实这里有一个方差、偏差均衡的问题，但是这里就假设损失函数越大，模型越容易出错）。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。</p>
<p>GRBT的优势：</p>
<ul>
<li>混合数据类型的自然处理</li>
<li>预测力强</li>
<li>健壮的输出空间</li>
</ul>
<p>Boosting主要是一种思想，表示“知错就改”。而Gradient Boosting是在这个思想下的一种函数（也可以说是模型）的优化的方法，首先将函数分解为可加的形式（其实所有的函数都是可加的，只是是否好放在这个框架中，以及最终的效果如何）。然后进行m次迭代，通过使得损失函数在梯度方向上减少，最终得到一个优秀的模型。值得一提的是，每次模型在梯度方向上的减少的部分，可以认为是一个“小”的或者“弱”的模型，最终我们会通过加权(也就是每次在梯度方向上下降的距离）的方式将这些“弱”的模型合并起来，形成一个更好的模型。</p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="组合模型"><strong>组合模型</strong></h2><p>下面简单的介绍下Bootstraping, Bagging, Boosting, AdaBoost, RandomForest 和Gradient boosting这些组合型算法.</p>
<h3 id="1-Bootstraping"><strong>1.Bootstraping</strong></h3><p><strong>Bootstraping</strong>: 名字来自成语“pull up by your own bootstraps”，意思就是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下：<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习scikit-learn入门教程（译）]]></title>
    <link href="http://csuldw.github.io/2015/07/21/2015-07-21%20An%20introduction%20to%20machine%20learning%20with%20scikit-learn%20/"/>
    <id>http://csuldw.github.io/2015/07/21/2015-07-21 An introduction to machine learning with scikit-learn /</id>
    <published>2015-07-21T13:31:00.000Z</published>
    <updated>2015-10-26T01:28:05.568Z</updated>
    <content type="html"><![CDATA[<p>原文链接：<a href="http://scikit-learn.github.io/dev/tutorial/basic/tutorial.html" target="_blank" rel="external">http://scikit-learn.github.io/dev/tutorial/basic/tutorial.html</a></p>
<p><strong>章节内容</strong></p>
<p>在这个章节中，我们主要介绍关于scikit-learn机器学习词库，并且将给出一个学习样例。</p>
<h2 id="机器学习：问题设置"><strong>机器学习：问题设置</strong></h2><p>通常，一个学习问题是通过一系列的n个样本数据来学习然后尝试预测未知数据的属性。如果每一个样本超过一个单一的数值，例如多维输入（也叫做多维数据），那么它就拥有了多个特征。<br><a id="more"></a><br>我们可以把学习问题划分为几个大的来别：</p>
<ul>
<li>监督学习: 在监督学习中，这些数据自带了我们想要预测的附加属性（<a href="http://scikit-learn.github.io/dev/supervised_learning.html#supervised-learning" target="_blank" rel="external">scikit-learn监督学习链接</a>），这个问题包括：<ul>
<li>分类：样本属于属于两类或者多类，我们想从已经被标记的数据中来预测未知数据的类别。一个分类问题的例子就是手写字识别。这个例子的目的是从有些的类别中识别出输入向量的类别。对于分类的另一种想法是作为监督学习的一种分离的表格(不是连续的)，在这个表格中，一个是被限制的类别数量，而且对于每个类别都有N个样例被提供；一个是尝试用正确的类别或者类来标记他们。</li>
<li>回归：如果期望的输出是由一个或者更多的连续的变量组成，那么就叫做回归。回归问题的例子将通过一条鲑鱼的年龄和重量预测它的长度。</li>
</ul>
</li>
<li>无监督学习：在无监督学习里面，训练数据是由一组没有任何类别标签值的一系列输入向量组成。这种问题的目的是可能可以在这些数据里发现相似的样例组，这些相似的样例被称作聚类。或者在输入空间里决定数据分布，称之为密度估算；或者将数据从高维空间映射到二维或三维空间中，称之为数据可视化问题。（<a href="http://scikit-learn.github.io/dev/unsupervised_learning.html#unsupervised-learning" target="_blank" rel="external">无监督学习链接</a>）</li>
</ul>
<p><strong>训练集和测试集</strong></p>
<p>机器学习是关于学习数据集的一些属性然后将它们应用到新的数据上。这就是为什么在机器学习中评价一个算法的通常惯例是把数据集切分为两个数据集，其中一个叫做训练集，用来学习数据的属性；另一个叫做测试集，在测试集上测试那些属性。</p>
<h2 id="加载样本数据集"><strong>加载样本数据集</strong></h2><p>scikit-learn带有一些标准的数据集，例如用于分类的<a href="http://en.wikipedia.org/wiki/Iris_flower_data_set" target="_blank" rel="external">iris</a>和<a href="http://archive.ics.uci.edu/ml/datasets/Pen-Based+Recognition+of+Handwritten+Digits" target="_blank" rel="external">digit</a>数据集和用于回归的<a href="http://archive.ics.uci.edu/ml/datasets/Housing" target="_blank" rel="external"> boston house prices dataset </a>.</p>
<p>下面，我们打开Python编译器，然后载入<strong>iris</strong>和digits数据集。我们的符号’$’表示shell提示，’&gt;&gt;&gt;’表示Python编译器提示</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="variable">$ </span>python</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn import datasets</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; iris = datasets.load_iris()</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; digits = datasets.load_digits()</span><br></pre></td></tr></table></figure>
<p> 数据集是一个类似字典的对象，包含所有的数据和一些和数据有关的元数据。数据存储在.data中，是个n_samples,n_features的数组。在监督问题的情况下，一个或多个类别变量存储在.target成员中。更多有关的不同数据集的细节可以在<a href="http://scikit-learn.github.io/dev/datasets/index.html#datasets" target="_blank" rel="external">dedicated section</a>查找。</p>
<p> 例如，在digits数据集情况下，digits.data 提供了可用于分类数字样本。</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="tag">print</span>(<span class="tag">digits</span><span class="class">.data</span>)  </span><br><span class="line"><span class="attr_selector">[[  0.   0.   5. ...,   0.   0.   0.]</span></span><br><span class="line"> <span class="attr_selector">[  0.   0.   0. ...,  10.   0.   0.]</span></span><br><span class="line"> <span class="attr_selector">[  0.   0.   0. ...,  16.   9.   0.]</span></span><br><span class="line"> ...,</span><br><span class="line"> <span class="attr_selector">[  0.   0.   1. ...,   6.   0.   0.]</span></span><br><span class="line"> <span class="attr_selector">[  0.   0.   2. ...,  12.   0.   0.]</span></span><br><span class="line"> <span class="attr_selector">[  0.   0.  10. ...,  12.   1.   0.]</span>]</span><br></pre></td></tr></table></figure>
<p>并且digits.target给出了digit数据集的真实结果，这些数字是和我们正在学习的每个数字图像相关的数字。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; digits<span class="class">.target</span></span><br><span class="line"><span class="function"><span class="title">array</span><span class="params">([<span class="number">0</span>, <span class="number">1</span>, <span class="number">2</span>, ..., <span class="number">8</span>, <span class="number">9</span>, <span class="number">8</span>])</span></span></span><br></pre></td></tr></table></figure>
<p><strong>数组的形状</strong></p>
<p>数据总是一些2D数组，shape(n_samples,n_features),尽管原始数据也许有一个不同的形状，就这个digits而言，每一个原始样例是一个shape(8,8)的图像，并且能被访问使用:</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; <span class="tag">digits</span><span class="class">.images</span><span class="attr_selector">[0]</span></span><br><span class="line"><span class="tag">array</span>(<span class="attr_selector">[[  0.,   0.,   5.,  13.,   9.,   1.,   0.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   0.,  13.,  15.,  10.,  15.,   5.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   3.,  15.,   2.,   0.,  11.,   8.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   4.,  12.,   0.,   0.,   8.,   8.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   5.,   8.,   0.,   0.,   9.,   8.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   4.,  11.,   0.,   1.,  12.,   7.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   2.,  14.,   5.,  10.,  12.,   0.,   0.]</span>,</span><br><span class="line">       <span class="attr_selector">[  0.,   0.,   6.,  13.,  10.,   0.,   0.,   0.]</span>])</span><br></pre></td></tr></table></figure>
<p><a href="http://scikit-learn.github.io/dev/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py" target="_blank" rel="external">simple example on this dataset </a>这个数据集表明了在scikit-learn中怎样从原始问题开始着手制作数据。</p>
<h2 id="学习和预测"><strong>学习和预测</strong></h2><p>在digits数据集中，给定一幅手写数字的数字图像，任务是预测结果。我们给定的样本有10种类别（是数字0到9），基于此我们建立一个估计方法能够预测我们没有见过的样本属于哪一类。</p>
<p>在scikit-learn中，用于分类的估计模型是一个实现了fit(x,y)方法和predict(T)方法的Python对象。</p>
<p>估计模型的例子是在实现了<a href="http://en.wikipedia.org/wiki/Support_vector_machine" target="_blank" rel="external">support vector classification支持向量机</a>的类 sklearn.svm.SVC。估计模型的构造函数带有模型参数，但是目前，我们将估计模型当做一个黑盒子。</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; from sklearn import svm  </span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; clf = svm.<span class="constant">SVC</span>(gamma=<span class="number">0</span>.<span class="number">001</span>, <span class="constant">C</span>=<span class="number">100</span>.)</span><br></pre></td></tr></table></figure>
<p><strong>选择模型参数</strong></p>
<p>在这个例子中，我们这设定了gamma值。可以通过使用<a href="http://scikit-learn.github.io/dev/modules/grid_search.html#grid-search" target="_blank" rel="external">网格搜索</a>和<a href="http://scikit-learn.github.io/dev/modules/cross_validation.html#cross-validation" target="_blank" rel="external">交叉验证</a>自动的找出最好的参数值</p>
<p>我们把我们的评估模型命名为clf，作为一个分类器，它现在必须拟合这个模型，也就是它必须从这个模型学习。我们通过将数据集传递给fit函数完成。作为训练集，除了最后一个样本，我们选择其余的所有样本。通过python语句[:-1]选择样本，这条语句将从digits.data中产生一个除了最后一个样本的新数组。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">clf.fit(digits.data[:-<span class="number">1</span>], digits.target[:-<span class="number">1</span>])    </span><br><span class="line">SVC(<span class="variable">C=</span><span class="number">100.0</span>, <span class="variable">cache_size=</span><span class="number">200</span>, <span class="variable">class_weight=</span>None, <span class="variable">coef0=</span><span class="number">0.0</span>, <span class="variable">degree=</span><span class="number">3</span>,  </span><br><span class="line">  <span class="variable">gamma=</span><span class="number">0.001</span>, <span class="variable">kernel=</span>'rbf', <span class="variable">max_iter=</span>-<span class="number">1</span>, <span class="variable">probability=</span>False,  </span><br><span class="line">  <span class="variable">random_state=</span>None, <span class="variable">shrinking=</span>True, <span class="variable">tol=</span><span class="number">0.001</span>, <span class="variable">verbose=</span>False)</span><br></pre></td></tr></table></figure>
<p>现在，我们可以预测新值，尤其是我们可以问分类器在digits数据集中的用来训练分类器时没有使用的最后一个数据是数字几：</p>
<p>相应的图像如下所示:</p>
<center><br><img src="http://img.blog.csdn.net/20150720185355481" alt="这里写图片描述"><br></center>

<p>正如你看到的，这是一个具有挑战性的任务：图象的分辨率很低。你认同这个分类器吗？</p>
<p>一个完整的分类问题实例可以通过下面的链接下载，用来作为你运行并且学习的例子 <a href="http://scikit-learn.github.io/dev/auto_examples/classification/plot_digits_classification.html#example-classification-plot-digits-classification-py" target="_blank" rel="external">Recognizing hand-written digits</a></p>
<h2 id="模型持久化"><strong>模型持久化</strong></h2><p>可以通过使用python的built-in持久化模型在scikit中保存一个模型，命名<a href="http://docs.python.org/library/pickle.html" target="_blank" rel="external">pickle</a>:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn import svm</span><br><span class="line">&gt;&gt;&gt; from sklearn import datasets</span><br><span class="line">&gt;&gt;&gt; clf = svm.<span class="function"><span class="title">SVC</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; iris = datasets.<span class="function"><span class="title">load_iris</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; X, y = iris<span class="class">.data</span>, iris<span class="class">.target</span></span><br><span class="line">&gt;&gt;&gt; clf.<span class="function"><span class="title">fit</span><span class="params">(X, y)</span></span>  </span><br><span class="line">SVC(C=<span class="number">1.0</span>, cache_size=<span class="number">200</span>, class_weight=None, coef0=<span class="number">0.0</span>,</span><br><span class="line">  decision_function_shape=None, degree=<span class="number">3</span>, gamma=<span class="string">'auto'</span>, kernel=<span class="string">'rbf'</span>,</span><br><span class="line">  max_iter=-<span class="number">1</span>, probability=False, random_state=None, shrinking=True,</span><br><span class="line">  tol=<span class="number">0.001</span>, verbose=False)</span><br><span class="line">&gt;&gt;&gt; import pickle</span><br><span class="line">&gt;&gt;&gt; s = pickle.<span class="function"><span class="title">dumps</span><span class="params">(clf)</span></span></span><br><span class="line">&gt;&gt;&gt; clf2 = pickle.<span class="function"><span class="title">loads</span><span class="params">(s)</span></span></span><br><span class="line">&gt;&gt;&gt; clf2.<span class="function"><span class="title">predict</span><span class="params">(X[<span class="number">0</span>])</span></span></span><br><span class="line"><span class="function"><span class="title">array</span><span class="params">([<span class="number">0</span>])</span></span></span><br><span class="line">&gt;&gt;&gt; y[<span class="number">0</span>]</span><br><span class="line"><span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>在scikit的特别情况下，使用joblib替换pickle(joblib.dump &amp; joblib.load)会更有趣,它在大数据上是更有效的，但是仅仅只能存入的是字典而不是字符串。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn<span class="class">.externals</span> import joblib</span><br><span class="line">&gt;&gt;&gt; joblib.<span class="function"><span class="title">dump</span><span class="params">(clf, <span class="string">'filename.pkl'</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>然后你就可以读取上面的pickled模型使用了（通常是在其它的Python程序中）：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; clf = joblib.<span class="function"><span class="title">load</span><span class="params">(<span class="string">'filename.pkl'</span>)</span></span></span><br></pre></td></tr></table></figure>
<h2 id="惯例"><strong>惯例</strong></h2><p>scikit-learn估计量有一些特定的规则是的分类器更具有预测性</p>
<p><strong>Type casting 类型转换</strong></p>
<p>除非特别指定，否则输入格式是float64</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; import numpy as np</span><br><span class="line">&gt;&gt;&gt; from sklearn import random_projection</span><br><span class="line">&gt;&gt;&gt; rng = np<span class="class">.random</span><span class="class">.RandomState</span>(<span class="number">0</span>)</span><br><span class="line">&gt;&gt;&gt; X = rng.<span class="function"><span class="title">rand</span><span class="params">(<span class="number">10</span>, <span class="number">2000</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; X = np.<span class="function"><span class="title">array</span><span class="params">(X, dtype=<span class="string">'float32'</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; X<span class="class">.dtype</span></span><br><span class="line"><span class="function"><span class="title">dtype</span><span class="params">(<span class="string">'float32'</span>)</span></span></span><br><span class="line">&gt;&gt;&gt; transformer = random_projection.<span class="function"><span class="title">GaussianRandomProjection</span><span class="params">()</span></span></span><br><span class="line">&gt;&gt;&gt; X_new = transformer.<span class="function"><span class="title">fit_transform</span><span class="params">(X)</span></span></span><br><span class="line">&gt;&gt;&gt; X_new<span class="class">.dtype</span></span><br><span class="line"><span class="function"><span class="title">dtype</span><span class="params">(<span class="string">'float64'</span>)</span></span></span><br></pre></td></tr></table></figure>
<p>在这个例子中，X是float32，通过fit_transform(X)把它转为float64</p>
<p>回归的输出值是float64，分类的也是：</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; from sklearn <span class="built_in">import</span> datasets</span><br><span class="line">&gt;&gt;&gt; from sklearn.svm <span class="built_in">import</span> SVC</span><br><span class="line">&gt;&gt;&gt; <span class="variable">iris =</span> datasets.load_iris()</span><br><span class="line">&gt;&gt;&gt; <span class="variable">clf =</span> SVC()</span><br><span class="line">&gt;&gt;&gt; clf.fit(iris.data, iris.target)  </span><br><span class="line">SVC(<span class="variable">C=</span><span class="number">1.0</span>, <span class="variable">cache_size=</span><span class="number">200</span>, <span class="variable">class_weight=</span>None, <span class="variable">coef0=</span><span class="number">0.0</span>,</span><br><span class="line">  <span class="variable">decision_function_shape=</span>None, <span class="variable">degree=</span><span class="number">3</span>, <span class="variable">gamma=</span>'auto', <span class="variable">kernel=</span>'rbf',</span><br><span class="line">  <span class="variable">max_iter=</span>-<span class="number">1</span>, <span class="variable">probability=</span>False, <span class="variable">random_state=</span>None, <span class="variable">shrinking=</span>True,</span><br><span class="line">  <span class="variable">tol=</span><span class="number">0.001</span>, <span class="variable">verbose=</span>False)</span><br><span class="line">&gt;&gt;&gt; list(clf.predict(iris.data[:<span class="number">3</span>]))</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>]</span><br><span class="line">&gt;&gt;&gt; clf.fit(iris.data, iris.target_names[iris.target])  </span><br><span class="line">SVC(<span class="variable">C=</span><span class="number">1.0</span>, <span class="variable">cache_size=</span><span class="number">200</span>, <span class="variable">class_weight=</span>None, <span class="variable">coef0=</span><span class="number">0.0</span>,</span><br><span class="line">  <span class="variable">decision_function_shape=</span>None, <span class="variable">degree=</span><span class="number">3</span>, <span class="variable">gamma=</span>'auto', <span class="variable">kernel=</span>'rbf',</span><br><span class="line">  <span class="variable">max_iter=</span>-<span class="number">1</span>, <span class="variable">probability=</span>False, <span class="variable">random_state=</span>None, <span class="variable">shrinking=</span>True,</span><br><span class="line">  <span class="variable">tol=</span><span class="number">0.001</span>, <span class="variable">verbose=</span>False)</span><br><span class="line">&gt;&gt;&gt; list(clf.predict(iris.data[:<span class="number">3</span>]))  </span><br><span class="line">['setosa', 'setosa', 'setosa']</span><br></pre></td></tr></table></figure>
<p>这里，第一次predict()返回的是一个整数数组，因为在拟合中用到了iris.target（一个整数数组），第二个predict返回的是一个字符串数组，因为用来拟合的是iris.target_names。</p>
<h2 id="Supplementary">Supplementary</h2><p>推介一个好用的python IDE：</p>
<blockquote>
<p>winPython下载地址：<a href="http://sourceforge.net/projects/winpython/files/WinPython_2.7/2.7.10.1/" target="_blank" rel="external">WinPython_2.7</a></p>
</blockquote>
<hr>
<p><br></p>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a> 个人博客： <a href="http://csuldw.github.io" target="_black">D.W’s Diary</a><br><br></strong></center>
]]></content>
    <summary type="html">
    <![CDATA[<p>原文链接：<a href="http://scikit-learn.github.io/dev/tutorial/basic/tutorial.html">http://scikit-learn.github.io/dev/tutorial/basic/tutorial.html</a></p>
<p><strong>章节内容</strong></p>
<p>在这个章节中，我们主要介绍关于scikit-learn机器学习词库，并且将给出一个学习样例。</p>
<h2 id="机器学习：问题设置"><strong>机器学习：问题设置</strong></h2><p>通常，一个学习问题是通过一系列的n个样本数据来学习然后尝试预测未知数据的属性。如果每一个样本超过一个单一的数值，例如多维输入（也叫做多维数据），那么它就拥有了多个特征。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Airbnb欺诈预测机器学习模型设计：准确率和召回率的故事（译）]]></title>
    <link href="http://csuldw.github.io/2015/07/18/2015-07-18-a%20precision%20and%20recall/"/>
    <id>http://csuldw.github.io/2015/07/18/2015-07-18-a precision and recall/</id>
    <published>2015-07-18T04:30:00.000Z</published>
    <updated>2015-10-29T13:06:21.168Z</updated>
    <content type="html"><![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>


<p>Airbnb网站基于允许任何人将闲置的房屋进行长期或短期出租构建商业模式，来自房客或房东的欺诈风险是必须解决的问题。Airbnb信任和安全小组通过构建机器学习模型进行欺诈预测，本文介绍了其设计思想。假想模型是预测某些虚拟人物是否为“反面人物”，基本步骤：构建模型预期，构建训练集和测试集，特征学习，模型性能评估。其中特征转换倾向于采用条件概率编码（CP-coding），评估度量是准确率（Precision）和召回率（Recall），通常偏向于高召回率。<br><a id="more"></a><br><strong>以下为全文内容：</strong></p>
<p>在Airbnb网站上，我们专注于创造一个这样的地方：一个人可以属于任何地方。部分归属感来自于我们用户之间的信任，同时认识到他们的安全是我们最关心的。</p>
<p>虽然我们绝大多数的社区是由友好和可靠的房东和房客组成，但仍然有一小部分用户，他们试图从我们的网站中（非法）获利。这些都是非常罕见的，尽管如此，信任和安全小组还是因此而产生。</p>
<p>信任和安全小组主要是解决任何可能会发生在我们平台的欺诈行为。我们最主要目的是试图保护我们的用户和公司免于不同类型的风险。例如：退款风险——一个绝大多数电子商务企业都熟悉的风险问题。为了减少此类欺诈行为，信任和安全小组的数据科学家构建了不同种类的机器学习模型，用来帮助识别不同类型的风险。想要获得我们模型背后更多的体系结构信息，请参考以前的文章 <a href="http://nerds.airbnb.com/architecting-machine-learning-system-risk/" target="_blank" rel="external">机器学习风险系统的设计</a>。</p>
<p>在这篇文章中，我对机器学习的模型建立给了一个简短的思维过程概述。当然，每个模型都有所不同，但希望它能够给读者在关于机器学习中我们如何使用数据来帮助保护我们的用户以及如何改善模型的不同处理方法上带来一个全新的认识。在这篇文章中，我们假设想要构建一个这样的模型：预测某些虚构的角色是否是反面人物。</p>
<h3 id="试图预测的是什么？">试图预测的是什么？</h3><p>在模型建立中最基本的问题就是明确你想要用这个模型来预测什么。我知道这个听起来似乎很愚蠢，但很多时候，通过这个问题可以引发出其它更深层的问题。</p>
<p>即使是一个看似简单的角色分类模型，随着我们逐步深入地思考，也可以提出许多更深层的问题。例如，我们想要怎样来给这个模型评分：仅仅是给当前新介绍的角色还是给所有角色？如果是前者，我们想要评分的角色和人物介绍中的角色评分相差多远？如果是后者，我们又该多长时间给这些角色评分呢？</p>
<p>第一个想法可能是根据人物介绍中给每个角色的评分来建立模型。然而，这种模型，我们可能不能随着时间的推移动态地追踪人物的评分。此外，我们可能会因为在介绍时的一些“好”的特征而忽略了潜在的反面人物。</p>
<p>相反，我们还可以建立这样一个模型，只要他/她出现在情节里面就评分一次。这将让我们在每个时间段都会有人物评分并检测出任何异常情况。但是，考虑到在每个角色单独出现的情况下可能没有任何的角色类别发展，所以这可能也不是最实际的方法。</p>
<p>深思熟虑之后，我们决定把模型设计成介于这两种想法之间的模型。例如，建立这样一种模型，在每次有意义的事情发生的时候对角色进行评分，比如结交新盟友，龙族领地占领等等。在这种方式下，我们仍然可以随着时间的变化来跟踪人物的评分，同时，对没有最新进展的角色也不会多加评分。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a2b74a98e.jpg" alt=""></p>
<h3 id="如何模拟得分？">如何模拟得分？</h3><p>因为我们的目的是分析每个时期的得分，所以我们的训练集要能反映出某段时间某个角色的类别行为，最后的训练数据集类似于下图：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a2e11dabc_middle.jpg?_=23712" alt=""></p>
<p>与每个角色相关的时间不一定是连续的，因为我们关心的是那些有着重要事件发展的时间。</p>
<p>在这个实例当中，Jarden在3个不同的场合有着重要的角色发展并且在一段时间内持续扩充他的军队。相比之下，Dineas 在5个不同的场合有着重要的角色发展并且主管着4个龙族中心基地。</p>
<h3 id="采样">采样</h3><p>在机器学习模型中，从观测数据中下采样是有必要的。采样过程本身很简单，一旦有了所需要的训练数据集，就可以在数据集上做一个基于行的采样。</p>
<p>然而，由于这里描述的模型是处理每个角色多个时期的样本，基于行采样可能会导致这样一种情况，即在建立模型的数据和用来验证的数据之间，场景附加的人物角色被分离开。如下表所示：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a304d75b7.jpg" alt=""></p>
<p>显然这并不是理想的采样，因为我们没有得到每个角色的整体描述，并且这些缺失的观测数据可能对建立一个好的模型至关重要。</p>
<p>出于这个原因，我们需要做基于角色的采样。这样做能确保在模型数据建立中包含所有场合附加的角色，或者什么都没有。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a31c8bc17.jpg" alt=""></p>
<p>此外，当我们将我们的数据集切分为训练集和测试集时，通常这样的逻辑也适用。</p>
<h3 id="特征设计">特征设计</h3><p>特征设计是机器学习不可或缺的一部分，通常情况下，在特征种类的选择上，对数据的充分理解有助于形成一个更好的模型设计思路。特征设计的实例包括特征规范化和分类特征处理。</p>
<p>特征规范化是标准化特征的一种方式，允许更合理的对比。如下表所示：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a34d3dd90.jpg" alt=""></p>
<p>从上表可知，每个人物都有10,000个士兵。然而，Serion掌权长达5年，而Dineas仅仅掌权2年。通过这些人物比较绝对的士兵数量可能并不是非常有效的。但是，通过人物掌权的年份来标准化他们可能会提供更好的见解，并且产生更有预测力的特征。</p>
<p>在分类特征的特征设计上值得单独的写一篇博客文章，因为有很多方式可以去处理它们。特别是对于缺失值的插补，请看一看以前的博客文章—— <a href="http://nerds.airbnb.com/overcoming-missing-values-in-a-rfc/" target="_blank" rel="external">使用随机森林分类器处理缺失值</a>。</p>
<p>转换分类特征最常见的方法就是矢量化（也称作one-hot encoding）。然而，在处理有许多不同级别的分类特征时，使用条件概率编码（CP-coding）则更为实用。</p>
<p>CP-coding的基本思想就是在给定的分类级别上，计算出某个特征值发生的概率。这种方法使得我们能够将所有级别的分类特征转化为一个单一的数值型变量。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a3b87ef92.jpg" alt=""></p>
<p>然而，这种类型转换可能会因为没有充分描述的类别而造成噪音数据。在上面的例子中，我们只有一个来自House 为 “Tallight”的观测样本。结果相应的概率就是0或1。为了避免这种问题的发生并且降低噪声数据，通常情况下，可以通过考虑加权平均值，全局概率或者引入一个平滑的超系数来调整如何计算概率。</p>
<p>那么，哪一种方法最好呢？这取决于分类特征的数量和级别。CP-coding是个不错的选择，因为他降低了特征的维数，但是这样会牺牲掉特征与特征之间的互信息，这种方法称之为矢量化保留。此外，我们可以整合这两种方法，即组合相似的类别特征，然后使用CP-coding处理整合的特征。</p>
<h3 id="模型性能评估">模型性能评估</h3><p>当谈及到评估模型性能的时候，我们需要留意正面角色和反面角色的比例。在我们的例子模型中，数据最后的统计格式为[character*period]（下表左）。然而，模型评估应该以角色类别测量（下表右）。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a3fb5a353.jpg" alt=""></p>
<p>结果，在模型的构建数据和模型的评估数据之间的正面人物和反面人物的比例有着明显的差异。当评估模型准确率和召回率的时候分配合适的权重值是相当重要的。</p>
<p>此外，因为我们可能会使用下采样以减少观测样本的数量，所以我们还需要调整模型占采样过程的准确率和召回率。</p>
<h3 id="评估准确率和召回率">评估准确率和召回率</h3><p>对于模型评估的两种主要的评估度量是准确率（Precision）和召回率（Recall）。在我们的例子当中，准确率是预测结果为反面角色中被正确预测为反面角色的比例。它在给定的阈值下衡量模型的准确度。另外，召回率是模型从原本为反面角色当中能够正确检测出为反面角色的比例。它在一个给定的阈值下以识别反面人物来衡量模型的综合指标。这两个变量很容易混淆，所以通过下表会更加的直观看出两者的不同。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a428441f6.jpg" alt=""></p>
<p>通常将最后的数据划分为四个不同的部分：</p>
<p>True Positives（TP）：角色是反面人物，模型预测为反面人物；<br>False Positives（FP）：角色是正面人物，模型预测为反面人物；<br>True Negatives（TN）：角色是正面人物，模型预测为正面人物；<br>False Negatives（FN）：角色是反面人物，模型预测为正面人物；<br>准确率计算：在所有被预测为反面人物中，模型正确预测的比例，即TP /（TP + FP）。</p>
<p>召回率计算：在所有原本就是反面人物中，模型正确预测的比例，即TP / (TP + FN）。</p>
<p>通过观察可以看出，尽管准确率和召回率的分子是相同的，但分母不同。</p>
<p>通常在选择高准确率和高召回率之间总有一种权衡。这要取决于构建模型的最终目的，对于某些情况而言，高准确率的选择可能会优于高召回率。然而，对于欺诈预测模型，通常要偏向于高召回率，即使会牺牲掉一些准确率。</p>
<p>有许多的方式可以用来改善模型的准确度和召回率。其中包括添加更好的特征，优化决策树剪枝或者建立一个更大的森林等等。不过，鉴于讨论广泛，我打算将其单独地放在一篇文章当中。</p>
<h3 id="结束语">结束语</h3><p>希望这篇文章能让读者了解到什么是构建机器学习模型所需要的。遗憾的是，没有放之四海而皆准的解决方案来构建一种好的模型，充分了解数据的上下文是关键，因为通过它我们能够从中提取出更多更好的预测特征，从而建立出更优化的模型。</p>
<p>最后，虽然将角色分为正面和反面是主观的，但类别标签的确是机器学习的一个非常重要的部分，而不好的类别标签通常会导致一个糟糕的模型。祝建模快乐!</p>
<p>注：这个模型确保每个角色都是正面角色或者是反面角色，即如果他们生来就是反面角色，那么在他们的整个生命当中都是反面角色。如果我们假设角色可以跨越类别标签作为中立人物，那么模型的设计将会完全不同。</p>
<p>英文原文： <a href="http://nerds.airbnb.com/designing-machine-learning-models/" target="_blank" rel="external">Designing Machine Learning Models: A Tale of Precision and Recall</a>（译者/刘帝伟 审校/刘翔宇、朱正贵 责编/周建丁）</p>
<p>关于译者： <a href="http://my.csdn.net/Dream_angel_Z" target="_blank" rel="external">刘帝伟</a>，中南大学在读研究生，关注机器学习、数据挖掘及生物信息领域。 </p>
<hr>
<p>本文为CSDN编译整理，未经允许不得转载，如需转载请联系market#csdn.net(#换成@)</p>
]]></content>
    <summary type="html">
    <![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>


<p>Airbnb网站基于允许任何人将闲置的房屋进行长期或短期出租构建商业模式，来自房客或房东的欺诈风险是必须解决的问题。Airbnb信任和安全小组通过构建机器学习模型进行欺诈预测，本文介绍了其设计思想。假想模型是预测某些虚拟人物是否为“反面人物”，基本步骤：构建模型预期，构建训练集和测试集，特征学习，模型性能评估。其中特征转换倾向于采用条件概率编码（CP-coding），评估度量是准确率（Precision）和召回率（Recall），通常偏向于高召回率。<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="译文" scheme="http://csuldw.github.io/categories/%E8%AF%91%E6%96%87/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[一个简单的Python函数运行时间计时器]]></title>
    <link href="http://csuldw.github.io/2015/07/16/2015-07-16%20Python%20timer/"/>
    <id>http://csuldw.github.io/2015/07/16/2015-07-16 Python timer/</id>
    <published>2015-07-16T12:24:25.000Z</published>
    <updated>2015-10-25T13:13:10.036Z</updated>
    <content type="html"><![CDATA[<p>在实际开发中，往往想要计算一段代码运行多长时间，下面我将该功能写入到一个函数里面，只要在每个函数前面调用该函数即可，见下面代码：</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#--------------------------------</span></span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> wraps  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fun_timer</span><span class="params">(function)</span>:</span></span><br><span class="line">    <span class="decorator">@wraps(function)</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">function_timer</span><span class="params">(*args, **kwargs)</span>:</span></span><br><span class="line">        t0 = time.time()</span><br><span class="line">        result = function(*args, **kwargs)</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        os.system(<span class="string">" echo Total time running %s: %s seconds"</span> % (function.func_name, str(t1-t0)) + <span class="string">" &gt;&gt; timecount.log"</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> function_timer</span><br><span class="line"><span class="comment">#-----------------------------------</span></span><br></pre></td></tr></table></figure>
<p>说明：<font color="green"><strong>一个记时器，只要在函数前面写上@fun_timer即可</strong></font>.</p>
<hr>
]]></content>
    <summary type="html">
    <![CDATA[<p>在实际开发中，往往想要计算一段代码运行多长时间，下面我将该功能写入到一个函数里面，只要在每个函数前面调用该函数即可，见下面代码：</p>]]>
    
    </summary>
    
      <category term="Python" scheme="http://csuldw.github.io/tags/Python/"/>
    
      <category term="函数计时器" scheme="http://csuldw.github.io/tags/%E5%87%BD%E6%95%B0%E8%AE%A1%E6%97%B6%E5%99%A8/"/>
    
      <category term="Python" scheme="http://csuldw.github.io/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[开发者成功使用机器学习的十大诀窍(译)]]></title>
    <link href="http://csuldw.github.io/2015/07/13/2015-07-13-10%20keys%20to%20successful%20machine%20learning%20for%20developers/"/>
    <id>http://csuldw.github.io/2015/07/13/2015-07-13-10 keys to successful machine learning for developers/</id>
    <published>2015-07-13T13:53:12.000Z</published>
    <updated>2015-07-24T01:53:30.000Z</updated>
    <content type="html"><![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>

<p>在提供发现埋藏数据深层的模式的能力上，机器学习有着潜在的能力使得应用程序更加的强大并且更能响应用户的需求。精心调校好的算法能够从巨大的并且互不相同的数据源中提取价值，同时没有人类思考和分析的限制。对于开发者而言，机器学习为应用业务的关键分析提供了希望，从而实现从改善客户体验到提供产品推荐上升至超个性化内容服务的任何应用程序。<br><a id="more"></a><br>像Amazon和Micorosoft这样的云供应商提供云功能的机器学习解决方案，承诺为开发者提供一个简单的方法，使得机器学习的能力能够融入到他们的应用程序当中，这也算是最近的头条新闻了。承诺似乎很好，但开发者还需谨慎。</p>
<p>对于开发人员而言，基于云的机器学习工具带来了使用机器学习创造和提供新的功能的可能性。然而，当我们使用不当时，这些工具会输出不好的结果，用户可能会因此而感到不安。测试过<a href="http://how-old.net/" target="_blank" rel="external">微软年龄检测机器学习工具</a>的人都会发现，伴随即插即用的易用性而来的是主要的精度问题——对于关键应用程序或者是重大决策，它应该不值得信赖。</p>
<p>想要在应用程序中成功地融入机器学习的开发者，需要注意以下的一些关键要点：</p>
<p><strong>1.算法使用的数据越多，它的精度会更加准确，所以如果可能要尽量避免抽样。</strong>机器学习理论在预测误差上有着非常直观的描述。简而言之，在机器学习模型和最优预测（在理论上达到最佳可能的误差）之间的预测误差的差距可以被分解为三个部分：</p>
<ul>
<li>由于没有找到正确函数形式的模型的误差</li>
<li>由于没有找到最佳参数的模型的误差</li>
<li>由于没用使用足够数据的模型的误差</li>
<li>如果训练集有限，它可能无法支撑解决这个问题所需的模型复杂性。统计学的基本规律告诉我们，如果我们可以的话，应该利用所有的数据而不是抽样。</li>
</ul>
<p><strong>2.对给定的问题选择效果最好的机器学习算法是决定成败的关键。</strong>例如，梯度提升树（GBT）是一个非常受欢迎的监督学习算法，由于其精度而被业内开发人员广泛使用。然而，尽管其高度受欢迎，我们也不能盲目的把这种算法应用于任何问题上。相反，我们使用的算法应该是能够最佳地拟合数据特征同时能够保证精度的算法。</p>
<p>为了证明这个观点，尝试做这样一个实验，在数据集 <a href="http://www.daviddlewis.com/resources/testcollections/rcv1/" target="_blank" rel="external">the popular text categorization dataset rcv1</a>上测试GBT算法和线性支持向量机（SVM）算法，并比较两者的精度。我们观察到在这个问题上，就错误率而言，线性SVM要优于GBT算法。这是因为在文本领域当中，数据通常是高维的。一个线性分类器能够在N-1维当中完美的分离出N个样本，所以，一个样本模型在这种数据上通常表现的更好。此外，模型越简单，通过利用有限的训练样本来避免过拟合的方式学习参数，并且提供一个精确的模型，产生的问题也会随之越少。</p>
<p>另一方面，GBT是高度非线性的并且更加强大，但是在这种环境中却更难学习并且更容易发生过拟合，往往结果精度也较低。</p>
<p><strong>3.为了得到一个更好的模型，必须选择最佳的的算法和相关的参数。</strong>这对于非数据科学家而言可能不容易。现代的机器学习算法有许多的参数可以调整。例如，对于流行的GBT算法单独的就有十二个参数可以设置，其中包括如何控制树的大小，学习率，行或列的采样方法，损失函数，正则化选项等等。一个特有的项目需要在给定的数据集上为每一个参数找到其最优值并且达到最精准的精度，这确实不是一件容易的事。但是为了得到最佳的结果，数据科学家需要训练大量的模型，而直觉和经验会帮助他们根据交叉验证的得分，然后决定使用什么参数再次尝试。</p>
<p><strong>4.机器学习模型会随着好的数据而变得更好，错误的数据收集和数据处理会降低你建立预测和归纳的机器学习模型的能力。</strong>根据经验，建议仔细审查与主题相关的数据，从而深入了解数据和幕后数据的生成过程。通常这个过程可以识别与记录、特征、值或采样相关的数据质量问题。</p>
<p><strong>5.理解数据特征并改进它们（通过创造新的特征或者去掉某个特征）对预测能力有着高度的影响。</strong>机器学习的一个基本任务就是找到能够被机器学习算法充分利用的丰富特征空间来替代原始数据。例如，特征转换是一种流行的方法，可以通过在原始数据的基础上使用数学上的转换提取新的特征来实现。最后的特征空间（也就是最后用来描述数据的特征）要能更好的捕获数据的多复杂性（如非线性和多种特征之间的相互作用），这对于成功的学习过程至关重要。</p>
<p><strong>6.在应用中，选择合适的灵感来自商业价值的目标函数/损失函数对于最后的成功至关重要。</strong>几乎所有的机器学习算法最后都被当成是一种优化问题。根据业务的性质，合理设置或调整优化的目标函数，是机器学习成功的关键。</p>
<p>以支持向量机为例，通过假设所有错误类型的权重相等，对一个二分类问题的泛化误差进行了优化。这对损失敏感的问题并不合适，如故障检测，其中某些类型的错误比重可能比其它类型的要高。在这种情况下，建议通过在特定的错误类型上，增加更多的惩罚来解释它们的权重，从而调整SVM的损失函数。</p>
<p><strong>7.确保正确地处理训练数据和测试数据，如此当在生产中部署该模型时，测试数据能够模拟输入数据。</strong>例如，我们可以看到，这对于时间依赖性数据是多么的重要。在这种情况下，使用标准的交叉验证方法进行训练，调整，那么测试模型的结果可能会有偏差，甚至会不准确。这是因为在实施平台上它不能准确的模拟输入数据的性质。为了纠正这一点，在部署时我们必须仿照模型来部署使用。我们应该使用一个基于时间的交叉验证，用时间较新的数据来验证训练模型。</p>
<p><strong>8.部署前理解模型的泛化误差。泛化误差衡量模型在未知数据上的性能好坏。</strong>因为一个模型在训练数据上的性能好并不意味着它在未知的数据上的表现也好。一个精心设计的模拟实际部署使用的模型评估过程，是估计模型泛化误差所需要的。</p>
<p>一不留心就很容易违反交叉验证的规则，并且也没有一种显而易见的方法来表现交叉验证的非正确性，通常在你试图寻找快捷方式计算时发生。在任何模型部署之前，有必要仔细注意交叉验证的正确性，以获得部署性能的科学评估。</p>
<p><strong>9.知道如何处理非结构化和半结构化数据，如文本、时间序列、空间、图形或者图像数据。</strong>大多数机器学习算法在处理特征空间中的数据时，一个特征集代表一个对象，特征集的每一个元素都描述对象的一个特点。在实际当中，数据引进时并不是这种格式化的形式，往往来自于最原始的格式，并且最后都必须被改造成机器学习算法能够识别的理想格式。比如，我们必须知道如何使用各种计算机视觉技术从图像中提取特征或者如何将自然语言处理技术应用于影片文本。</p>
<p><strong>10.学会将商业问题转换成机器学习算法。</strong>一些重要的商业问题，比如欺诈检测、产品推荐、广告精准投放，都有“标准”的机器学习表达形式并且在实践当中取得了合理的成就。即使对于这些众所周知的问题，也还有鲜为人知但功能更强大的表达形式，从而带来更高的预测精度。对于一般在博客和论坛中讨论的小实例的商业问题，适当的机器学习方法则不太明显。</p>
<p>如果你是一个开发者，学习这十个通往成功的诀窍可能似乎是一个艰难的任务，但是不要气馁。事实上，开发者不是数据科学家。认为开发人员可以充分利用所有的机学习工具是不公平的。但是这并不意味着开发人员没有机会去学习一些有水准的数据科学从而改进他们的应用。随着适当的企业解决方案和自动化程度的提高，开发人员可以做模型构建到实施部署的一切事情，使用机器学习最佳实践来保持高精度。</p>
<p>自动化是在应用程序中扩展机器学习的关键。即使你能够供得起一批小的数据科学家团队和开发者携手合作，也没有足够的人才。像Skytree的AutoModel（自动化模型）能够帮助开发者自动地确定最佳的参数并且使得算法得到最大的模型精度。一个易于使用的接口可以引导开发人员通过训练加工，调整并且测试模型来防止统计上的错误。</p>
<p>自动化机器学习过程，有许多方式，包括数据科学家或开发者的人工智能原理，允许算法去思考，学习并且承受更多的建模重任。也就是说，认为数据科学家能够从机器学习中解耦是错误的，特别是在关键任务模型上。谨防这种能够简单使用机器学习功能的承诺，即能够在不需要正确复杂的思考下或者可扩展的应用技术下就使用机器学习——这通常并不会得到高预测精度和机器学习提供的高商业价值结果。更糟糕的是，在应用程序中使用不好的模型实际上可能会适得其反，并迅速在其用户之间建立不信任的产品或服务。</p>
<p>英文原文： <a href="http://www.infoworld.com/article/2943862/application-development/what-developers-need-to-know-about-machine-learning.html" target="_blank" rel="external">10 keys to successful machine learning for developers</a> （译者/<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a> 审校/刘翔宇、朱正贵 责编/周建丁）</p>
<p>作者简介：Alexander Gray，Skytree首席技术官，佐治亚理工学院计算机学院副教授，主要致力于大规模数据集的机器学习算法技术研发，1993年开始在NASA喷气推进实验室机器学习系统小组从事大规模科学数据的工作。</p>
<p>译者简介： <a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>，中南大学软件学院在读研究生，关注机器学习、数据挖掘及生物信息领域。</p>
<p>【预告】<a href="http://ccai2015.csdn.net/" target="_blank" rel="external">首届中国人工智能大会（CCAI 2015）</a>将于7月26-27日在北京友谊宾馆召开。机器学习与模式识别、大数据的机遇与挑战、人工智能与认知科学、智能机器人四个主题专家云集。人工智能产品库将同步上线，预约咨询：QQ：1192936057。欢迎关注。</p>
<p>本文为CSDN编译整理，未经允许不得转载，如需转载请联系market#csdn.net(#换成@)</p>
]]></content>
    <summary type="html">
    <![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>

<p>在提供发现埋藏数据深层的模式的能力上，机器学习有着潜在的能力使得应用程序更加的强大并且更能响应用户的需求。精心调校好的算法能够从巨大的并且互不相同的数据源中提取价值，同时没有人类思考和分析的限制。对于开发者而言，机器学习为应用业务的关键分析提供了希望，从而实现从改善客户体验到提供产品推荐上升至超个性化内容服务的任何应用程序。<br>]]>
    
    </summary>
    
      <category term="AI" scheme="http://csuldw.github.io/tags/AI/"/>
    
      <category term="CV" scheme="http://csuldw.github.io/tags/CV/"/>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据预处理-PDB文件]]></title>
    <link href="http://csuldw.github.io/2015/07/07/2015-07-07-PDB/"/>
    <id>http://csuldw.github.io/2015/07/07/2015-07-07-PDB/</id>
    <published>2015-07-07T15:23:23.000Z</published>
    <updated>2015-10-29T13:05:58.976Z</updated>
    <content type="html"><![CDATA[<p>以下代码为个人原创，python实现，是处理PDB文件的部分常用代码，仅供参考！</p>
<h3 id="1-下载PDB文件">1.下载PDB文件</h3><p>下面是一个下载PDB文件的函数，传入的参数是一个写有pdb名字的namefile文件，函数的核心部分是三个系统命令，先通过wget下载，然后解压，最后替换名字。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">downloadpdb</span><span class="params">(namefile)</span>:</span></span><br><span class="line">    inputfile = open(namefile, <span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> eachline <span class="keyword">in</span> inputfile:</span><br><span class="line">        pdbname = eachline.lower().strip()</span><br><span class="line">        os.system(<span class="string">"wget http://ftp.wwpdb.org/pub/pdb/data/structures/all/pdb/pdb"</span> + pdbname + <span class="string">".ent.gz"</span>)</span><br><span class="line">        os.system(<span class="string">"gzip -d pdb"</span> + pdbname + <span class="string">'.ent.gz'</span>)</span><br><span class="line">        os.system(<span class="string">"mv pdb"</span> + pdbname + <span class="string">".ent "</span> + pdbname.upper() + <span class="string">'.pdb'</span>)</span><br></pre></td></tr></table></figure>
<p>测试用例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">'/ifs/home/liudiwei/datasets/RPdatas'</span>)</span><br><span class="line">downloadpdb(<span class="string">'protein.name'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-PDB转DSSP">2.PDB转DSSP</h3><p>将下载的PDB文件转成DSSP文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理一行dssp数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">formatdsspline</span><span class="params">(dsspline)</span>:</span></span><br><span class="line">    eachline  = dsspline</span><br><span class="line">    col = <span class="string">'\t'</span> + eachline[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">5</span>:<span class="number">10</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">10</span>:<span class="number">12</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">12</span>:<span class="number">15</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">15</span>:<span class="number">25</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">25</span>:<span class="number">39</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">29</span>:<span class="number">34</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">34</span>:<span class="number">38</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">38</span>:<span class="number">50</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">50</span>:<span class="number">61</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">61</span>:<span class="number">72</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">72</span>:<span class="number">83</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">83</span>:<span class="number">92</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">92</span>:<span class="number">97</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">97</span>:<span class="number">103</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">103</span>:<span class="number">109</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">109</span>:<span class="number">115</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">115</span>:<span class="number">122</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">122</span>:<span class="number">129</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">129</span>:<span class="number">136</span>]</span><br><span class="line">    <span class="keyword">return</span> col</span><br></pre></td></tr></table></figure>
<p>PDB转DSSP格式，需要DSSP软件</p>
<p>参数：</p>
<ul>
<li>pdbdir: pdb文件目录   </li>
<li>dsspdir: 生成的dssp文件目录（需创建）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdbToDSSP</span><span class="params">(pdbnamefile,pdbdir, dsspdir)</span>:</span>    </span><br><span class="line">    pdbfiles = os.listdir(pdbdir)</span><br><span class="line">    <span class="comment">#对于每个pdb文件，生成对应的dssp文件，并保存在dssp目录下</span></span><br><span class="line">    <span class="keyword">for</span> pdb_file <span class="keyword">in</span> pdbfiles:</span><br><span class="line">        pdb_name = pdb_file.split(<span class="string">'.'</span>)[<span class="number">0</span>].upper()</span><br><span class="line">        command = <span class="string">'DSSPCMBI.EXE -x '</span> + pdbdir +<span class="string">'/'</span>+ pdb_file + <span class="string">'  '</span>+ dsspdir +<span class="string">"/"</span>+ pdb_name +<span class="string">'.dssp'</span></span><br><span class="line">        os.system(command) </span><br><span class="line">    dsspfiles = os.listdir(dsspdir)</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(dsspdir + <span class="string">"/DSSP"</span>):      <span class="comment">#判断DSSP文件是否存在，存在则删除</span></span><br><span class="line">        dsspfiles.remove(<span class="string">"DSSP"</span>)</span><br><span class="line">    output=open(dsspdir + <span class="string">'/DSSP'</span>,<span class="string">'w'</span>)</span><br><span class="line">    <span class="comment">#循环读取dssp文件，将其合并成一个整的DSSP</span></span><br><span class="line">    <span class="keyword">with</span> open(pdbnamefile, <span class="string">'r'</span>) <span class="keyword">as</span> namefile:</span><br><span class="line">        <span class="keyword">for</span> eachline <span class="keyword">in</span> namefile:</span><br><span class="line">            pdb_name = eachline.strip() </span><br><span class="line">            dssp_file = pdb_name + <span class="string">'.dssp'</span></span><br><span class="line">        <span class="comment">#for dssp_file in dsspfiles:</span></span><br><span class="line">            <span class="comment">#pdb_name = dssp_file.split('.')[0]</span></span><br><span class="line">            <span class="keyword">with</span> open(dsspdir + <span class="string">'/'</span> + dssp_file,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(dsspdir + <span class="string">'/format'</span>):</span><br><span class="line">                    os.mkdir(dsspdir + <span class="string">'/format'</span>)</span><br><span class="line">                <span class="keyword">with</span> open(dsspdir + <span class="string">'/format/'</span> + pdb_name + <span class="string">'.dssp.format'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> singleOut:</span><br><span class="line">                    count = <span class="number">0</span>; preRes=[]</span><br><span class="line">                    sets = set(<span class="string">''</span>);content=<span class="string">''</span>   </span><br><span class="line">                    <span class="keyword">for</span> eachline <span class="keyword">in</span> f.readlines():</span><br><span class="line">                        list1=[];oneline=[]</span><br><span class="line">                        count+=<span class="number">1</span></span><br><span class="line">                        list1.append(pdb_name)                             </span><br><span class="line">                        <span class="keyword">if</span> count &gt;= <span class="number">29</span>:</span><br><span class="line">                            eachline = formatdsspline(eachline)</span><br><span class="line">                            oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> oneline[<span class="number">3</span>].strip():</span><br><span class="line">                                preRes = oneline[<span class="number">3</span>].strip()                        </span><br><span class="line">                            </span><br><span class="line">                            list1.append(eachline)</span><br><span class="line">                            content += <span class="string">""</span>.join(list1)+<span class="string">'\n'</span>                            </span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'!'</span> == oneline[<span class="number">4</span>].strip():</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'!*'</span> <span class="keyword">in</span> eachline <span class="keyword">or</span> <span class="keyword">not</span> oneline[<span class="number">3</span>].strip():</span><br><span class="line">                                <span class="keyword">if</span> preRes <span class="keyword">in</span> sets <span class="keyword">and</span> len(sets):</span><br><span class="line">                                    content=<span class="string">''</span></span><br><span class="line">                                    preRes=[]</span><br><span class="line">                                    <span class="keyword">continue</span></span><br><span class="line">                                sets.add(preRes)</span><br><span class="line">                                output.write(content)</span><br><span class="line">                                singleOut.write(content)</span><br><span class="line">                                content=<span class="string">''</span></span><br><span class="line">                    <span class="keyword">if</span> preRes <span class="keyword">and</span> preRes <span class="keyword">not</span> <span class="keyword">in</span> sets:</span><br><span class="line">                        output.write(content)</span><br><span class="line">                        singleOut.write(content)</span><br><span class="line">    output.close()</span><br></pre></td></tr></table></figure>
<p>测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#test</span></span><br><span class="line">pdbdir = <span class="string">'z:/datasets/protein/pdb'</span></span><br><span class="line">dsspdir = <span class="string">'Z:/datasets/protein/DSSPdir'</span> </span><br><span class="line">proname = <span class="string">'Z:/datasets/protein/protein.name'</span></span><br><span class="line">pdbToDSSP(proname,pdbdir, dsspdir)</span><br></pre></td></tr></table></figure>
<h3 id="3-DSSP抽取序列">3.DSSP抽取序列</h3><p>从一个整合的DSSP文件中抽取序列文件 </p>
<p>从格式化后的dssp文件获取序列信息</p>
<p>参数：dsspfile为格式过的DSSP文件,seqfile为输出的序列文件,同时输出序列文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSeqFromDSSP</span><span class="params">(dsspfile, seqfile, minLen)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(dsspfile, <span class="string">'r'</span>) <span class="keyword">as</span> inputfile:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> seqfile.strip():</span><br><span class="line">            seqfile = <span class="string">'protein'</span>+minLen + <span class="string">'.dssp.seq'</span></span><br><span class="line">        outchain = open(<span class="string">'protein40.chain.all'</span>, <span class="string">'w'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(seqfile, <span class="string">'w'</span>) <span class="keyword">as</span> outputfile:</span><br><span class="line">            residue=[];Ntype=[]</span><br><span class="line">            preType=[];preRes=[]</span><br><span class="line">            firstline=[];secondline=[];content=<span class="string">''</span></span><br><span class="line">            <span class="keyword">for</span> eachline <span class="keyword">in</span> inputfile:</span><br><span class="line">                oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line">                residue = oneline[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> residue.strip(): </span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                Ntype = oneline[<span class="number">3</span>].strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> Ntype.strip():</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> preRes!=residue:</span><br><span class="line">                    content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">                    <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span> <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:</span><br><span class="line">                        outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                        outputfile.write(content)</span><br><span class="line">                    firstline=[]</span><br><span class="line">                    firstline.append(<span class="string">'&gt;'</span> + residue + <span class="string">':'</span> + Ntype)</span><br><span class="line">                    secondline=[];secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">                    preRes = residue;preType = Ntype</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> Ntype != preType:</span><br><span class="line">                    content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">                    <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span>  <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:</span><br><span class="line">                        outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                        outputfile.write(content)</span><br><span class="line">                    firstline=[]</span><br><span class="line">                    firstline.append(<span class="string">'&gt;'</span> + residue + <span class="string">':'</span> + Ntype)</span><br><span class="line">                    secondline=[];secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">                    preRes = residue;preType = Ntype</span><br><span class="line">                <span class="keyword">else</span>: <span class="comment">#如果Ntype不为空，且等于preType</span></span><br><span class="line">                    secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">            content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">            <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span> <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:  <span class="comment">#选择长度大于40</span></span><br><span class="line">                outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                outputfile.write(content)</span><br><span class="line">        outchain.close()</span><br></pre></td></tr></table></figure>
<p>测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">r"E:\3-CSU\Academic\_Oriented\analysis\experiment\Datasets\ptest.dssp"</span>)</span><br><span class="line">pdbfile = <span class="string">'DSSP'</span></span><br><span class="line">outfile = <span class="string">'protein.seq'</span></span><br><span class="line">getSeqFromDSSP(pdbfile,outfile)</span><br></pre></td></tr></table></figure>
<h3 id="4-对序列做blast聚类">4.对序列做blast聚类</h3><p>设置相应的参数，在服务器上跑blast，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifs/share/lib/cd-hit-v4.5.4/cd-hit -i /ifs/home/liudiwei/datasets/protein40.seq -o /ifs/home/liudiwei/experiment/cdhit/fasta.40 -c <span class="number">0.4</span> -n <span class="number">2</span> -M <span class="number">2000</span></span><br></pre></td></tr></table></figure>
<h3 id="5-生成聚类后的DSSP，得到protein-name、protein-seq、protein-chain三个文件">5.生成聚类后的DSSP，得到protein.name、protein.seq、protein.chain三个文件</h3><p>从原来的DSSP文件中，根据聚类后的链名抽取新的DSSP文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># extract dssp from old dssp file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractDSSP</span><span class="params">(dsspfile, chainname, outfile)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(outfile, <span class="string">'w'</span>) <span class="keyword">as</span> outdssp:</span><br><span class="line">        <span class="keyword">with</span> open(dsspfile, <span class="string">'r'</span>) <span class="keyword">as</span> inputdssp:</span><br><span class="line">            <span class="keyword">for</span> eachline <span class="keyword">in</span> inputdssp:</span><br><span class="line">                oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line">                <span class="comment">#preNum = oneline[2].strip()     </span></span><br><span class="line">                <span class="keyword">with</span> open(chainname,<span class="string">'r'</span>) <span class="keyword">as</span> chainfile:       </span><br><span class="line">                    <span class="keyword">for</span> eachchain <span class="keyword">in</span> chainfile:</span><br><span class="line">                        protein_ame = eachchain[<span class="number">1</span>:<span class="number">5</span>]</span><br><span class="line">                        chain_id = eachchain[<span class="number">6</span>:<span class="number">7</span>]</span><br><span class="line">                        <span class="keyword">if</span> oneline[<span class="number">0</span>].strip() == protein_ame <span class="keyword">and</span> oneline[<span class="number">3</span>].strip() == chain_id:</span><br><span class="line">                            outdssp.write(eachline)</span><br><span class="line">                            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>测试实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dsspfile = <span class="string">'/ifs/home/liudiwei/datasets/832.protein/DSSPdir1/DSSP'</span></span><br><span class="line">chainname = <span class="string">'/ifs/home/liudiwei/experiment/step1/832p.cluster/cdhit/protein.chain'</span></span><br><span class="line">outfile = <span class="string">'/ifs/home/liudiwei/experiment/step1/832p.cluster/cdhit/DSSP'</span></span><br><span class="line">extractDSSP(dsspfile, chainname, outfile )</span><br></pre></td></tr></table></figure>
<hr>
<center><br><strong>本栏目持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong><br></center>]]></content>
    <summary type="html">
    <![CDATA[<p>以下代码为个人原创，python实现，是处理PDB文件的部分常用代码，仅供参考！</p>
<h3 id="1-下载PDB文件">1.下载PDB文件</h3><p>下面是一个下载PDB文件的函数，传入的参数是一个写有pdb名字的namefile文件，函数的核心部分是三个系统命令，先通过wget下载，然后解压，最后替换名字。</p>]]>
    
    </summary>
    
      <category term="BioInfo" scheme="http://csuldw.github.io/tags/BioInfo/"/>
    
      <category term="BioMed" scheme="http://csuldw.github.io/categories/BioMed/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-Adaboost]]></title>
    <link href="http://csuldw.github.io/2015/07/05/2015-07-12-Adaboost/"/>
    <id>http://csuldw.github.io/2015/07/05/2015-07-12-Adaboost/</id>
    <published>2015-07-05T14:53:12.000Z</published>
    <updated>2015-10-29T13:06:03.803Z</updated>
    <content type="html"><![CDATA[<p><strong>本章内容</strong></p>
<ul>
<li>组合相似的分类器来提高分类性能</li>
<li>应用AdaBoost算法</li>
<li>处理非均衡分类问题</li>
</ul>
<p><strong>主题：</strong>利用AdaBoost元算法提高分类性能</p>
<a id="more"></a>
<h3 id="1-基于数据集多重抽样的分类器">1.基于数据集多重抽样的分类器</h3><table>
<thead>
<tr>
<th>Feature</th>
<th>AdaBoost</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>泛化错误率低，易编码，可以应用在大部分分类器上，无需参数调整</td>
</tr>
<tr>
<td>缺点</td>
<td>对离群点敏感</td>
</tr>
<tr>
<td>数据</td>
<td>数值型和标称型数据</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>bagging:基于数据随机重抽样的分类器构建方法</p>
<p>自举汇聚法(bootstrap aggregating),也称为bagging方法，是从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原始数据集的大小相等。每个数据集都是通过在原始数据集中随机选择一个本来进行替换而得到的。</p>
<p>在S个数据集建好之后，将某个学习算法分别作用域每个数据集得到了S个分类器。当我们对新数据进行分类时，就可以应用S个分类器进行分类。与此同时，选择分类器投票结果最多的类别作为最后的分类结果。</p>
<p>有一些比较先进的bagging方法，如<strong>随机森林</strong>（RF）。</p>
<p>boosting是一种与bagging很类似的技术。不论是boosting还是bagging当中，当使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过训练集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>boosting方法有多个版本，当前最流行便属于<strong>AdaBoost</strong>。</p>
<p><strong>AdaBoost的一般流程</strong></p>
<p>（1）收集数据：可以使用任何方法；<br><br>（2）准备数据：依赖于所使用的若分类器类型；<br><br>（3）分析数据：可以使用任意方法<br><br>（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练若分类器；<br><br>（5）测试算法：计算分类的错误率；<br><br>（6）使用算法：同SVM一样，AdaBoost预测的两个类别中的一个，如果想要把它应用到多个类的场合，那么就像多类SVM中的做法一样对AdaBoost进行修改。</p>
<h3 id="2-训练算法：基于错误提升分类器的性能">2.训练算法：基于错误提升分类器的性能</h3><p>AdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程：训练集中的每个样本，赋予其一个权重，这些权重构成向量D。一开始，这些权重都初试化成相等值。首先在训练数据上训练处一个若分类器并计算该分类器的错误率，然后在同一数据集上再次训练若分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分队的样本的权重值将会降低，而第一次分错的样本的权重将会提高。为了从所有分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个分类器的错误率进行计算的。其中错误率定义为</p>
<p>$$\epsilon=\dfrac{为正确分类的样本数目}{所有样本数目}$$</p>
<p>alpha计算公式</p>
<p>$$\alpha=\dfrac{1}{2}ln(\dfrac{1-\epsilon}{\epsilon})$$</p>
<p>计算出alpha值之后，可以对权重向量D进行更新，使得正确分类的样本的权重值降低而分错的样本权重值升高，D的计算方法如下<br>如果某个样本被正确分类，更新该样本权重值为：</p>
<p>$$D^{(t+1)}_i=\dfrac{D_i^{(t)} e^{-\alpha}}{Sum(D)}$$</p>
<p>如果某个样本被错误分类，更新该样本的权重值为：</p>
<p>$$D^{(t+1)}_i=\dfrac{D_i^{(t)} e^{\alpha}}{Sum(D)}$$</p>
<p>计算出D后，AdaBoost接着开始下一轮的迭代。AdaBoost算法会不断地重复训练和调整权重的过程，知道训练错误率为0或者若分类器的数目达到用户指定值为止。</p>
<p>在建立完整的AdaBoost算法之前，需要通过一些代码建立若分类器及保存数据集的权重。</p>
<h3 id="3-基于单层决策树构建若分类器">3.基于单层决策树构建若分类器</h3><p>单层决策树是一种简单的决策树。首先构建一个简单的数据集,建立一个adaboost.py文件并加入下列代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadSimpData</span><span class="params">()</span>:</span></span><br><span class="line">    datMat = matrix([[ <span class="number">1.</span> ,  <span class="number">2.1</span>],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.1</span>],</span><br><span class="line">        [ <span class="number">1.3</span>,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">1.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.</span> ]])</span><br><span class="line">    classLabels = [<span class="number">1.0</span>, <span class="number">1.0</span>, -<span class="number">1.0</span>, -<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">    <span class="keyword">return</span> datMat,classLabels</span><br></pre></td></tr></table></figure>
<p>导入数据</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; import adaboost</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; datMat,classLabels=adaboost.loadSimpData()</span><br></pre></td></tr></table></figure>
<p>下面两个函数，一个用于测试是否某个值小于或者大于我们正在测试的阈值，一个会在一个加权数据集中循环，并找到具有最低错误率的单层决策树。</p>
<p>伪代码如下：<br></p>
<pre><code>将最小错误率<span class="keyword">min</span>Error设为无穷大
对数据及中的每一个特征（第一层循环）：
    对每个步长（第二层循环）：
        对每个不等号（第三层循环）：
            建立一颗单层决策树并利用加权数据集对它进行测试
            如果错误率低于<span class="keyword">min</span>Error，则将当前单层决策树设置为最佳单层决策树
返回最佳单层决策树
</code></pre><p>单层决策树生成函数代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMatrix,dimen,threshVal,threshIneq)</span>:</span><span class="comment">#just classify the data</span></span><br><span class="line">    retArray = ones((shape(dataMatrix)[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &lt;= threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &gt; threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> retArray</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(dataArr,classLabels,D)</span>:</span></span><br><span class="line">    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T</span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    numSteps = <span class="number">10.0</span>; bestStump = &#123;&#125;; bestClasEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    minError = inf <span class="comment">#init error sum, to +infinity</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):<span class="comment">#loop over all dimensions</span></span><br><span class="line">        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();</span><br><span class="line">        stepSize = (rangeMax-rangeMin)/numSteps</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(-<span class="number">1</span>,int(numSteps)+<span class="number">1</span>):<span class="comment">#loop over all range in current dimension</span></span><br><span class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment">#go over less than and greater than</span></span><br><span class="line">                threshVal = (rangeMin + float(j) * stepSize)</span><br><span class="line">                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)<span class="comment">#call stump classify with i, j, lessThan</span></span><br><span class="line">                errArr = mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">                errArr[predictedVals == labelMat] = <span class="number">0</span></span><br><span class="line">                weightedError = D.T*errArr  <span class="comment">#calc total error multiplied by D</span></span><br><span class="line">                <span class="comment">#print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError)</span></span><br><span class="line">                <span class="keyword">if</span> weightedError &lt; minError:</span><br><span class="line">                    minError = weightedError</span><br><span class="line">                    bestClasEst = predictedVals.copy()</span><br><span class="line">                    bestStump[<span class="string">'dim'</span>] = i</span><br><span class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</span><br><span class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</span><br><span class="line">    <span class="keyword">return</span> bestStump,minError,bestClasEst</span><br></pre></td></tr></table></figure>
<h3 id="4-AdaBoost算法的实现">4.AdaBoost算法的实现</h3><p>整个实现的伪代码如下：</p>
<pre><code>对每次迭代：
    利用<span class="function"><span class="title">buildStump</span><span class="params">()</span></span>函数找到最佳的单层决策树
    将最佳单层决策树加入到单层决策树数据中
    计算alpha
    计算心的权重向量D
    更新累计类别估计值
    如果错误率低于<span class="number">0.0</span> 则退出循环
</code></pre><p>基于单层决策树的AdaBoost训练过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(dataArr,classLabels,numIt=<span class="number">40</span>)</span>:</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    D = mat(ones((m,<span class="number">1</span>))/m)   <span class="comment">#init D to all equal</span></span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        bestStump,error,classEst = buildStump(dataArr,classLabels,D)<span class="comment">#build Stump</span></span><br><span class="line">        <span class="comment">#print "D:",D.T</span></span><br><span class="line">        alpha = float(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/max(error,<span class="number">1e-16</span>)))<span class="comment">#calc alpha, throw in max(error,eps) to account for error=0</span></span><br><span class="line">        bestStump[<span class="string">'alpha'</span>] = alpha  </span><br><span class="line">        weakClassArr.append(bestStump)                  <span class="comment">#store Stump Params in Array</span></span><br><span class="line">        <span class="comment">#print "classEst: ",classEst.T</span></span><br><span class="line">        expon = multiply(-<span class="number">1</span>*alpha*mat(classLabels).T,classEst) <span class="comment">#exponent for D calc, getting messy</span></span><br><span class="line">        D = multiply(D,exp(expon))                              <span class="comment">#Calc New D for next iteration</span></span><br><span class="line">        D = D/D.sum()</span><br><span class="line">        <span class="comment">#calc training error of all classifiers, if this is 0 quit for loop early (use break)</span></span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="comment">#print "aggClassEst: ",aggClassEst.T</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,<span class="number">1</span>)))</span><br><span class="line">        errorRate = aggErrors.sum()/m</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"total error: "</span>,errorRate</span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>: <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr,aggClassEst</span><br></pre></td></tr></table></figure>
<h3 id="5-测试算法">5.测试算法</h3><p>拥有了多个若分类器以及其对应的alpha值，进行测试就方便了。</p>
<p>AdaBoost分类函数:利用训练处的多个若分类器进行分类的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass,classifierArr)</span>:</span></span><br><span class="line">    dataMatrix = mat(datToClass)<span class="comment">#do stuff similar to last aggClassEst in adaBoostTrainDS</span></span><br><span class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)):</span><br><span class="line">        classEst = stumpClassify(dataMatrix,classifierArr[i][<span class="string">'dim'</span>],\</span><br><span class="line">                                 classifierArr[i][<span class="string">'thresh'</span>],\</span><br><span class="line">                                 classifierArr[i][<span class="string">'ineq'</span>])<span class="comment">#call stump classify</span></span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line">        <span class="keyword">print</span> aggClassEst</span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></figure>
<h3 id="6-绘制ROC曲线">6.绘制ROC曲线</h3><p>ROC曲线绘制代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    cur = (<span class="number">1.0</span>,<span class="number">1.0</span>) <span class="comment">#cursor</span></span><br><span class="line">    ySum = <span class="number">0.0</span> <span class="comment">#variable to calculate AUC</span></span><br><span class="line">    numPosClas = sum(array(classLabels)==<span class="number">1.0</span>)</span><br><span class="line">    yStep = <span class="number">1</span>/float(numPosClas); xStep = <span class="number">1</span>/float(len(classLabels)-numPosClas)</span><br><span class="line">    sortedIndicies = predStrengths.argsort()<span class="comment">#get sorted index, it's reverse</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    fig.clf()</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment">#loop through all the values, drawing a line segment at each point</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> sortedIndicies.tolist()[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">if</span> classLabels[index] == <span class="number">1.0</span>:</span><br><span class="line">            delX = <span class="number">0</span>; delY = yStep;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            delX = xStep; delY = <span class="number">0</span>;</span><br><span class="line">            ySum += cur[<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#draw line from cur to (cur[0]-delX,cur[1]-delY)</span></span><br><span class="line">        ax.plot([cur[<span class="number">0</span>],cur[<span class="number">0</span>]-delX],[cur[<span class="number">1</span>],cur[<span class="number">1</span>]-delY], c=<span class="string">'b'</span>)</span><br><span class="line">        cur = (cur[<span class="number">0</span>]-delX,cur[<span class="number">1</span>]-delY)</span><br><span class="line">    ax.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">'b--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'False positive rate'</span>); plt.ylabel(<span class="string">'True positive rate'</span>)</span><br><span class="line">    plt.title(<span class="string">'ROC curve for AdaBoost horse colic detection system'</span>)</span><br><span class="line">    ax.axis([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the Area Under the Curve is: "</span>,ySum*xStep</span><br></pre></td></tr></table></figure>
<h3 id="References">References</h3><p>【1】Machine Learning in Action 机器学习实战 第七章</p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>本章内容</strong></p>
<ul>
<li>组合相似的分类器来提高分类性能</li>
<li>应用AdaBoost算法</li>
<li>处理非均衡分类问题</li>
</ul>
<p><strong>主题：</strong>利用AdaBoost元算法提高分类性能</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Python模拟ls命令]]></title>
    <link href="http://csuldw.github.io/2015/06/10/2015-06-10%20Python%20simulate%20command/"/>
    <id>http://csuldw.github.io/2015/06/10/2015-06-10 Python simulate command/</id>
    <published>2015-06-10T11:24:00.000Z</published>
    <updated>2015-10-29T13:38:06.081Z</updated>
    <content type="html"><![CDATA[<p>模拟控制台命令</p>
<p>写一个程序 lsrm 要求如下:</p>
<pre><code>模拟linux的命令<span class="keyword">ls</span>部分功能
当使用命令
lsrm -<span class="keyword">ll</span>
显示目录下所有 <span class="keyword">py</span> 结尾的文件
增加难度 (<span class="number">1</span>.使用递归 显示所有目录里的 <span class="keyword">py</span> 结尾文件)
</code></pre><a id="more"></a>
<hr>
<p>首先定义一个outputFile函数，参数只设置一个infile，表示的是文件名或者目录名，然后进行判断，如果是文件，而且以py结尾，则输出；否则，如果是目录，则循环遍历每个每个文件。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="string">"""</span><br><span class="line">Created on Wed Oct 28 19:25:20 2015</span><br><span class="line"></span><br><span class="line">@author: liudiwei</span><br><span class="line">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">outputFile</span><span class="params">(infile)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> os.path.isdir(infile):</span><br><span class="line">        filelist = getFileList(infile)</span><br><span class="line">        <span class="keyword">for</span> eachfile <span class="keyword">in</span> filelist:</span><br><span class="line">            os.chdir(infile)</span><br><span class="line">            outputFile(eachfile)</span><br><span class="line">            os.chdir(<span class="string">".."</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">".py"</span> <span class="keyword">in</span> infile:</span><br><span class="line">            <span class="keyword">print</span> infile</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__==<span class="string">'__main__'</span>:</span><br><span class="line">    filepath = <span class="string">r"F:\CSU\Academic\analysis\experiment\code"</span>; </span><br><span class="line">    filelist = getFileList(filepath)</span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">True</span>:</span><br><span class="line">        command = raw_input(<span class="string">'# '</span> )</span><br><span class="line">        <span class="keyword">if</span> command == <span class="string">'lsrm -ll'</span>:    </span><br><span class="line">            outputFile(filepath)</span><br><span class="line">        <span class="keyword">elif</span> command == <span class="string">"stop"</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>运行结果如下图所示：</p>
<center><br><img src="/assets/images/20151029094653.png" alt="output"><br></center>


<p>注意：在寻找子目录的文件时，需将工作目切换到子目录，档子目录遍历完毕后，再前换到上一层目录os.chdir(“..”).</p>
]]></content>
    <summary type="html">
    <![CDATA[<p>模拟控制台命令</p>
<p>写一个程序 lsrm 要求如下:</p>
<pre><code>模拟linux的命令<span class="keyword">ls</span>部分功能
当使用命令
lsrm -<span class="keyword">ll</span>
显示目录下所有 <span class="keyword">py</span> 结尾的文件
增加难度 (<span class="number">1</span>.使用递归 显示所有目录里的 <span class="keyword">py</span> 结尾文件)
</code></pre>]]>
    
    </summary>
    
      <category term="Python" scheme="http://csuldw.github.io/tags/Python/"/>
    
      <category term="Python" scheme="http://csuldw.github.io/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-Apriori关联分析]]></title>
    <link href="http://csuldw.github.io/2015/06/04/2015-06-04-Apriori/"/>
    <id>http://csuldw.github.io/2015/06/04/2015-06-04-Apriori/</id>
    <published>2015-06-04T13:53:12.000Z</published>
    <updated>2015-10-29T13:05:49.025Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong> 学习一个算法，我们最关心的并不是算法本身，而是一个算法能够干什么，能应用到什么地方。很多的时候，我们都需要从大量数据中提取出有用的信息，从大规模数据中寻找物品间的隐含关系叫做关联分析(association analysis)或者关联规则学习(association rule learning)。比如在平时的购物中，那些商品一起捆绑购买销量会比较好，又比如购物商城中的那些推荐信息，都是根据用户平时的搜索或者是购买情况来生成的。如果是蛮力搜索的话代价太高了，所以Apriori就出现了，就是为了解决这类问题的。</p>
<a id="more"></a>
<p><strong>内容纲要</strong></p>
<ul>
<li>关联分析</li>
<li>Apriori算法理论</li>
<li><p>Apriori实现</p>
<ul>
<li>频繁项集生成</li>
<li>关联规则生成</li>
</ul>
</li>
<li><p>reference</p>
</li>
</ul>
<p><strong>Apriori算法</strong></p>
<ul>
<li>优点：易编码实现</li>
<li>缺点：在大数据集上可能较慢</li>
<li>适合数据类型：数值型或者标称型数据</li>
</ul>
<h3 id="1_关联分析"><strong>1 关联分析</strong></h3><p>说到关联分析，顾名思义的就可以联想到，所谓关联就是两个东西之间存在的某种联系。关联分析最有名的例子是“尿布和啤酒”，以前在美国西部的一家连锁店，店家发现男人们在周四购买尿布后还会购买啤酒。于是他便得出一个推理，尿布和啤酒存在某种关联。但是具体怎么来评判呢？</p>
<p>那么，这里用的是<strong>支持度</strong>和<strong>可信度</strong>来评判!</p>
<p>一个项集的支持度（support）被定义为数据集中包含该数据集的记录所占的比例。可信度或置信度（confidence）是正对一条关联规则来定义的，比如{尿布}-&gt;{啤酒}，这条规则的可信度定义为“支持度{尿布，啤酒}/支持度{尿布}”</p>
<p>比如有规则X=&gt;Y，它的<strong>支持度</strong>可以计算为包含XUY所有商品的交易量相对所有交易量的比例（也就是X和Y同时出现一次交易的概率）。<strong>可信度</strong>定义为包含XUY所有物品的交易量相对仅包含X的交易量的比值，也就是说可信度对应给定X时的条件概率。关联规则挖掘，其目的是自动发起这样的规则，同时计算这些规则的质量。</p>
<p>计算公式如下：</p>
<p>$$支持度=\frac{交易量包含XUY}{交易量}$$</p>
<p>$$可信度=\frac{交易量包含XUY}{交易量包含X}$$</p>
<p>支持度和可信度是用来量化关联分析是否成功的方法。关联分析的目的包括两个：发现频繁项集和发现关联规则。首先我们要找到频繁项集，然后根据频繁项集找出关联规则。下面使用apriori算法来发现频繁项集。</p>
<h3 id="2_Apriori理论"><strong>2 Apriori理论</strong></h3><p><strong>算法的一般过程：</strong></p>
<ul>
<li>收集数据：使用任何方法</li>
<li>准备数据：任意数据类型都可以，因为我们只保存集合</li>
<li>分析数据：使用任何方法</li>
<li>训练算法：使用Apriori算法来找到频繁项集</li>
<li>测试算法：不需要测试过程</li>
<li>使用算法：用于发现频繁项集以及物品之间的关联规则</li>
</ul>
<p><strong>使用Apriori算法，首先计算出单个元素的支持度，然后选出单个元素置信度大于我们要求的数值，比如0.5或是0.7等。然后增加单个元素组合的个数，只要组合项的支持度大于我们要求的数值就把它加到我们的频繁项集中，依次递归。</strong></p>
<p>然后根据计算的支持度选出来的频繁项集来生成关联规则。</p>
<h3 id="3_Apriori实现"><strong>3 Apriori实现</strong></h3><p>首先定义一些算法的辅助函数<br>加载数据集的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    list = [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br><span class="line">    <span class="keyword">return</span> list</span><br></pre></td></tr></table></figure>
<p>根据数据集构建集合C1，该集合是大小为1的所有候选集的集合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    C1 = [] <span class="comment">#C1是大小为1的所有候选项集的集合</span></span><br><span class="line">    <span class="keyword">for</span> transaction <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> transaction:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                C1.append([item])             </span><br><span class="line">    C1.sort()</span><br><span class="line">    <span class="keyword">return</span> map(frozenset, C1)<span class="comment">#use frozen set so we can use it as a key in a dict</span></span><br></pre></td></tr></table></figure>
<p>根据构建出来的频繁项集，选出满足我们需要的大于我们给定的支持度的项集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#D表示数据集，CK表示候选项集，minSupport表示最小的支持度，自己设定</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span>:</span></span><br><span class="line">    ssCnt = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> tid <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> can <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="keyword">if</span> can.issubset(tid):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ssCnt.has_key(can): ssCnt[can]=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>: ssCnt[can] += <span class="number">1</span></span><br><span class="line">    numItems = float(len(D))</span><br><span class="line">    retList = [] <span class="comment">#存储满足最小支持度要求的项集</span></span><br><span class="line">    supportData = &#123;&#125; <span class="comment">#每个项集的支持度字典</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> ssCnt:  <span class="comment">#计算所有项集的支持度</span></span><br><span class="line">        support = ssCnt[key]/numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            retList.insert(<span class="number">0</span>,key)</span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br></pre></td></tr></table></figure>
<h4 id="3-1_频繁项集"><strong>3.1 频繁项集</strong></h4><p>关于频繁项集的产生，我们单独的抽取出来<br>首先需要一个生成合并项集的函数，将两个子集合并的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#LK是频繁项集列表，K表示接下来合并的项集中的单个想的个数&#123;1,2,3&#125;表示k=3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span><span class="params">(Lk, k)</span>:</span> <span class="comment">#creates Ck</span></span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, lenLk): </span><br><span class="line">            L1 = list(Lk[i])[:k-<span class="number">2</span>]; L2 = list(Lk[j])[:k-<span class="number">2</span>] <span class="comment">#前k-2个项相同时，将两个集合合并</span></span><br><span class="line">            L1.sort(); L2.sort()</span><br><span class="line">            <span class="keyword">if</span> L1==L2: <span class="comment">#if first k-2 elements are equal</span></span><br><span class="line">                retList.append(Lk[i] | Lk[j]) <span class="comment">#set union</span></span><br><span class="line">    <span class="keyword">return</span> retList</span><br></pre></td></tr></table></figure>
<p>接着定义生成频繁项集的函数</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#只需要输入数据集和支持度即可</span><br><span class="line">def <span class="function"><span class="title">apriori</span><span class="params">(dataSet, minSupport = <span class="number">0.5</span>)</span></span>:</span><br><span class="line">    C1 = <span class="function"><span class="title">createC1</span><span class="params">(dataSet)</span></span></span><br><span class="line">    D = <span class="function"><span class="title">map</span><span class="params">(set, dataSet)</span></span></span><br><span class="line">    L1, supportData = <span class="function"><span class="title">scanD</span><span class="params">(D, C1, minSupport)</span></span></span><br><span class="line">    L = [L1]</span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    while (<span class="function"><span class="title">len</span><span class="params">(L[k-<span class="number">2</span>])</span></span> &gt; <span class="number">0</span>):</span><br><span class="line">        Ck = <span class="function"><span class="title">aprioriGen</span><span class="params">(L[k-<span class="number">2</span>], k)</span></span></span><br><span class="line">        Lk, supK = <span class="function"><span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span></span><span class="id">#scan</span> DB to get Lk</span><br><span class="line">        supportData.<span class="function"><span class="title">update</span><span class="params">(supK)</span></span></span><br><span class="line">        L.<span class="function"><span class="title">append</span><span class="params">(Lk)</span></span></span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">    return L, supportData#返回频繁项集和每个项集的支持度值</span><br></pre></td></tr></table></figure>
<h4 id="3-2_关联规则生成"><strong>3.2 关联规则生成</strong></h4><p>通过频繁项集，我们可以得到相应的规则，但是具体规则怎么得出来的呢？下面给出一个规则生成函数，具体原理参考注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入的参数分别为：频繁项集、支持度数据字典、自定义的最小支持度，返回的是可信度规则列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span><span class="params">(L, supportData, minConf=<span class="number">0.7</span>)</span>:</span>  <span class="comment">#支持度是通过scanD得到的字典</span></span><br><span class="line">    bigRuleList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(L)):<span class="comment">#只去频繁项集中元素个数大于2的子集，如&#123;1,2&#125;&#123;1,2,3&#125;，不取&#123;2&#125;&#123;3&#125;,etc...</span></span><br><span class="line">        <span class="keyword">for</span> freqSet <span class="keyword">in</span> L[i]:</span><br><span class="line">            H1 = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freqSet]</span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">1</span>):</span><br><span class="line">                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                calcConf(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> bigRuleList</span><br></pre></td></tr></table></figure>
<p>下面定义一个用来计算置信度的函数，通过该函数抽取出符合我们要求的规则，如freqSet为{1,2}，H为{1}，{2}，可以计算出{1}—&gt;{2}和{2}—&gt;{1}的质心度，即下面的conf变量，然后用if语句判断是否符合我们的要求。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算可信度，找到满足最小可信度的要求规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConf</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    prunedH = [] <span class="comment">#create new list to return</span></span><br><span class="line">    <span class="keyword">for</span> conseq <span class="keyword">in</span> H:</span><br><span class="line">        conf = supportData[freqSet]/supportData[freqSet-conseq] <span class="comment">#calc confidence</span></span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf: </span><br><span class="line">            <span class="keyword">print</span> freqSet-conseq,<span class="string">'--&gt;'</span>,conseq,<span class="string">'conf:'</span>,conf</span><br><span class="line">            brl.append((freqSet-conseq, conseq, conf))</span><br><span class="line">            prunedH.append(conseq)</span><br><span class="line">    <span class="keyword">return</span> prunedH</span><br></pre></td></tr></table></figure>
<p>下面的函数是用来合并子集的，比如我现在的频繁项集是{2,3,5},它的构造元素是{2},{3},{5}，所以需要将{2},{3},{5}两两合并然后再根据上面的calcConf函数计算置信度。代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#从最初的项集中生成更多的规则</span><br><span class="line">def <span class="function"><span class="title">rulesFromConseq</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span></span>:</span><br><span class="line">    m = <span class="function"><span class="title">len</span><span class="params">(H[<span class="number">0</span>])</span></span></span><br><span class="line">    <span class="keyword">if</span> (<span class="function"><span class="title">len</span><span class="params">(freqSet)</span></span> &gt; (m + <span class="number">1</span>)): #进一步合并项集</span><br><span class="line">        Hmp1 = <span class="function"><span class="title">aprioriGen</span><span class="params">(H, m+<span class="number">1</span>)</span></span><span class="id">#create</span> Hm+<span class="number">1</span> new candidates</span><br><span class="line">        Hmp1 = <span class="function"><span class="title">calcConf</span><span class="params">(freqSet, Hmp1, supportData, brl, minConf)</span></span></span><br><span class="line">        <span class="keyword">if</span> (<span class="function"><span class="title">len</span><span class="params">(Hmp1)</span></span> &gt; <span class="number">1</span>):    <span class="id">#need</span> at least two sets to merge</span><br><span class="line">            <span class="function"><span class="title">rulesFromConseq</span><span class="params">(freqSet, Hmp1, supportData, brl, minConf)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="3-3_测试"><strong>3.3 测试</strong></h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataSet = <span class="function"><span class="title">loadDataSet</span><span class="params">()</span></span>			 	#加载数据集</span><br><span class="line">L,suppoData = <span class="function"><span class="title">apriori</span><span class="params">(dataSet)</span></span>		#计算频繁项集</span><br><span class="line">rules = <span class="function"><span class="title">generateRules</span><span class="params">(L,suppoData,minConf=<span class="number">0.7</span>)</span></span> #抽取规则</span><br></pre></td></tr></table></figure>
<p>得到的结果为：<br><img src="http://img.blog.csdn.net/20150604094036589" alt="这里写图片描述"></p>
<p>L表示的是符合条件的频繁项集，rules表示最后抽取出来的符合条件的规则；还可以查看各个项集的支持度，如下所示。<br><img src="http://img.blog.csdn.net/20150604094213762" alt="这里写图片描述"></p>
<h3 id="Reference"><strong>Reference</strong></h3><p>[1]<strong>《机器学习实战》</strong>书籍第11章</p>
<p><br></p>
<hr>
<center><strong>本栏目Machine Learning 算法实现持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center>

<p><br><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong> 学习一个算法，我们最关心的并不是算法本身，而是一个算法能够干什么，能应用到什么地方。很多的时候，我们都需要从大量数据中提取出有用的信息，从大规模数据中寻找物品间的隐含关系叫做关联分析(association analysis)或者关联规则学习(association rule learning)。比如在平时的购物中，那些商品一起捆绑购买销量会比较好，又比如购物商城中的那些推荐信息，都是根据用户平时的搜索或者是购买情况来生成的。如果是蛮力搜索的话代价太高了，所以Apriori就出现了，就是为了解决这类问题的。</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法--K-means聚类]]></title>
    <link href="http://csuldw.github.io/2015/06/03/2015-06-03-K-means/"/>
    <id>http://csuldw.github.io/2015/06/03/2015-06-03-K-means/</id>
    <published>2015-06-03T04:30:00.000Z</published>
    <updated>2015-10-29T13:05:41.391Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong> k均值算法是一种聚类算法，所谓聚类，他是一种无监督学习，将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果和分类相同，而只是类别没有预先定义。</p>
<a id="more"></a>
<p><strong>算法的目的：</strong> 使各个样本与所在类均值的<strong>误差平方和达到最小</strong>（这也是评价K-means算法最后聚类效果的评价标准）</p>
<center><strong>K-均值聚类</strong></center>

<ul>
<li>优点：容易实现</li>
<li>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</li>
<li>适合数据类型：数值型数据</li>
</ul>
<h3 id="伪代码"><strong>伪代码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建k个点作为起始质心（经常随机选择）</span></span><br><span class="line"><span class="comment">#当任意一个点的蔟分配结果发生变化时</span></span><br><span class="line">	<span class="comment">#对数据集中的每个数据点</span></span><br><span class="line">		<span class="comment">#对每个质心</span></span><br><span class="line">			<span class="comment">#计算质心到数据点之间的距离</span></span><br><span class="line">		<span class="comment">#将数据点分配到距其最近的蔟</span></span><br><span class="line">	<span class="comment">#对每个蔟，计算蔟中所有点的均值并将均值作为质心</span></span><br></pre></td></tr></table></figure>
<h3 id="代码实现"><strong>代码实现</strong></h3><p>因为我们用到的是数值类型的数据，这里编写一个加载数据集的函数，返回值是一个矩阵形式。<br>下面代码应写在一个py文件里，我这里写在kMeans.py文件中。</p>
<p>文件的头部引入numpy<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure></p>
<p><strong>数据集加载代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集文件，没有返回类标号的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    openfile = open(fileName)    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> openfile.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        floatLine = map(float,curLine)</span><br><span class="line">        dataMat.append(floatLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></figure>
<p>因为在k均值算法中要计算点到质心的距离，所以这里将距离计算写成一个函数，计算欧几里得距离公式：</p>
<p>$$d=\sqrt{(x_2-x_1)^2+…+(z_2-z_1)^2}$$</p>
<p><strong>函数代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA,vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA-vecB,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<p><strong>接下来初始化k个蔟的质心函数centroid</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入的数据时numpy的矩阵格式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataMat, k)</span>:</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))  </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        minJ = min(dataMat[:,j]) <span class="comment"># 找出矩阵dataMat第j列最小值</span></span><br><span class="line">        rangeJ = float(max(dataMat[:,j]) - minJ) <span class="comment">#计算第j列最大值和最小值的差</span></span><br><span class="line">        <span class="comment">#赋予一个随机质心，它的值在整个数据集的边界之内</span></span><br><span class="line">        centroids[:,j] = minJ + rangeJ * random.rand(k,<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids <span class="comment">#返回一个随机的质心矩阵</span></span><br></pre></td></tr></table></figure>
<p><strong>K-means算法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k-均值算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataMat,k,distE = distEclud , createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataMat)[<span class="number">0</span>]    <span class="comment"># 获得行数m</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 初试化一个矩阵，用来记录簇索引和存储误差                               </span></span><br><span class="line">    centroids = createCent(dataMat,k) <span class="comment"># 随机的得到一个质心矩阵蔟</span></span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):    <span class="comment">#对每个数据点寻找最近的质心</span></span><br><span class="line">            minDist = inf; minIndex = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment"># 遍历质心蔟，寻找最近的质心    </span></span><br><span class="line">                distJ1 = distE(centroids[j,:],dataMat[i,:]) <span class="comment">#计算数据点和质心的欧式距离</span></span><br><span class="line">                <span class="keyword">if</span> distJ1 &lt; minDist: </span><br><span class="line">                    minDist = distJ1; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex:</span><br><span class="line">                clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):    <span class="comment">#更新质心的位置</span></span><br><span class="line">            ptsInClust = dataMat[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]    </span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>
<p><strong>测试：</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataMat = <span class="function"><span class="title">mat</span><span class="params">(loadDataSet(<span class="string">'testSet.txt'</span>)</span></span>)</span><br><span class="line"><span class="function"><span class="title">kMeans</span><span class="params">(dataMat,<span class="number">4</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>输出结果：</strong></p>
<p>===================</p>
<p>[[-3.66087851  2.30869657]<br> [ 3.24377288  3.04700412]<br> [ 2.52577861 -3.12485493]<br> [-2.79672694  3.19201596]]<br>[[-3.78710372 -1.66790611]<br> [ 2.6265299   3.10868015]<br> [ 1.62908469 -2.92689085]<br> [-2.18799937  3.01824781]]<br>[[-3.53973889 -2.89384326]<br> [ 2.6265299   3.10868015]<br> [ 2.65077367 -2.79019029]<br> [-2.46154315  2.78737555]]</p>
<p>===================</p>
<p>上面的结果给出了四个质心。可以看出，经过3次迭代之后K-均值算法收敛。质心会保存在第一个返回值中，第二个是每个点的簇分布情况。</p>
<p><strong>附件：</strong></p>
<p>上面测试的数据集为：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">1<span class="class">.658985</span>	4<span class="class">.285136</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.453687</span>	3<span class="class">.424321</span></span><br><span class="line">4<span class="class">.838138</span>	<span class="tag">-1</span><span class="class">.151539</span></span><br><span class="line"><span class="tag">-5</span><span class="class">.379713</span>	<span class="tag">-3</span><span class="class">.362104</span></span><br><span class="line">0<span class="class">.972564</span>	2<span class="class">.924086</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.567919</span>	1<span class="class">.531611</span></span><br><span class="line">0<span class="class">.450614</span>	<span class="tag">-3</span><span class="class">.302219</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.487105</span>	<span class="tag">-1</span><span class="class">.724432</span></span><br><span class="line">2<span class="class">.668759</span>	1<span class="class">.594842</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.156485</span>	3<span class="class">.191137</span></span><br><span class="line">3<span class="class">.165506</span>	<span class="tag">-3</span><span class="class">.999838</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.786837</span>	<span class="tag">-3</span><span class="class">.099354</span></span><br><span class="line">4<span class="class">.208187</span>	2<span class="class">.984927</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.123337</span>	2<span class="class">.943366</span></span><br><span class="line">0<span class="class">.704199</span>	<span class="tag">-0</span><span class="class">.479481</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.392370</span>	<span class="tag">-3</span><span class="class">.963704</span></span><br><span class="line">2<span class="class">.831667</span>	1<span class="class">.574018</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.790153</span>	3<span class="class">.343144</span></span><br><span class="line">2<span class="class">.943496</span>	<span class="tag">-3</span><span class="class">.357075</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.195883</span>	<span class="tag">-2</span><span class="class">.283926</span></span><br><span class="line">2<span class="class">.336445</span>	2<span class="class">.875106</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.786345</span>	2<span class="class">.554248</span></span><br><span class="line">2<span class="class">.190101</span>	<span class="tag">-1</span><span class="class">.906020</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.403367</span>	<span class="tag">-2</span><span class="class">.778288</span></span><br><span class="line">1<span class="class">.778124</span>	3<span class="class">.880832</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.688346</span>	2<span class="class">.230267</span></span><br><span class="line">2<span class="class">.592976</span>	<span class="tag">-2</span><span class="class">.054368</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.007257</span>	<span class="tag">-3</span><span class="class">.207066</span></span><br><span class="line">2<span class="class">.257734</span>	3<span class="class">.387564</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.679011</span>	0<span class="class">.785119</span></span><br><span class="line">0<span class="class">.939512</span>	<span class="tag">-4</span><span class="class">.023563</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.674424</span>	<span class="tag">-2</span><span class="class">.261084</span></span><br><span class="line">2<span class="class">.046259</span>	2<span class="class">.735279</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.189470</span>	1<span class="class">.780269</span></span><br><span class="line">4<span class="class">.372646</span>	<span class="tag">-0</span><span class="class">.822248</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.579316</span>	<span class="tag">-3</span><span class="class">.497576</span></span><br><span class="line">1<span class="class">.889034</span>	5<span class="class">.190400</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.798747</span>	2<span class="class">.185588</span></span><br><span class="line">2<span class="class">.836520</span>	<span class="tag">-2</span><span class="class">.658556</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.837877</span>	<span class="tag">-3</span><span class="class">.253815</span></span><br><span class="line">2<span class="class">.096701</span>	3<span class="class">.886007</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.709034</span>	2<span class="class">.923887</span></span><br><span class="line">3<span class="class">.367037</span>	<span class="tag">-3</span><span class="class">.184789</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.121479</span>	<span class="tag">-4</span><span class="class">.232586</span></span><br><span class="line">2<span class="class">.329546</span>	3<span class="class">.179764</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.284816</span>	3<span class="class">.273099</span></span><br><span class="line">3<span class="class">.091414</span>	<span class="tag">-3</span><span class="class">.815232</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.762093</span>	<span class="tag">-2</span><span class="class">.432191</span></span><br><span class="line">3<span class="class">.542056</span>	2<span class="class">.778832</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.736822</span>	4<span class="class">.241041</span></span><br><span class="line">2<span class="class">.127073</span>	<span class="tag">-2</span><span class="class">.983680</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.323818</span>	<span class="tag">-3</span><span class="class">.938116</span></span><br><span class="line">3<span class="class">.792121</span>	5<span class="class">.135768</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.786473</span>	3<span class="class">.358547</span></span><br><span class="line">2<span class="class">.624081</span>	<span class="tag">-3</span><span class="class">.260715</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.009299</span>	<span class="tag">-2</span><span class="class">.978115</span></span><br><span class="line">2<span class="class">.493525</span>	1<span class="class">.963710</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.513661</span>	2<span class="class">.642162</span></span><br><span class="line">1<span class="class">.864375</span>	<span class="tag">-3</span><span class="class">.176309</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.171184</span>	<span class="tag">-3</span><span class="class">.572452</span></span><br><span class="line">2<span class="class">.894220</span>	2<span class="class">.489128</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.562539</span>	2<span class="class">.884438</span></span><br><span class="line">3<span class="class">.491078</span>	<span class="tag">-3</span><span class="class">.947487</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.565729</span>	<span class="tag">-2</span><span class="class">.012114</span></span><br><span class="line">3<span class="class">.332948</span>	3<span class="class">.983102</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.616805</span>	3<span class="class">.573188</span></span><br><span class="line">2<span class="class">.280615</span>	<span class="tag">-2</span><span class="class">.559444</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.651229</span>	<span class="tag">-3</span><span class="class">.103198</span></span><br><span class="line">2<span class="class">.321395</span>	3<span class="class">.154987</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.685703</span>	2<span class="class">.939697</span></span><br><span class="line">3<span class="class">.031012</span>	<span class="tag">-3</span><span class="class">.620252</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.599622</span>	<span class="tag">-2</span><span class="class">.185829</span></span><br><span class="line">4<span class="class">.196223</span>	1<span class="class">.126677</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.133863</span>	3<span class="class">.093686</span></span><br><span class="line">4<span class="class">.668892</span>	<span class="tag">-2</span><span class="class">.562705</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.793241</span>	<span class="tag">-2</span><span class="class">.149706</span></span><br><span class="line">2<span class="class">.884105</span>	3<span class="class">.043438</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.967647</span>	2<span class="class">.848696</span></span><br><span class="line">4<span class="class">.479332</span>	<span class="tag">-1</span><span class="class">.764772</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.905566</span>	<span class="tag">-2</span><span class="class">.911070</span></span><br></pre></td></tr></table></figure>]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong> k均值算法是一种聚类算法，所谓聚类，他是一种无监督学习，将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果和分类相同，而只是类别没有预先定义。</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[sklearn训练模型的保存与恢复（Python）]]></title>
    <link href="http://csuldw.github.io/2015/05/31/2015-05-31%20scikit-learn%20training%20model's%20save%20and%20reused/"/>
    <id>http://csuldw.github.io/2015/05/31/2015-05-31 scikit-learn training model's save and reused/</id>
    <published>2015-05-31T12:52:00.000Z</published>
    <updated>2015-10-30T00:46:34.394Z</updated>
    <content type="html"><![CDATA[<p>在做模型训练的时候，尤其是在训练集上做交叉验证，通常想要将模型保存下来，然后放到独立的测试集上测试，下面介绍的是Python中训练模型的保存和再使用。</p>
<p>scikit-learn已经有了模型持久化的操作，导入joblib即可</p>
<figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br></pre></td></tr></table></figure>
<a id="more"></a>
<h3 id="模型保存"><strong>模型保存</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span>os.chdir(<span class="string">"workspace/model_save"</span>)</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>X = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">1</span>, <span class="number">1</span>]]</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>y = [<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>clf = svm.SVC()</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>clf.fit(X, y)  </span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>clf.fit(train_X,train_y)</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>joblib.dump(clf, <span class="string">"train_model.m"</span>)</span><br></pre></td></tr></table></figure>
<p>通过joblib的dump可以将模型保存到本地，clf是训练的分类器</p>
<h3 id="模型从本地调回"><strong>模型从本地调回</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;&gt; </span>clf = joblib.load(<span class="string">"train_model.m"</span>)</span><br></pre></td></tr></table></figure>
<p>通过joblib的load方法，加载保存的模型。</p>
<p>然后就可以在测试集上测试了</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">clf.<span class="function"><span class="title">predit</span><span class="params">(test_X，test_y)</span></span></span><br></pre></td></tr></table></figure>
<hr>
<p><br></p>
<center><strong>本栏目机器学习持续更新中，欢迎来访：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z 博客</a><br>新浪微博： <a href="http://weibo.com/liudiwei210" target="_black">@拾毅者</a> 个人博客： <a href="http://csuldw.github.io" target="_black">AirHax’s Notes</a><br><br></strong></center>

]]></content>
    <summary type="html">
    <![CDATA[<p>在做模型训练的时候，尤其是在训练集上做交叉验证，通常想要将模型保存下来，然后放到独立的测试集上测试，下面介绍的是Python中训练模型的保存和再使用。</p>
<p>scikit-learn已经有了模型持久化的操作，导入joblib即可</p>
<figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.externals <span class="keyword">import</span> joblib</span><br></pre></td></tr></table></figure>]]>
    
    </summary>
    
      <category term="Python" scheme="http://csuldw.github.io/tags/Python/"/>
    
      <category term="scikit-learn" scheme="http://csuldw.github.io/tags/scikit-learn/"/>
    
      <category term="Python" scheme="http://csuldw.github.io/categories/Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-朴素贝叶斯Python实现]]></title>
    <link href="http://csuldw.github.io/2015/05/28/2015-05-28-NB/"/>
    <id>http://csuldw.github.io/2015/05/28/2015-05-28-NB/</id>
    <published>2015-05-28T04:59:00.000Z</published>
    <updated>2015-10-29T13:05:27.162Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong>前面提到的K最近邻算法和决策树算法，数据实例最终被明确的划分到某个分类中，下面介绍一种不能完全确定数据实例应该划分到哪个类别，或者说只能给数据实例属于给定分类的概率。<br>categories: ML<br><a id="more"></a></p>
<h3 id="基于贝叶斯决策理论的分类方法之朴素贝叶斯"><strong>基于贝叶斯决策理论的分类方法之朴素贝叶斯</strong></h3><ul>
<li>优点：在数据较少的情况下仍然有效，可以处理多类别问题</li>
<li>缺点：对于输入数据的准备方式较为敏感 </li>
<li>适用数据类型：标称型数据。</li>
</ul>
<h3 id="朴素贝叶斯的一般过程"><strong>朴素贝叶斯的一般过程</strong></h3><ul>
<li>收集数据：可以使用任何方式</li>
<li>准备数据：需要数据型或是布尔型数据</li>
<li>分类数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好</li>
<li>训练算法：计算不同的独立特征的条件概率</li>
<li>测试算法：计算错误率</li>
<li>使用算法：文档分类</li>
</ul>
<h3 id="原理"><strong>原理</strong></h3><p>主要是运用<strong>贝叶斯定理</strong></p>
<p>$$ P(H|X) = \dfrac{P(X|H) p(H)}{P(X)} $$</p>
<h3 id="算法实现"><strong>算法实现</strong></h3><p>下面做一个简单的留言板分类，自动判别留言类别：侮辱类和非侮辱类，分别使用1和0表示。下面来做一下这个实验。以下函数全部写在一个叫bayes.py文件中，后面的实验室通过导入bayes.py，调用里面的函数来做的。</p>
<p>导入numpy包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h4 id="1-加载数据集"><strong>1.加载数据集</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    postingList=[[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</span><br><span class="line">                 [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                 [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                 [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]    <span class="comment">#1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList,classVec</span><br></pre></td></tr></table></figure>
<p>该函数返回的是<strong>词条切分集合</strong>和<strong>类标签</strong>。</p>
<h4 id="2-根据样本创建一个词库"><strong>2.根据样本创建一个词库</strong></h4><p>下面的函数是根据上面给出来的样本数据所创建出来的一个词库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    vocabSet = set([])  <span class="comment">#create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet = vocabSet | set(document) <span class="comment">#union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br></pre></td></tr></table></figure>
<h4 id="3-统计每个样本在词库中的出现情况"><strong>3.统计每个样本在词库中的出现情况</strong></h4><p>下面的函数功能是把单个样本映射到词库中去，统计单个样本在词库中的出现情况，1表示出现过，0表示没有出现，函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="keyword">print</span> <span class="string">"the word: %s is not in my Vocabulary!"</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<h4 id="4-计算条件概率和类标签概率"><strong>4.计算条件概率和类标签概率</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCategory)</span>:</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs) <span class="comment">#计算某个类发生的概率</span></span><br><span class="line">    p0Num = ones(numWords); p1Num = ones(numWords) <span class="comment">#初始样本个数为1，防止条件概率为0，影响结果       </span></span><br><span class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span>  <span class="comment">#作用同上                      </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    p1Vect = log(p1Num/p1Denom)         <span class="comment">#计算类标签为1时的其它属性发生的条件概率</span></span><br><span class="line">    p0Vect = log(p0Num/p0Denom)         <span class="comment">#计算标签为0时的其它属性发生的条件概率</span></span><br><span class="line">    <span class="keyword">return</span> p0Vect,p1Vect,pAbusive       <span class="comment">#返回条件概率和类标签为1的概率</span></span><br></pre></td></tr></table></figure>
<p>说明：</p>
<h4 id="5-训练贝叶斯分类算法"><strong>5.训练贝叶斯分类算法</strong></h4><p>该算法包含四个输入，vec2Classify表示待分类的样本在词库中的映射集合，p0Vec表示条件概率$P(w_i|c=0)$，p1Vec表示条件概率$P(w_i|c=1)$，pClass1表示类标签为1时的概率$P(c=1)$。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function"><span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span></span>:</span><br><span class="line">    p1 = <span class="function"><span class="title">sum</span><span class="params">(vec2Classify * p1Vec)</span></span> + <span class="function"><span class="title">log</span><span class="params">(pClass1)</span></span>    <span class="id">#element-wise</span> mult</span><br><span class="line">    p0 = <span class="function"><span class="title">sum</span><span class="params">(vec2Classify * p0Vec)</span></span> + <span class="function"><span class="title">log</span><span class="params">(<span class="number">1.0</span> - pClass1)</span></span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        return <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        return <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>其中p1和p0表示的是</p>
<p>$$lnp(w_1|c=1)p(w_2|c=1)…p(w_n|c=1)*p(c=1)$$</p>
<p>和</p>
<p>$$lnp(w_1|c=0)p(w_2|c=0)…p(w_n|c=0)*p(c=0)$$</p>
<p>取对数是因为防止p(w_1|c=1)p(w_2|c=1)p(w_3|c=1)…p(w_n|c=1)多个小于1的数相乘结果值下溢。</p>
<h4 id="6-文档词袋模型,修改函数setOfWords2Vec"><strong>6.文档词袋模型,修改函数setOfWords2Vec</strong></h4><p>词袋模型主要修改上面的第三个步骤，因为有的词可能出现多次，所以在单个样本映射到词库的时候需要多次统计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<h4 id="7-测试函数"><strong>7.测试函数</strong></h4><p>下面给出一个测试函数，直接调用该测试函数就可以实现简单的分类，测试结果看下个部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="comment">#step1：加载数据集和类标号</span></span><br><span class="line">    listOPosts,listClasses = loadDataSet()</span><br><span class="line">    <span class="comment">#step2：创建词库</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># step3：计算每个样本在词库中的出现情况</span></span><br><span class="line">    trainMat=[]</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment">#step4：调用第四步函数，计算条件概率</span></span><br><span class="line">    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))</span><br><span class="line">    <span class="comment"># step5</span></span><br><span class="line">    <span class="comment"># 测试1 </span></span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry,<span class="string">'classified as: '</span>,classifyNB(thisDoc,p0V,p1V,pAb)</span><br><span class="line">    <span class="comment"># 测试2</span></span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry,<span class="string">'classified as: '</span>,classifyNB(thisDoc,p0V,p1V,pAb)</span><br></pre></td></tr></table></figure>
<h4 id="8-实验"><strong>8.实验</strong></h4><p>首先导入库，然后导入bayes.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">r"E:\3-CSU\Academic\Machine Leaning\机器学习实战\src\machinelearninginaction\Ch04"</span>)</span><br><span class="line"><span class="keyword">import</span> bayes</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20150528122404620" alt="这里写图片描述"></p>
<p>可以看出，贝叶斯算法将[‘love’, ‘my’, ‘dalmation’]分为“无侮辱”一类，将[‘stupid’, ‘garbage’]分为“侮辱”性质的一类。</p>
<h3 id="Reference"><strong>Reference</strong></h3><p><strong>[1]《Machine Learning in Action 》机器学习实战</strong></p>
<hr>
<center><br><br>本栏目Machine Learning 算法实现持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a><br><br></center>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong>前面提到的K最近邻算法和决策树算法，数据实例最终被明确的划分到某个分类中，下面介绍一种不能完全确定数据实例应该划分到哪个类别，或者说只能给数据实例属于给定分类的概率。<br>categories: ML<br>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-K最近邻从原理到实现]]></title>
    <link href="http://csuldw.github.io/2015/05/21/2015-05-21-KNN/"/>
    <id>http://csuldw.github.io/2015/05/21/2015-05-21-KNN/</id>
    <published>2015-05-21T12:34:00.000Z</published>
    <updated>2015-10-29T13:05:18.791Z</updated>
    <content type="html"><![CDATA[<p><strong>引文</strong>：决策树和基于规则的分类器都是<strong>积极学习方法</strong>（eager learner）的例子，因为一旦训练数据可用，他们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为<strong>消极学习法</strong>（lazy learner）。<strong>最近邻分类器</strong>就是这样的一种方法。</p>
<a id="more"></a>
<h3 id="1-K最近邻分类器原理"><strong>1.K最近邻分类器原理</strong></h3><p>首先给出一张图，根据这张图来理解最近邻分类器，如下：</p>
<center><img src="http://img.blog.csdn.net/20150521201557111" alt="这里写图片描述"><br></center>

<p>根据上图所示，有两类不同的样本数据，分别用<strong>蓝色的小正方形</strong>和<strong>红色的小三角形</strong>表示，而图正中间的那个<strong>绿色的圆</strong>所标示的数据则是待分类的数据。也就是说，现在， 我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。</p>
<p>　　我们常说，物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他or她身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到：</p>
<ul>
<li>如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li>
<li>如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。</li>
</ul>
<p>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</p>
<p>KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p>KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。</p>
<p>前面的例子中强调了选择合适的K值的重要性。如果太小，则最近邻分类器容易受到训练数据的噪声而产生的过分拟合的影响；相反，如果K太大，最近分类器可能会误会分类测试样例，因为最近邻列表中可能包含远离其近邻的数据点。（如下图所示）</p>
<center><img src="http://img.blog.csdn.net/20150521203027253" alt="这里写图片描述"><br><br><strong>K较大时的最近邻分类</strong><br><br></center>


<p>可见，K值的选取还是非常关键。</p>
<hr>
<h3 id="2-算法"><strong>2.算法</strong></h3><p>算法步骤如下所示：</p>
<center><img src="http://img.blog.csdn.net/20150521203212596" alt="这里写图片描述"></center>

<p>对每个测试样例$z = (x’,y’)$，算法计算它和所有训练样例$（x,y）属于D$之间的距离（或相似度），以确定其最近邻列表$D_z$。如果训练样例的数目很大，那么这种计算的开销就会很大。不过，可以使索引技术降低为测试样例找最近邻是的计算量。</p>
<p>一旦得到最近邻列表，测试样例就可以根据最近邻的多数类进行分类，使用多数表决方法。</p>
<hr>
<h3 id="3-K最邻近算法实现（Python）"><strong>3.K最邻近算法实现（Python）</strong></h3><p>KNN.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createDataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        group = array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])</span><br><span class="line">        labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">        <span class="keyword">return</span> group,labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">KnnClassify</span><span class="params">(self,testX,trainX,labels,K)</span>:</span></span><br><span class="line">        [N,M]=trainX.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#calculate the distance between testX and other training samples</span></span><br><span class="line">        difference = tile(testX,(N,<span class="number">1</span>)) - trainX <span class="comment"># tile for array and repeat for matrix in Python, == repmat in Matlab</span></span><br><span class="line">        difference = difference ** <span class="number">2</span> <span class="comment"># take pow(difference,2)</span></span><br><span class="line">        distance = difference.sum(<span class="number">1</span>) <span class="comment"># take the sum of difference from all dimensions</span></span><br><span class="line">        distance = distance ** <span class="number">0.5</span></span><br><span class="line">        sortdiffidx = distance.argsort()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># find the k nearest neighbours</span></span><br><span class="line">        vote = &#123;&#125; <span class="comment">#create the dictionary</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">            ith_label = labels[sortdiffidx[i]];</span><br><span class="line">            vote[ith_label] = vote.get(ith_label,<span class="number">0</span>)+<span class="number">1</span> <span class="comment">#get(ith_label,0) : if dictionary 'vote' exist key 'ith_label', return vote[ith_label]; else return 0</span></span><br><span class="line">        sortedvote = sorted(vote.iteritems(),key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 'key = lambda x: x[1]' can be substituted by operator.itemgetter(1)</span></span><br><span class="line">        <span class="keyword">return</span> sortedvote[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">k = KNN() <span class="comment">#create KNN object</span></span><br><span class="line">group,labels = k.createDataset()</span><br><span class="line">cls = k.KnnClassify([<span class="number">0</span>,<span class="number">0</span>],group,labels,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> cls</span><br></pre></td></tr></table></figure>
<hr>
<p>运行：</p>
<ol>
<li>在Python Shell 中可以运行KNN.py</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> os</span><br><span class="line">&gt;&gt;&gt;os.chdir(<span class="string">"/home/liudiwei/code/data_miningKNN/"</span>)</span><br><span class="line">&gt;&gt;&gt;execfile(<span class="string">"KNN.py"</span>)</span><br></pre></td></tr></table></figure>
<p>输出:B<br>（B表示类别）</p>
<p>2.或者terminal中直接运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python KNN.py</span><br></pre></td></tr></table></figure>
<p>3.也可以不在KNN.py中写输出，而选择在Shell中获得结果，i.e.,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> KNN</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>KNN.k.KnnClassify([<span class="number">0</span>,<span class="number">0</span>],KNN.group,KNN.labels,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="References"><strong>References</strong></h3><p>【1】Introduction to Data Mining <a href="http://vdisk.weibo.com/s/akTUdytgliZM8" target="_blank" rel="external">数据挖掘导论</a><br>【2】<a href="http://blog.csdn.net/abcjennifer/article/details/19757987" target="_blank" rel="external">Rachel Zhang-K近邻分类算法实现 in Python</a></p>
<hr>
<p>附件（两张自己的计算过程图）：</p>
<center><img src="http://img.blog.csdn.net/20150524192410343" alt="这里写图片描述"><br><strong>图1 KNN算法核心部分</strong><br></center><br><center><br><img src="http://img.blog.csdn.net/20150524192640924" alt="这里写图片描述"><br><strong>图2 简易计算过程</strong><br></center><br>说明：上述图片仅供参考，看不懂就自己测试一组数据如[0,1]慢慢推导一下吧<br><br>———-<br><br><center><strong>本栏目机器学习算法持续更新中……</strong></center>]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文</strong>：决策树和基于规则的分类器都是<strong>积极学习方法</strong>（eager learner）的例子，因为一旦训练数据可用，他们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为<strong>消极学习法</strong>（lazy learner）。<strong>最近邻分类器</strong>就是这样的一种方法。</p>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-决策树理论]]></title>
    <link href="http://csuldw.github.io/2015/05/08/2015-05-08-decision%20tree/"/>
    <id>http://csuldw.github.io/2015/05/08/2015-05-08-decision tree/</id>
    <published>2015-05-08T13:53:12.000Z</published>
    <updated>2015-10-29T13:05:02.256Z</updated>
    <content type="html"><![CDATA[<p><strong>用较少的东西，同样可以做好的事情。越是小的决策树，越优于大的决策树。</strong></p>
<h2 id="引文"><strong>引文</strong></h2><p>数据分类是一个两阶段过程，包括学习阶段（构建分类模型）和分类阶段（使用模型预测给定数据的类标号）。决策树分类算法是监督学习的一种，即Supervised learning。</p>
<ul>
<li>分类过程的第一阶段也可以看做学习一个映射或函数y=f(x),它可以预测给定元组X的类标号y。</li>
<li>在第二阶段，使用模型进行分类。首先评估分类器的预测准确率。这个过程要尽量的减少<strong>过拟合</strong>。（为什么是尽量减少而不是避免呢，因为过拟合一般是避免不了的，再好的模型也会有过拟合的情况）。</li>
</ul>
<a id="more"></a>
<h2 id="1_决策树归纳"><strong>1 决策树归纳</strong></h2><p> 决策树归纳是从有类标号的训练元组中学习决策树。常用的决策树算法有ID3，C4.5和CART。它们都是采用贪心（即非回溯的）方法，其中决策树自顶向下递归的分治方法构造。其中划分属性的方法各不相同，ID3使用的是<strong>信息增益</strong>，C4.5使用的是<strong>信息增益率</strong>，而CART使用的是<strong>Gini基尼指数</strong>。下面来简单介绍下决策树的理论知识。内容包含<strong>熵</strong>、<strong>信息增益</strong>、<strong>信息增益率</strong>以及<strong>Gini指数</strong>的计算公式。</p>
<h2 id="2_基本原理"><strong>2 基本原理</strong></h2><h3 id="2-1_算法优点"><strong>2.1 算法优点</strong></h3><p> 决策树算法的优点如下：<br>（1）分类精度高；<br>（2）成的模式简单；<br>（3）对噪声数据有很好的健壮性。<br>因而是目前应用最为广泛的归纳推理算法之一，在数据挖掘中受到研究者的广泛关注。</p>
<h3 id="2-2_算法一般流程"><strong>2.2 算法一般流程</strong></h3><p>（1）收集数据：任意方法和途径。<br>（2）准备数据：书构造算法只适用于标称型数据，因此数据必须离散化。<br>（3）分析数据：构造树完成后，检查图形是否符合预测。<br>（4）训练算法：决策树的数据构造。<br>（5）测试算法：一般将决策树用于分类，可以用错误率衡量，而错误率使用经验率计算。<br>（6）使用算法：决策树可以用于任何监督学习算法。  </p>
<h3 id="2-3_实例"><strong>2.3 实例</strong></h3><p><strong>信息增益和熵（克劳德.香农提出）</strong></p>
<h4 id="1-使用信息增益进行决策树归纳"><strong>1.使用信息增益进行决策树归纳</strong></h4><p><strong>信息增益度量属性选择</strong></p>
<p><strong>熵（Entropy）的计算公式</strong></p>
<p>熵定义为<strong>信息增益的期望值</strong>。熵越大，一个变量的不确定性就越大（也就是可取的值很多），把它分析清楚需要的信息量也就越大，熵是整个系统的平均信息量。对$D$中的元组分类所需要的期望信息由下列公式给出：</p>
<p>$$Entropy=H(D)=E(I(D))=-\sum_i^{n}p_ilog_2(p_i)，p_i是D中任意元组属于类C_i非零概率。$$</p>
<p>熵越大，说明系统越混乱，携带的信息就越少。熵越小，说明系统越有序，携带的信息就越多。信息的作用就是在于消除不确定性。</p>
<p>ID3划分特征使用的就是信息增益IG.</p>
<p>一个属性的信息增益越大，表明属性对样本的熵减少的能力就更强，该属性使得数据所属类别的不确定性变为确定性的能力越强。</p>
<p>注：<strong>需要的期望信息越小，分区的纯度越高。</strong></p>
<p><strong>信息增益计算</strong></p>
<p>首先计算特征A对数据集D的经验<strong>条件熵</strong>$H(D|A)$,在数学上就是条件概率分布（Condition Probability）</p>
<p>$$H(D|A)=\sum_j\dfrac{|D_j|}{|D|}\times H(D_j)，项\dfrac{|D_i|}{|D|}充当第j个分区的权重$$</p>
<p>引入条件熵，在信息论中主要是为了消除结果的不确定性。<br>然后计算信息增益</p>
<p>$$Gain(A) = H(D) - H(D|A)$$</p>
<p>其中，$Gain(A)$即为所求的信息增益。</p>
<p>下面来应用一个实例，<strong>训练元组数据D</strong></p>
<p><img src="http://img.blog.csdn.net/20150513110022176" alt="这里写图片描述"></p>
<p>在这里</p>
<p>$$H(D|age)=\dfrac{5}{14}\times(-\dfrac{2}{5}log_2\dfrac{2}{5}-\dfrac{3}{5}log_2 \dfrac{3}{5})+\dfrac{4}{14}\times(-\dfrac{4}{4}log_2\dfrac{0}{4}-\dfrac{0}{4}log_2 \dfrac{0}{4})+\dfrac{5}{14}\times(-\dfrac{3}{5}log_2\dfrac{3}{5}-\dfrac{2}{5}log_2 \dfrac{2}{5})=0.694位$$</p>
<p>根据计算出来的条件熵，计算按$age$划分的信息增益，计算方法如下：</p>
<p>$$Gain(age)=H(D)-H(D|age)=0.940-0.964=0.246位$$</p>
<p>类似的可以计算出其它属性的信息增益：</p>
<p>$$ Gain(income)=0.029位，<br>Gain(student)=0.151位，Gain(credit_rating)=0.048位 $$</p>
<p>由于$age$在属性中具有最高的信息增益，所以它被选作分裂特征。下面再进行递归计算信息增益，在此就不展示了。</p>
<p>ID3采用的就是就是IG，算法步骤如下：</p>
<center><br><img src="http://img.blog.csdn.net/20150921091028096" alt="这里写图片描述"><br><img src="http://img.blog.csdn.net/20150921091053471" alt="这里写图片描述"><br></center>

<h4 id="2-使用增益率计算"><strong>2.使用增益率计算</strong></h4><p><strong>ID3使用的是信息增益，C4.5使用的是信息增益率。</strong></p>
<p>C4.5算法继承了ID3算法的优点，并在以下几方面对ID3算法进行了改进：<br>1) 用信息增益率来选择属性，克服了用信息增益选择属性时偏向选择取值多的属性的不足；<br>2) 在树构造过程中进行剪枝；<br>3) 能够完成对连续属性的离散化处理；<br>4) 能够对不完整数据进行处理。  </p>
<p>C4.5算法有如下优点：产生的分类规则易于理解，准确率较高。其缺点是：在构造树的过程中，需要对数据集进行多次的顺序扫描和排序，因而导致算法的低效。此外，C4.5只适合于能够驻留于内存的数据集，当训练集大得无法在内存容纳时程序无法运行。</p>
<p>另外，无论是ID3还是C4.5最好在小数据集上使用，决策树分类一般只试用于小数据。当属性取值很多时最好选择C4.5算法，ID3得出的效果会非常差。</p>
<p><strong>分裂信息计算公式：</strong></p>
<p>$$Split_H(D|A)=-∑\dfrac{|D_j|}{|D|}\times log_2(\dfrac{|D_j|}{|D|})$$</p>
<p>增益率定义为：</p>
<p>$$Gain_Rate(A)=\dfrac{Gain(A)}{Split_H(D|A)}$$</p>
<p>选择具有最大增益率的特征作为分裂特征。</p>
<h4 id="3-基尼指数Gini_index"><strong>3.基尼指数Gini index</strong></h4><p>基尼指数在CART中使用，Gini index度量的是数据分区或训练元组集D的不纯度。计算方式如下：</p>
<p>$$Gini(D)=1-\sum p^{2}_i，其中，p_i是D中元组数以C_i类的概率，对m个类计算和。$$</p>
<h2 id="3-学习推介"><strong>3.学习推介</strong></h2><p>Andrew W. Moore PPT <a href="http://www.autonlab.org/tutorials/dtree18.pdf" target="_blank" rel="external">dtree</a><br>决策树Python实现，单独成文，网址：<a href="http://blog.csdn.net/dream_angel_z/article/details/45965463" target="_blank" rel="external">决策树实现</a><br>Wikipedia维基百科-<a href="https://en.wikipedia.org/wiki/Decision_tree" target="_blank" rel="external">Decision Tree决策树</a></p>
<p>最后，附一张决策树的优点和缺点图：</p>
<center><br><img src="http://img.blog.csdn.net/20150921094738621" alt="这里写图片描述"><br></center>


<h2 id="4-Reference"><strong>4.Reference</strong></h2><p>[1]数据挖掘概念与技术 Third Edition,韩家伟.<br>[2]机器学习实战 ,Peter Harrington.  </p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>用较少的东西，同样可以做好的事情。越是小的决策树，越优于大的决策树。</strong></p>
<h2 id="引文"><strong>引文</strong></h2><p>数据分类是一个两阶段过程，包括学习阶段（构建分类模型）和分类阶段（使用模型预测给定数据的类标号）。决策树分类算法是监督学习的一种，即Supervised learning。</p>
<ul>
<li>分类过程的第一阶段也可以看做学习一个映射或函数y=f(x),它可以预测给定元组X的类标号y。</li>
<li>在第二阶段，使用模型进行分类。首先评估分类器的预测准确率。这个过程要尽量的减少<strong>过拟合</strong>。（为什么是尽量减少而不是避免呢，因为过拟合一般是避免不了的，再好的模型也会有过拟合的情况）。</li>
</ul>]]>
    
    </summary>
    
      <category term="Machine Learning" scheme="http://csuldw.github.io/tags/Machine-Learning/"/>
    
      <category term="ML" scheme="http://csuldw.github.io/categories/ML/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[Git-同步一个fork]]></title>
    <link href="http://csuldw.github.io/2015/04/15/2015-04-12-Syncing-a-fork/"/>
    <id>http://csuldw.github.io/2015/04/15/2015-04-12-Syncing-a-fork/</id>
    <published>2015-04-15T05:14:54.000Z</published>
    <updated>2015-10-22T12:31:21.168Z</updated>
    <content type="html"><![CDATA[<h2 id="如何使用搜索引擎">如何使用搜索引擎</h2><p>其实这个问题并不难，我又被坑了。百度搜的东西不靠谱啊，以后这种问题一定要用<strong>英文</strong>在 <a href="http://www.google.com" target="_blank" rel="external">Google</a> 或者 <a href="http://cn.bing.com/" target="_blank" rel="external">Bing</a> 上搜索，这样才能搜到原汁原味的答案。就当是一个教训吧。   </p>
<p>搜索 fork sync，就可以看到 GitHub 自己的帮助文档 <a href="https://help.github.com/articles/syncing-a-fork/" target="_blank" rel="external">Syncing a fork</a> 点进去看这篇的时候，注意到有一个 Tip: Before you can sync your fork with an upstream repository, you must <a href="https://help.github.com/articles/configuring-a-remote-for-a-fork/" target="_blank" rel="external">configure a remote that points to the upstream repository</a> in Git.<br>根据这两篇文章，问题迎刃而解！ </p>
<a id="more"></a>
<hr>
<h2 id="具体方法">具体方法</h2><hr>
<h3 id="Configuring_a_remote_for_a_fork">Configuring a remote for a fork</h3><ul>
<li><p>给 fork 配置一个 remote   </p>
</li>
<li><p>主要使用 <code>git remote -v</code> 查看远程状态。   </p>
</li>
</ul>
<pre><code>git remote -v
# origin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)
# origin  https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)
</code></pre>

<ul>
<li>添加一个将被同步给 fork 远程的上游仓库      </li>
</ul>
<pre><code>git remote add upstream https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git
</code></pre>


<ul>
<li>再次查看状态确认是否配置成功。   </li>
</ul>
<pre><code>git remote -v
# origin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (fetch)
# origin    https://github.com/YOUR_USERNAME/YOUR_FORK.git (push)
# upstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (fetch)
# upstream  https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY.git (push)
</code></pre>

<hr>
<h3 id="Syncing_a_fork">Syncing a fork</h3><ul>
<li>从上游仓库 fetch 分支和提交点，传送到本地，并会被存储在一个本地分支 upstream/master<br><code>git fetch upstream</code>    </li>
</ul>
<pre><code>git fetch upstream
# remote: Counting objects: 75, done.
# remote: Compressing objects: 100% (53/53), done.
# remote: Total 62 (delta 27), reused 44 (delta 9)
# Unpacking objects: 100% (62/62), done.
# From https://github.com/ORIGINAL_OWNER/ORIGINAL_REPOSITORY
#  * [new branch]      master     -> upstream/master
</code></pre>

<ul>
<li>切换到本地主分支(如果不在的话)<br><code>git checkout master</code>    </li>
</ul>
<pre><code>git checkout master
# Switched to branch 'master'
</code></pre>

<ul>
<li>把 upstream/master 分支合并到本地 master 上，这样就完成了同步，并且不会丢掉本地修改的内容。<br><code>git merge upstream/master</code>      </li>
</ul>
<pre><code>git merge upstream/master
# Updating a422352..5fdff0f
# Fast-forward
#  README                    |    9 -------
#  README.md                 |    7 ++++++
#  2 files changed, 7 insertions(+), 9 deletions(-)
#  delete mode 100644 README
#  create mode 100644 README.md
</code></pre>

<ul>
<li>如果想更新到 GitHub 的 fork 上，直接 <code>git push origin master</code> 就好了。</li>
</ul>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="如何使用搜索引擎">如何使用搜索引擎</h2><p>其实这个问题并不难，我又被坑了。百度搜的东西不靠谱啊，以后这种问题一定要用<strong>英文</strong>在 <a href="http://www.google.com">Google</a> 或者 <a href="http://cn.bing.com/">Bing</a> 上搜索，这样才能搜到原汁原味的答案。就当是一个教训吧。   </p>
<p>搜索 fork sync，就可以看到 GitHub 自己的帮助文档 <a href="https://help.github.com/articles/syncing-a-fork/">Syncing a fork</a> 点进去看这篇的时候，注意到有一个 Tip: Before you can sync your fork with an upstream repository, you must <a href="https://help.github.com/articles/configuring-a-remote-for-a-fork/">configure a remote that points to the upstream repository</a> in Git.<br>根据这两篇文章，问题迎刃而解！ </p>]]>
    
    </summary>
    
      <category term="GitHub" scheme="http://csuldw.github.io/tags/GitHub/"/>
    
      <category term="GitHub" scheme="http://csuldw.github.io/categories/GitHub/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[代码校验工具 SublimeLinter 的安装与使用]]></title>
    <link href="http://csuldw.github.io/2015/03/26/2015-03-26-sublimeLinter/"/>
    <id>http://csuldw.github.io/2015/03/26/2015-03-26-sublimeLinter/</id>
    <published>2015-03-26T07:14:54.000Z</published>
    <updated>2015-10-26T04:00:46.610Z</updated>
    <content type="html"><![CDATA[<h2 id="序">序</h2><p>本文我将讲述一下 SublimeLinter 的安装过程。<br>其组件 jshint 的安装与使用。<br>其组件 csslint 的安装与使用。<br>我将基于 <a href="http://sublimetext.com/3" target="_blank" rel="external">Sublime Text 3</a> 来安装。<br>使用 Sublime Text 2 的用户阅读本文是没有帮助的。   </p>
<p>SublimeLinter 是 Sublime 的插件，它的作用是检查代码语法是否有错误，并提示。习惯了 IDE 下写代码的人一定需要一款在 Sublime 上类似的语法检查工具。下面我们开始。   </p>
<a id="more"></a>
<hr>
<h2 id="安装_SublimeLinter">安装 SublimeLinter</h2><p>如同其他插件一样使用 Package Control 来安装。   </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter</code>。进行安装.   </li>
</ol>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-sublimeLinter.jpg" alt="SublimeLinter">   </p>
<p>安装完成后可以看到这样一段话：   </p>
<pre><code class="markdown">Welcome to SublimeLinter, a linter framework for Sublime Text 3.

                  * * * IMPORTANT! * * *

         SublimeLinter 3 is NOT a drop-in replacement for
        earlier versions.

         Linters *NOT* included with SublimeLinter 3, 
         they must be installed separately.

         The settings are different.

                 * * * READ THE DOCS! * * *

 Otherwise you will never know how to install linters, nor will
 you know about all of the great new features in SublimeLinter 3.

 For complete documentation on how to install and use SublimeLinter,
 please see:

 http://www.sublimelinter.com</code></pre>   

<p>可以看到具体的 Linters 组件<strong>不</strong>被包含在 SublimeLinter 3 中，所以我们要额外独立安装组件。<br>可以针对不同的语言安装不同的组件。   </p>
<hr>
<h2 id="JavaScript_语法检查">JavaScript 语法检查</h2><p>SublimeLinter-jshint 是基于 nodeJS 下的 jshint 的插件，实际上 SublimeLinter-jshint 调用了 nodeJS 中 jshint 的接口来进行语法检查的。   </p>
<hr>
<h3 id="安装_SublimeLinter-jshint">安装 SublimeLinter-jshint</h3><p>为了让 JavaScript 代码有语法检查，我们安装 SublimeLinter-jshint<br>同样的方法，我们安装 SublimeLinter-jshint    </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter-jshint</code>。进行安装.   </li>
</ol>
<p>如下图   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint.jpg" alt="SublimeLinter-jshint">   </p>
<p>安装完成后我们可以看到下面的一段话   </p>
<h2 id="SublimeLinter-jshint"><pre><code class="markdown">SublimeLinter-jshint</code></pre></h2><p>  This linter plugin for SublimeLinter provides an interface to jshint.</p>
<p>  <strong> IMPORTANT! </strong></p>
<p>  Before this plugin will activate, you <em>must</em><br>  follow the installation instructions here:</p>
<p>  <a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">https://github.com/SublimeLinter/SublimeLinter-jshint</a><br></p>
<hr>
<h3 id="安装_nodeJS_和_jshint">安装 nodeJS 和 jshint</h3><p>在插件开始工作之前，我们必须再看一下上述插件的<a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">安装说明</a><br>通过 <a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">SublimeLinter-jshint 的说明</a> 我们可以看到，这个组件依赖于 nodeJS 下的 jshint，所以我们安装 nodeJS 环境和 nodeJS 下的 jshint。   </p>
<ol>
<li>安装 <a href="https://nodejs.org/" target="_blank" rel="external">Node.js</a>   </li>
<li>通过 npm 安装<code>jshint</code>   </li>
</ol>
<p>在命令行下输入如下代码，完成安装   </p>
<pre><code>npm <span class="keyword">install</span> -g jshint
</code></pre><p>安装完成后命令行中出现如下的信息   </p>
<pre><code><span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\jshint -&gt; <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\jshint\bin\jshint
jshint<span class="variable">@2</span>.<span class="number">6.3</span> <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\jshint
├── strip-json-comments<span class="variable">@1</span>.<span class="number">0.2</span>
├── underscore<span class="variable">@1</span>.<span class="number">6.0</span>
├── exit<span class="variable">@0</span>.<span class="number">1.2</span>
├── shelljs<span class="variable">@0</span>.<span class="number">3.0</span>
├── console-browserify<span class="variable">@1</span>.<span class="number">1.0</span> (date-now<span class="variable">@0</span>.<span class="number">1.4</span>)
├── htmlparser2<span class="variable">@3</span>.<span class="number">8.2</span> (domelementtype<span class="variable">@1</span>.<span class="number">3.0</span>, entities<span class="variable">@1</span>.<span class="number">0.0</span>, domhandler<span class="variable">@2</span>.<span class="number">3.0</span>, readable-stream<span class="variable">@1</span>.<span class="number">1.13</span>, domutils<span class="variable">@1</span>.<span class="number">5.1</span>)
├── minimatch<span class="variable">@1</span>.<span class="number">0.0</span> (sigmund<span class="variable">@1</span>.<span class="number">0.0</span>, lru-cache<span class="variable">@2</span>.<span class="number">5.0</span>)
└── cli<span class="variable">@0</span>.<span class="number">6.6</span> (glob<span class="variable">@3</span>.<span class="number">2.11</span>)
</code></pre><p>可以查看 jshint 版本，已确认安装完成。  </p>
<pre><code>C:<span class="command">\Users</span><span class="command">\Administrator</span>&gt;jshint -v
jshint v2.6.3
</code></pre><p>现在，恭喜你，我们使用 Sublime 编辑 JavaScript 文件，就会有语法检查了！   </p>
<p>在编辑过程中，会有如下提示   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint-test.jpg" alt="SublimeLinter-jshint-test"></p>
<p>点击提示点后，Sublime 状态栏也会有相应的说明   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint-test2.jpg" alt="SublimeLinter-jshint-test2"></p>
<hr>
<h2 id="css_语法检查">css 语法检查</h2><p>与 jshint 同理，SublimeLinter-csslint 也是基于 nodeJS 下的 csslint 的插件，实际上 SublimeLinter-csslint 调用了 nodeJS 中 csslint 的接口来进行语法检查的。   </p>
<hr>
<h3 id="安装_SublimeLinter-csslint">安装 SublimeLinter-csslint</h3><p>同样的方法。   </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter-csslint</code>。进行安装.   </li>
</ol>
<p>如下图   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint.jpg" alt="SublimeLinter-csslint">   </p>
<p>安装完成后我们可以看到下面的一段话   </p>
<pre><code><span class="header">SublimeLinter-csslint
-------------------------------</span>
This linter plugin for SublimeLinter provides an interface to csslint.

<span class="bullet">** </span>IMPORTANT! **

Before this plugin will activate, you <span class="strong">*must*</span>
follow the installation instructions here:

https://github.com/SublimeLinter/SublimeLinter-csslint
</code></pre><p>在使用插件之前，必须遵循上述网址中的<a href="https://github.com/SublimeLinter/SublimeLinter-csslint" target="_blank" rel="external">安装说明</a>   </p>
<hr>
<h3 id="在_nodeJS_下安装_csslint">在 nodeJS 下安装 csslint</h3><p>进入上述的 GitHub 地址，csslint 的说明页。我们知道了和 jshint 一样，csslint 也是基于 nodeJS 下的 csslint 来使用的。   </p>
<p>这里安装 nodeJS 过程省略。<br>只需用 npm 安装 csslint 即可。   </p>
<p>在命令行中输入     </p>
<pre><code>npm <span class="keyword">install</span> -g csslint   
</code></pre><p>安装完成后命令行中出现如下的信息     </p>
<pre><code><span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\csslint -&gt; <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\csslint\cli.js
csslint<span class="variable">@0</span>.<span class="number">10.0</span> <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\csslint
└── parserlib<span class="variable">@0</span>.<span class="number">2.5</span>
</code></pre><p>可以查看 csslint 版本，已确认安装完成。   </p>
<pre><code>C:<span class="command">\Users</span><span class="command">\Administrator</span>&gt;csslint --version
v0.10.0
</code></pre><p>现在，恭喜你，我们使用 Sublime 编辑 css 文件，就会有语法检查了！     </p>
<p>在编辑过程中，会有如下提示   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint-test.jpg" alt="SublimeLinter-csslint-test"></p>
<p>点击提示点后，Sublime 状态栏也会有相应的说明   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint-test2.jpg" alt="SublimeLinter-csslint-test2"></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="序">序</h2><p>本文我将讲述一下 SublimeLinter 的安装过程。<br>其组件 jshint 的安装与使用。<br>其组件 csslint 的安装与使用。<br>我将基于 <a href="http://sublimetext.com/3">Sublime Text 3</a> 来安装。<br>使用 Sublime Text 2 的用户阅读本文是没有帮助的。   </p>
<p>SublimeLinter 是 Sublime 的插件，它的作用是检查代码语法是否有错误，并提示。习惯了 IDE 下写代码的人一定需要一款在 Sublime 上类似的语法检查工具。下面我们开始。   </p>]]>
    
    </summary>
    
      <category term="SublimeLinter" scheme="http://csuldw.github.io/tags/SublimeLinter/"/>
    
      <category term="工具" scheme="http://csuldw.github.io/categories/%E5%B7%A5%E5%85%B7/"/>
    
  </entry>
  
</feed>