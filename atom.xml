<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  
  <title><![CDATA[刘帝伟-技术博客]]></title>
  <subtitle><![CDATA[悄悄是别离的笙箫，沉默是今晚的康桥。]]></subtitle>
  <link href="/atom.xml" rel="self"/>
  <link href="http://csuldw.github.io//"/>
  <updated>2015-07-23T02:29:24.333Z</updated>
  <id>http://csuldw.github.io//</id>
  
  <author>
    <name><![CDATA[刘帝伟]]></name>
    <email><![CDATA[csu.ldw@csu.edu.cn]]></email>
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title><![CDATA[Airbnb欺诈预测机器学习模型设计：准确率和召回率的故事（译）]]></title>
    <link href="http://csuldw.github.io/2015/07/18/2015-07-18-a%20precision%20and%20recall/"/>
    <id>http://csuldw.github.io/2015/07/18/2015-07-18-a precision and recall/</id>
    <published>2015-07-18T04:30:00.000Z</published>
    <updated>2015-07-23T02:29:24.333Z</updated>
    <content type="html"><![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>


<p>Airbnb网站基于允许任何人将闲置的房屋进行长期或短期出租构建商业模式，来自房客或房东的欺诈风险是必须解决的问题。Airbnb信任和安全小组通过构建机器学习模型进行欺诈预测，本文介绍了其设计思想。假想模型是预测某些虚拟人物是否为“反面人物”，基本步骤：构建模型预期，构建训练集和测试集，特征学习，模型性能评估。其中特征转换倾向于采用条件概率编码（CP-coding），评估度量是准确率（Precision）和召回率（Recall），通常偏向于高召回率。<br><a id="more"></a><br><strong>以下为全文内容：</strong></p>
<p>在Airbnb网站上，我们专注于创造一个这样的地方：一个人可以属于任何地方。部分归属感来自于我们用户之间的信任，同时认识到他们的安全是我们最关心的。</p>
<p>虽然我们绝大多数的社区是由友好和可靠的房东和房客组成，但仍然有一小部分用户，他们试图从我们的网站中（非法）获利。这些都是非常罕见的，尽管如此，信任和安全小组还是因此而产生。</p>
<p>信任和安全小组主要是解决任何可能会发生在我们平台的欺诈行为。我们最主要目的是试图保护我们的用户和公司免于不同类型的风险。例如：退款风险——一个绝大多数电子商务企业都熟悉的风险问题。为了减少此类欺诈行为，信任和安全小组的数据科学家构建了不同种类的机器学习模型，用来帮助识别不同类型的风险。想要获得我们模型背后更多的体系结构信息，请参考以前的文章 <a href="http://nerds.airbnb.com/architecting-machine-learning-system-risk/" target="_blank" rel="external">机器学习风险系统的设计</a>。</p>
<p>在这篇文章中，我对机器学习的模型建立给了一个简短的思维过程概述。当然，每个模型都有所不同，但希望它能够给读者在关于机器学习中我们如何使用数据来帮助保护我们的用户以及如何改善模型的不同处理方法上带来一个全新的认识。在这篇文章中，我们假设想要构建一个这样的模型：预测某些虚构的角色是否是反面人物。</p>
<h3 id="试图预测的是什么？">试图预测的是什么？</h3><p>在模型建立中最基本的问题就是明确你想要用这个模型来预测什么。我知道这个听起来似乎很愚蠢，但很多时候，通过这个问题可以引发出其它更深层的问题。</p>
<p>即使是一个看似简单的角色分类模型，随着我们逐步深入地思考，也可以提出许多更深层的问题。例如，我们想要怎样来给这个模型评分：仅仅是给当前新介绍的角色还是给所有角色？如果是前者，我们想要评分的角色和人物介绍中的角色评分相差多远？如果是后者，我们又该多长时间给这些角色评分呢？</p>
<p>第一个想法可能是根据人物介绍中给每个角色的评分来建立模型。然而，这种模型，我们可能不能随着时间的推移动态地追踪人物的评分。此外，我们可能会因为在介绍时的一些“好”的特征而忽略了潜在的反面人物。</p>
<p>相反，我们还可以建立这样一个模型，只要他/她出现在情节里面就评分一次。这将让我们在每个时间段都会有人物评分并检测出任何异常情况。但是，考虑到在每个角色单独出现的情况下可能没有任何的角色类别发展，所以这可能也不是最实际的方法。</p>
<p>深思熟虑之后，我们决定把模型设计成介于这两种想法之间的模型。例如，建立这样一种模型，在每次有意义的事情发生的时候对角色进行评分，比如结交新盟友，龙族领地占领等等。在这种方式下，我们仍然可以随着时间的变化来跟踪人物的评分，同时，对没有最新进展的角色也不会多加评分。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a2b74a98e.jpg" alt=""></p>
<h3 id="如何模拟得分？">如何模拟得分？</h3><p>因为我们的目的是分析每个时期的得分，所以我们的训练集要能反映出某段时间某个角色的类别行为，最后的训练数据集类似于下图：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a2e11dabc_middle.jpg?_=23712" alt=""></p>
<p>与每个角色相关的时间不一定是连续的，因为我们关心的是那些有着重要事件发展的时间。</p>
<p>在这个实例当中，Jarden在3个不同的场合有着重要的角色发展并且在一段时间内持续扩充他的军队。相比之下，Dineas 在5个不同的场合有着重要的角色发展并且主管着4个龙族中心基地。</p>
<h3 id="采样">采样</h3><p>在机器学习模型中，从观测数据中下采样是有必要的。采样过程本身很简单，一旦有了所需要的训练数据集，就可以在数据集上做一个基于行的采样。</p>
<p>然而，由于这里描述的模型是处理每个角色多个时期的样本，基于行采样可能会导致这样一种情况，即在建立模型的数据和用来验证的数据之间，场景附加的人物角色被分离开。如下表所示：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a304d75b7.jpg" alt=""></p>
<p>显然这并不是理想的采样，因为我们没有得到每个角色的整体描述，并且这些缺失的观测数据可能对建立一个好的模型至关重要。</p>
<p>出于这个原因，我们需要做基于角色的采样。这样做能确保在模型数据建立中包含所有场合附加的角色，或者什么都没有。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a31c8bc17.jpg" alt=""></p>
<p>此外，当我们将我们的数据集切分为训练集和测试集时，通常这样的逻辑也适用。</p>
<h3 id="特征设计">特征设计</h3><p>特征设计是机器学习不可或缺的一部分，通常情况下，在特征种类的选择上，对数据的充分理解有助于形成一个更好的模型设计思路。特征设计的实例包括特征规范化和分类特征处理。</p>
<p>特征规范化是标准化特征的一种方式，允许更合理的对比。如下表所示：</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a34d3dd90.jpg" alt=""></p>
<p>从上表可知，每个人物都有10,000个士兵。然而，Serion掌权长达5年，而Dineas仅仅掌权2年。通过这些人物比较绝对的士兵数量可能并不是非常有效的。但是，通过人物掌权的年份来标准化他们可能会提供更好的见解，并且产生更有预测力的特征。</p>
<p>在分类特征的特征设计上值得单独的写一篇博客文章，因为有很多方式可以去处理它们。特别是对于缺失值的插补，请看一看以前的博客文章—— <a href="http://nerds.airbnb.com/overcoming-missing-values-in-a-rfc/" target="_blank" rel="external">使用随机森林分类器处理缺失值</a>。</p>
<p>转换分类特征最常见的方法就是矢量化（也称作one-hot encoding）。然而，在处理有许多不同级别的分类特征时，使用条件概率编码（CP-coding）则更为实用。</p>
<p>CP-coding的基本思想就是在给定的分类级别上，计算出某个特征值发生的概率。这种方法使得我们能够将所有级别的分类特征转化为一个单一的数值型变量。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a3b87ef92.jpg" alt=""></p>
<p>然而，这种类型转换可能会因为没有充分描述的类别而造成噪音数据。在上面的例子中，我们只有一个来自House 为 “Tallight”的观测样本。结果相应的概率就是0或1。为了避免这种问题的发生并且降低噪声数据，通常情况下，可以通过考虑加权平均值，全局概率或者引入一个平滑的超系数来调整如何计算概率。</p>
<p>那么，哪一种方法最好呢？这取决于分类特征的数量和级别。CP-coding是个不错的选择，因为他降低了特征的维数，但是这样会牺牲掉特征与特征之间的互信息，这种方法称之为矢量化保留。此外，我们可以整合这两种方法，即组合相似的类别特征，然后使用CP-coding处理整合的特征。</p>
<h3 id="模型性能评估">模型性能评估</h3><p>当谈及到评估模型性能的时候，我们需要留意正面角色和反面角色的比例。在我们的例子模型中，数据最后的统计格式为[character*period]（下表左）。然而，模型评估应该以角色类别测量（下表右）。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a3fb5a353.jpg" alt=""></p>
<p>结果，在模型的构建数据和模型的评估数据之间的正面人物和反面人物的比例有着明显的差异。当评估模型准确率和召回率的时候分配合适的权重值是相当重要的。</p>
<p>此外，因为我们可能会使用下采样以减少观测样本的数量，所以我们还需要调整模型占采样过程的准确率和召回率。</p>
<h3 id="评估准确率和召回率">评估准确率和召回率</h3><p>对于模型评估的两种主要的评估度量是准确率（Precision）和召回率（Recall）。在我们的例子当中，准确率是预测结果为反面角色中被正确预测为反面角色的比例。它在给定的阈值下衡量模型的准确度。另外，召回率是模型从原本为反面角色当中能够正确检测出为反面角色的比例。它在一个给定的阈值下以识别反面人物来衡量模型的综合指标。这两个变量很容易混淆，所以通过下表会更加的直观看出两者的不同。</p>
<p><img src="http://img.ptcms.csdn.net/article/201507/13/55a2a428441f6.jpg" alt=""></p>
<p>通常将最后的数据划分为四个不同的部分：</p>
<p>True Positives（TP）：角色是反面人物，模型预测为反面人物；<br>False Positives（FP）：角色是正面人物，模型预测为反面人物；<br>True Negatives（TN）：角色是正面人物，模型预测为正面人物；<br>False Negatives（FN）：角色是反面人物，模型预测为正面人物；<br>准确率计算：在所有被预测为反面人物中，模型正确预测的比例，即TP /（TP + FP）。</p>
<p>召回率计算：在所有原本就是反面人物中，模型正确预测的比例，即TP / (TP + FN）。</p>
<p>通过观察可以看出，尽管准确率和召回率的分子是相同的，但分母不同。</p>
<p>通常在选择高准确率和高召回率之间总有一种权衡。这要取决于构建模型的最终目的，对于某些情况而言，高准确率的选择可能会优于高召回率。然而，对于欺诈预测模型，通常要偏向于高召回率，即使会牺牲掉一些准确率。</p>
<p>有许多的方式可以用来改善模型的准确度和召回率。其中包括添加更好的特征，优化决策树剪枝或者建立一个更大的森林等等。不过，鉴于讨论广泛，我打算将其单独地放在一篇文章当中。</p>
<h3 id="结束语">结束语</h3><p>希望这篇文章能让读者了解到什么是构建机器学习模型所需要的。遗憾的是，没有放之四海而皆准的解决方案来构建一种好的模型，充分了解数据的上下文是关键，因为通过它我们能够从中提取出更多更好的预测特征，从而建立出更优化的模型。</p>
<p>最后，虽然将角色分为正面和反面是主观的，但类别标签的确是机器学习的一个非常重要的部分，而不好的类别标签通常会导致一个糟糕的模型。祝建模快乐!</p>
<p>注：这个模型确保每个角色都是正面角色或者是反面角色，即如果他们生来就是反面角色，那么在他们的整个生命当中都是反面角色。如果我们假设角色可以跨越类别标签作为中立人物，那么模型的设计将会完全不同。</p>
<p>英文原文： <a href="http://nerds.airbnb.com/designing-machine-learning-models/" target="_blank" rel="external">Designing Machine Learning Models: A Tale of Precision and Recall</a>（译者/刘帝伟 审校/刘翔宇、朱正贵 责编/周建丁）</p>
<p>关于译者： <a href="http://my.csdn.net/Dream_angel_Z" target="_blank" rel="external">刘帝伟</a>，中南大学在读研究生，关注机器学习、数据挖掘及生物信息领域。 </p>
<hr>
<p>本文为CSDN编译整理，未经允许不得转载，如需转载请联系market#csdn.net(#换成@)</p>
]]></content>
    <summary type="html">
    <![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>


<p>Airbnb网站基于允许任何人将闲置的房屋进行长期或短期出租构建商业模式，来自房客或房东的欺诈风险是必须解决的问题。Airbnb信任和安全小组通过构建机器学习模型进行欺诈预测，本文介绍了其设计思想。假想模型是预测某些虚拟人物是否为“反面人物”，基本步骤：构建模型预期，构建训练集和测试集，特征学习，模型性能评估。其中特征转换倾向于采用条件概率编码（CP-coding），评估度量是准确率（Precision）和召回率（Recall），通常偏向于高召回率。<br>]]>
    
    </summary>
    
      <category term="machine-learning" scheme="http://csuldw.github.io/tags/machine-learning/"/>
    
      <category term="Machine-Learning" scheme="http://csuldw.github.io/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[开发者成功使用机器学习的十大诀窍(译)]]></title>
    <link href="http://csuldw.github.io/2015/07/13/2015-07-13-10%20keys%20to%20successful%20machine%20learning%20for%20developers/"/>
    <id>http://csuldw.github.io/2015/07/13/2015-07-13-10 keys to successful machine learning for developers/</id>
    <published>2015-07-13T13:53:12.000Z</published>
    <updated>2015-07-23T02:29:24.375Z</updated>
    <content type="html"><![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>

<p>在提供发现埋藏数据深层的模式的能力上，机器学习有着潜在的能力使得应用程序更加的强大并且更能响应用户的需求。精心调校好的算法能够从巨大的并且互不相同的数据源中提取价值，同时没有人类思考和分析的限制。对于开发者而言，机器学习为应用业务的关键分析提供了希望，从而实现从改善客户体验到提供产品推荐上升至超个性化内容服务的任何应用程序。<br><a id="more"></a><br>像Amazon和Micorosoft这样的云供应商提供云功能的机器学习解决方案，承诺为开发者提供一个简单的方法，使得机器学习的能力能够融入到他们的应用程序当中，这也算是最近的头条新闻了。承诺似乎很好，但开发者还需谨慎。</p>
<p>对于开发人员而言，基于云的机器学习工具带来了使用机器学习创造和提供新的功能的可能性。然而，当我们使用不当时，这些工具会输出不好的结果，用户可能会因此而感到不安。测试过<a href="http://how-old.net/" target="_blank" rel="external">微软年龄检测机器学习工具</a>的人都会发现，伴随即插即用的易用性而来的是主要的精度问题——对于关键应用程序或者是重大决策，它应该不值得信赖。</p>
<p>想要在应用程序中成功地融入机器学习的开发者，需要注意以下的一些关键要点：</p>
<p><strong>1.算法使用的数据越多，它的精度会更加准确，所以如果可能要尽量避免抽样。</strong>机器学习理论在预测误差上有着非常直观的描述。简而言之，在机器学习模型和最优预测（在理论上达到最佳可能的误差）之间的预测误差的差距可以被分解为三个部分：</p>
<ul>
<li>由于没有找到正确函数形式的模型的误差</li>
<li>由于没有找到最佳参数的模型的误差</li>
<li>由于没用使用足够数据的模型的误差</li>
<li>如果训练集有限，它可能无法支撑解决这个问题所需的模型复杂性。统计学的基本规律告诉我们，如果我们可以的话，应该利用所有的数据而不是抽样。</li>
</ul>
<p><strong>2.对给定的问题选择效果最好的机器学习算法是决定成败的关键。</strong>例如，梯度提升树（GBT）是一个非常受欢迎的监督学习算法，由于其精度而被业内开发人员广泛使用。然而，尽管其高度受欢迎，我们也不能盲目的把这种算法应用于任何问题上。相反，我们使用的算法应该是能够最佳地拟合数据特征同时能够保证精度的算法。</p>
<p>为了证明这个观点，尝试做这样一个实验，在数据集 <a href="http://www.daviddlewis.com/resources/testcollections/rcv1/" target="_blank" rel="external">the popular text categorization dataset rcv1</a>上测试GBT算法和线性支持向量机（SVM）算法，并比较两者的精度。我们观察到在这个问题上，就错误率而言，线性SVM要优于GBT算法。这是因为在文本领域当中，数据通常是高维的。一个线性分类器能够在N-1维当中完美的分离出N个样本，所以，一个样本模型在这种数据上通常表现的更好。此外，模型越简单，通过利用有限的训练样本来避免过拟合的方式学习参数，并且提供一个精确的模型，产生的问题也会随之越少。</p>
<p>另一方面，GBT是高度非线性的并且更加强大，但是在这种环境中却更难学习并且更容易发生过拟合，往往结果精度也较低。</p>
<p><strong>3.为了得到一个更好的模型，必须选择最佳的的算法和相关的参数。</strong>这对于非数据科学家而言可能不容易。现代的机器学习算法有许多的参数可以调整。例如，对于流行的GBT算法单独的就有十二个参数可以设置，其中包括如何控制树的大小，学习率，行或列的采样方法，损失函数，正则化选项等等。一个特有的项目需要在给定的数据集上为每一个参数找到其最优值并且达到最精准的精度，这确实不是一件容易的事。但是为了得到最佳的结果，数据科学家需要训练大量的模型，而直觉和经验会帮助他们根据交叉验证的得分，然后决定使用什么参数再次尝试。</p>
<p><strong>4.机器学习模型会随着好的数据而变得更好，错误的数据收集和数据处理会降低你建立预测和归纳的机器学习模型的能力。</strong>根据经验，建议仔细审查与主题相关的数据，从而深入了解数据和幕后数据的生成过程。通常这个过程可以识别与记录、特征、值或采样相关的数据质量问题。</p>
<p><strong>5.理解数据特征并改进它们（通过创造新的特征或者去掉某个特征）对预测能力有着高度的影响。</strong>机器学习的一个基本任务就是找到能够被机器学习算法充分利用的丰富特征空间来替代原始数据。例如，特征转换是一种流行的方法，可以通过在原始数据的基础上使用数学上的转换提取新的特征来实现。最后的特征空间（也就是最后用来描述数据的特征）要能更好的捕获数据的多复杂性（如非线性和多种特征之间的相互作用），这对于成功的学习过程至关重要。</p>
<p><strong>6.在应用中，选择合适的灵感来自商业价值的目标函数/损失函数对于最后的成功至关重要。</strong>几乎所有的机器学习算法最后都被当成是一种优化问题。根据业务的性质，合理设置或调整优化的目标函数，是机器学习成功的关键。</p>
<p>以支持向量机为例，通过假设所有错误类型的权重相等，对一个二分类问题的泛化误差进行了优化。这对损失敏感的问题并不合适，如故障检测，其中某些类型的错误比重可能比其它类型的要高。在这种情况下，建议通过在特定的错误类型上，增加更多的惩罚来解释它们的权重，从而调整SVM的损失函数。</p>
<p><strong>7.确保正确地处理训练数据和测试数据，如此当在生产中部署该模型时，测试数据能够模拟输入数据。</strong>例如，我们可以看到，这对于时间依赖性数据是多么的重要。在这种情况下，使用标准的交叉验证方法进行训练，调整，那么测试模型的结果可能会有偏差，甚至会不准确。这是因为在实施平台上它不能准确的模拟输入数据的性质。为了纠正这一点，在部署时我们必须仿照模型来部署使用。我们应该使用一个基于时间的交叉验证，用时间较新的数据来验证训练模型。</p>
<p><strong>8.部署前理解模型的泛化误差。泛化误差衡量模型在未知数据上的性能好坏。</strong>因为一个模型在训练数据上的性能好并不意味着它在未知的数据上的表现也好。一个精心设计的模拟实际部署使用的模型评估过程，是估计模型泛化误差所需要的。</p>
<p>一不留心就很容易违反交叉验证的规则，并且也没有一种显而易见的方法来表现交叉验证的非正确性，通常在你试图寻找快捷方式计算时发生。在任何模型部署之前，有必要仔细注意交叉验证的正确性，以获得部署性能的科学评估。</p>
<p><strong>9.知道如何处理非结构化和半结构化数据，如文本、时间序列、空间、图形或者图像数据。</strong>大多数机器学习算法在处理特征空间中的数据时，一个特征集代表一个对象，特征集的每一个元素都描述对象的一个特点。在实际当中，数据引进时并不是这种格式化的形式，往往来自于最原始的格式，并且最后都必须被改造成机器学习算法能够识别的理想格式。比如，我们必须知道如何使用各种计算机视觉技术从图像中提取特征或者如何将自然语言处理技术应用于影片文本。</p>
<p><strong>10.学会将商业问题转换成机器学习算法。</strong>一些重要的商业问题，比如欺诈检测、产品推荐、广告精准投放，都有“标准”的机器学习表达形式并且在实践当中取得了合理的成就。即使对于这些众所周知的问题，也还有鲜为人知但功能更强大的表达形式，从而带来更高的预测精度。对于一般在博客和论坛中讨论的小实例的商业问题，适当的机器学习方法则不太明显。</p>
<p>如果你是一个开发者，学习这十个通往成功的诀窍可能似乎是一个艰难的任务，但是不要气馁。事实上，开发者不是数据科学家。认为开发人员可以充分利用所有的机学习工具是不公平的。但是这并不意味着开发人员没有机会去学习一些有水准的数据科学从而改进他们的应用。随着适当的企业解决方案和自动化程度的提高，开发人员可以做模型构建到实施部署的一切事情，使用机器学习最佳实践来保持高精度。</p>
<p>自动化是在应用程序中扩展机器学习的关键。即使你能够供得起一批小的数据科学家团队和开发者携手合作，也没有足够的人才。像Skytree的AutoModel（自动化模型）能够帮助开发者自动地确定最佳的参数并且使得算法得到最大的模型精度。一个易于使用的接口可以引导开发人员通过训练加工，调整并且测试模型来防止统计上的错误。</p>
<p>自动化机器学习过程，有许多方式，包括数据科学家或开发者的人工智能原理，允许算法去思考，学习并且承受更多的建模重任。也就是说，认为数据科学家能够从机器学习中解耦是错误的，特别是在关键任务模型上。谨防这种能够简单使用机器学习功能的承诺，即能够在不需要正确复杂的思考下或者可扩展的应用技术下就使用机器学习——这通常并不会得到高预测精度和机器学习提供的高商业价值结果。更糟糕的是，在应用程序中使用不好的模型实际上可能会适得其反，并迅速在其用户之间建立不信任的产品或服务。</p>
<p>英文原文： <a href="http://www.infoworld.com/article/2943862/application-development/what-developers-need-to-know-about-machine-learning.html" target="_blank" rel="external">10 keys to successful machine learning for developers</a> （译者/<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a> 审校/刘翔宇、朱正贵 责编/周建丁）</p>
<p>作者简介：Alexander Gray，Skytree首席技术官，佐治亚理工学院计算机学院副教授，主要致力于大规模数据集的机器学习算法技术研发，1993年开始在NASA喷气推进实验室机器学习系统小组从事大规模科学数据的工作。</p>
<p>译者简介： <a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">刘帝伟</a>，中南大学软件学院在读研究生，关注机器学习、数据挖掘及生物信息领域。</p>
<p>【预告】<a href="http://ccai2015.csdn.net/" target="_blank" rel="external">首届中国人工智能大会（CCAI 2015）</a>将于7月26-27日在北京友谊宾馆召开。机器学习与模式识别、大数据的机遇与挑战、人工智能与认知科学、智能机器人四个主题专家云集。人工智能产品库将同步上线，预约咨询：QQ：1192936057。欢迎关注。</p>
<p>本文为CSDN编译整理，未经允许不得转载，如需转载请联系market#csdn.net(#换成@)</p>
]]></content>
    <summary type="html">
    <![CDATA[<div style="text-align:right;padding-bottom:7px;">译者：<a href="http://blog.csdn.net/dream_angel_z">刘帝伟</a>   审校：刘翔宇 朱正贵   责编：周建丁</div>

<p>在提供发现埋藏数据深层的模式的能力上，机器学习有着潜在的能力使得应用程序更加的强大并且更能响应用户的需求。精心调校好的算法能够从巨大的并且互不相同的数据源中提取价值，同时没有人类思考和分析的限制。对于开发者而言，机器学习为应用业务的关键分析提供了希望，从而实现从改善客户体验到提供产品推荐上升至超个性化内容服务的任何应用程序。<br>]]>
    
    </summary>
    
      <category term="machine-learning CV AI" scheme="http://csuldw.github.io/tags/machine-learning-CV-AI/"/>
    
      <category term="Machine-Learning" scheme="http://csuldw.github.io/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[数据预处理-PDB文件]]></title>
    <link href="http://csuldw.github.io/2015/07/07/2015-07-07-PDB/"/>
    <id>http://csuldw.github.io/2015/07/07/2015-07-07-PDB/</id>
    <published>2015-07-07T15:23:23.000Z</published>
    <updated>2015-07-23T02:29:24.460Z</updated>
    <content type="html"><![CDATA[<p>以下代码为个人原创，python实现，是处理PDB文件的部分常用代码，仅供参考！</p>
<h3 id="1-下载PDB文件">1.下载PDB文件</h3><p>下面是一个下载PDB文件的函数，传入的参数是一个写有pdb名字的namefile文件，函数的核心部分是三个系统命令，先通过wget下载，然后解压，最后替换名字。</p>
<a id="more"></a>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">downloadpdb</span><span class="params">(namefile)</span>:</span></span><br><span class="line">    inputfile = open(namefile, <span class="string">'r'</span>)</span><br><span class="line">    <span class="keyword">for</span> eachline <span class="keyword">in</span> inputfile:</span><br><span class="line">        pdbname = eachline.lower().strip()</span><br><span class="line">        os.system(<span class="string">"wget http://ftp.wwpdb.org/pub/pdb/data/structures/all/pdb/pdb"</span> + pdbname + <span class="string">".ent.gz"</span>)</span><br><span class="line">        os.system(<span class="string">"gzip -d pdb"</span> + pdbname + <span class="string">'.ent.gz'</span>)</span><br><span class="line">        os.system(<span class="string">"mv pdb"</span> + pdbname + <span class="string">".ent "</span> + pdbname.upper() + <span class="string">'.pdb'</span>)</span><br></pre></td></tr></table></figure>
<p>测试用例</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">'/ifs/home/liudiwei/datasets/RPdatas'</span>)</span><br><span class="line">downloadpdb(<span class="string">'protein.name'</span>)</span><br></pre></td></tr></table></figure>
<h3 id="2-PDB转DSSP">2.PDB转DSSP</h3><p>将下载的PDB文件转成DSSP文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 处理一行dssp数据</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">formatdsspline</span><span class="params">(dsspline)</span>:</span></span><br><span class="line">    eachline  = dsspline</span><br><span class="line">    col = <span class="string">'\t'</span> + eachline[<span class="number">0</span>:<span class="number">5</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">5</span>:<span class="number">10</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">10</span>:<span class="number">12</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">12</span>:<span class="number">15</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">15</span>:<span class="number">25</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">25</span>:<span class="number">39</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">29</span>:<span class="number">34</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">34</span>:<span class="number">38</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">38</span>:<span class="number">50</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">50</span>:<span class="number">61</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">61</span>:<span class="number">72</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">72</span>:<span class="number">83</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">83</span>:<span class="number">92</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">92</span>:<span class="number">97</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">97</span>:<span class="number">103</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">103</span>:<span class="number">109</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">109</span>:<span class="number">115</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">115</span>:<span class="number">122</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">122</span>:<span class="number">129</span>]</span><br><span class="line">    col += <span class="string">'\t'</span> + eachline[<span class="number">129</span>:<span class="number">136</span>]</span><br><span class="line">    <span class="keyword">return</span> col</span><br></pre></td></tr></table></figure>
<p>PDB转DSSP格式，需要DSSP软件</p>
<p>参数：</p>
<ul>
<li>pdbdir: pdb文件目录   </li>
<li>dsspdir: 生成的dssp文件目录（需创建）</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">pdbToDSSP</span><span class="params">(pdbnamefile,pdbdir, dsspdir)</span>:</span>    </span><br><span class="line">    pdbfiles = os.listdir(pdbdir)</span><br><span class="line">    <span class="comment">#对于每个pdb文件，生成对应的dssp文件，并保存在dssp目录下</span></span><br><span class="line">    <span class="keyword">for</span> pdb_file <span class="keyword">in</span> pdbfiles:</span><br><span class="line">        pdb_name = pdb_file.split(<span class="string">'.'</span>)[<span class="number">0</span>].upper()</span><br><span class="line">        command = <span class="string">'DSSPCMBI.EXE -x '</span> + pdbdir +<span class="string">'/'</span>+ pdb_file + <span class="string">'  '</span>+ dsspdir +<span class="string">"/"</span>+ pdb_name +<span class="string">'.dssp'</span></span><br><span class="line">        os.system(command) </span><br><span class="line">    dsspfiles = os.listdir(dsspdir)</span><br><span class="line">    <span class="keyword">if</span> os.path.exists(dsspdir + <span class="string">"/DSSP"</span>):      <span class="comment">#判断DSSP文件是否存在，存在则删除</span></span><br><span class="line">        dsspfiles.remove(<span class="string">"DSSP"</span>)</span><br><span class="line">    output=open(dsspdir + <span class="string">'/DSSP'</span>,<span class="string">'w'</span>)</span><br><span class="line">    <span class="comment">#循环读取dssp文件，将其合并成一个整的DSSP</span></span><br><span class="line">    <span class="keyword">with</span> open(pdbnamefile, <span class="string">'r'</span>) <span class="keyword">as</span> namefile:</span><br><span class="line">        <span class="keyword">for</span> eachline <span class="keyword">in</span> namefile:</span><br><span class="line">            pdb_name = eachline.strip() </span><br><span class="line">            dssp_file = pdb_name + <span class="string">'.dssp'</span></span><br><span class="line">        <span class="comment">#for dssp_file in dsspfiles:</span></span><br><span class="line">            <span class="comment">#pdb_name = dssp_file.split('.')[0]</span></span><br><span class="line">            <span class="keyword">with</span> open(dsspdir + <span class="string">'/'</span> + dssp_file,<span class="string">"r"</span>) <span class="keyword">as</span> f:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(dsspdir + <span class="string">'/format'</span>):</span><br><span class="line">                    os.mkdir(dsspdir + <span class="string">'/format'</span>)</span><br><span class="line">                <span class="keyword">with</span> open(dsspdir + <span class="string">'/format/'</span> + pdb_name + <span class="string">'.dssp.format'</span>,<span class="string">'w'</span>) <span class="keyword">as</span> singleOut:</span><br><span class="line">                    count = <span class="number">0</span>; preRes=[]</span><br><span class="line">                    sets = set(<span class="string">''</span>);content=<span class="string">''</span>   </span><br><span class="line">                    <span class="keyword">for</span> eachline <span class="keyword">in</span> f.readlines():</span><br><span class="line">                        list1=[];oneline=[]</span><br><span class="line">                        count+=<span class="number">1</span></span><br><span class="line">                        list1.append(pdb_name)                             </span><br><span class="line">                        <span class="keyword">if</span> count &gt;= <span class="number">29</span>:</span><br><span class="line">                            eachline = formatdsspline(eachline)</span><br><span class="line">                            oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line"></span><br><span class="line">                            <span class="keyword">if</span> oneline[<span class="number">3</span>].strip():</span><br><span class="line">                                preRes = oneline[<span class="number">3</span>].strip()                        </span><br><span class="line">                            </span><br><span class="line">                            list1.append(eachline)</span><br><span class="line">                            content += <span class="string">""</span>.join(list1)+<span class="string">'\n'</span>                            </span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'!'</span> == oneline[<span class="number">4</span>].strip():</span><br><span class="line">                                <span class="keyword">continue</span></span><br><span class="line">                            </span><br><span class="line">                            <span class="keyword">if</span> <span class="string">'!*'</span> <span class="keyword">in</span> eachline <span class="keyword">or</span> <span class="keyword">not</span> oneline[<span class="number">3</span>].strip():</span><br><span class="line">                                <span class="keyword">if</span> preRes <span class="keyword">in</span> sets <span class="keyword">and</span> len(sets):</span><br><span class="line">                                    content=<span class="string">''</span></span><br><span class="line">                                    preRes=[]</span><br><span class="line">                                    <span class="keyword">continue</span></span><br><span class="line">                                sets.add(preRes)</span><br><span class="line">                                output.write(content)</span><br><span class="line">                                singleOut.write(content)</span><br><span class="line">                                content=<span class="string">''</span></span><br><span class="line">                    <span class="keyword">if</span> preRes <span class="keyword">and</span> preRes <span class="keyword">not</span> <span class="keyword">in</span> sets:</span><br><span class="line">                        output.write(content)</span><br><span class="line">                        singleOut.write(content)</span><br><span class="line">    output.close()</span><br></pre></td></tr></table></figure>
<p>测试</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#test</span></span><br><span class="line">pdbdir = <span class="string">'z:/datasets/protein/pdb'</span></span><br><span class="line">dsspdir = <span class="string">'Z:/datasets/protein/DSSPdir'</span> </span><br><span class="line">proname = <span class="string">'Z:/datasets/protein/protein.name'</span></span><br><span class="line">pdbToDSSP(proname,pdbdir, dsspdir)</span><br></pre></td></tr></table></figure>
<h3 id="3-DSSP抽取序列">3.DSSP抽取序列</h3><p>从一个整合的DSSP文件中抽取序列文件 </p>
<p>从格式化后的dssp文件获取序列信息</p>
<p>参数：dsspfile为格式过的DSSP文件,seqfile为输出的序列文件,同时输出序列文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getSeqFromDSSP</span><span class="params">(dsspfile, seqfile, minLen)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(dsspfile, <span class="string">'r'</span>) <span class="keyword">as</span> inputfile:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> seqfile.strip():</span><br><span class="line">            seqfile = <span class="string">'protein'</span>+minLen + <span class="string">'.dssp.seq'</span></span><br><span class="line">        outchain = open(<span class="string">'protein40.chain.all'</span>, <span class="string">'w'</span>)</span><br><span class="line">        <span class="keyword">with</span> open(seqfile, <span class="string">'w'</span>) <span class="keyword">as</span> outputfile:</span><br><span class="line">            residue=[];Ntype=[]</span><br><span class="line">            preType=[];preRes=[]</span><br><span class="line">            firstline=[];secondline=[];content=<span class="string">''</span></span><br><span class="line">            <span class="keyword">for</span> eachline <span class="keyword">in</span> inputfile:</span><br><span class="line">                oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line">                residue = oneline[<span class="number">0</span>]</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> residue.strip(): </span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                Ntype = oneline[<span class="number">3</span>].strip()</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> Ntype.strip():</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> preRes!=residue:</span><br><span class="line">                    content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">                    <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span> <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:</span><br><span class="line">                        outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                        outputfile.write(content)</span><br><span class="line">                    firstline=[]</span><br><span class="line">                    firstline.append(<span class="string">'&gt;'</span> + residue + <span class="string">':'</span> + Ntype)</span><br><span class="line">                    secondline=[];secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">                    preRes = residue;preType = Ntype</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                <span class="keyword">if</span> Ntype != preType:</span><br><span class="line">                    content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">                    <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span>  <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:</span><br><span class="line">                        outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                        outputfile.write(content)</span><br><span class="line">                    firstline=[]</span><br><span class="line">                    firstline.append(<span class="string">'&gt;'</span> + residue + <span class="string">':'</span> + Ntype)</span><br><span class="line">                    secondline=[];secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">                    preRes = residue;preType = Ntype</span><br><span class="line">                <span class="keyword">else</span>: <span class="comment">#如果Ntype不为空，且等于preType</span></span><br><span class="line">                    secondline.append(oneline[<span class="number">4</span>].strip())</span><br><span class="line">            content = <span class="string">''</span>.join(firstline)+<span class="string">'\n'</span> + <span class="string">''</span>.join(secondline) +<span class="string">'\n'</span></span><br><span class="line">            <span class="keyword">if</span> len(secondline) &gt;=minLen <span class="keyword">and</span> <span class="keyword">not</span> <span class="string">'X'</span> <span class="keyword">in</span> secondline:  <span class="comment">#选择长度大于40</span></span><br><span class="line">                outchain.write(<span class="string">''</span>.join(firstline) + <span class="string">'\n'</span>)</span><br><span class="line">                outputfile.write(content)</span><br><span class="line">        outchain.close()</span><br></pre></td></tr></table></figure>
<p>测试：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">os.chdir(<span class="string">r"E:\3-CSU\Academic\_Oriented\analysis\experiment\Datasets\ptest.dssp"</span>)</span><br><span class="line">pdbfile = <span class="string">'DSSP'</span></span><br><span class="line">outfile = <span class="string">'protein.seq'</span></span><br><span class="line">getSeqFromDSSP(pdbfile,outfile)</span><br></pre></td></tr></table></figure>
<h3 id="4-对序列做blast聚类">4.对序列做blast聚类</h3><p>设置相应的参数，在服务器上跑blast，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifs/share/lib/cd-hit-v4.5.4/cd-hit -i /ifs/home/liudiwei/datasets/protein40.seq -o /ifs/home/liudiwei/experiment/cdhit/fasta.40 -c <span class="number">0.4</span> -n <span class="number">2</span> -M <span class="number">2000</span></span><br></pre></td></tr></table></figure>
<h3 id="5-生成聚类后的DSSP，得到protein-name、protein-seq、protein-chain三个文件">5.生成聚类后的DSSP，得到protein.name、protein.seq、protein.chain三个文件</h3><p>从原来的DSSP文件中，根据聚类后的链名抽取新的DSSP文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"> <span class="comment"># extract dssp from old dssp file</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extractDSSP</span><span class="params">(dsspfile, chainname, outfile)</span>:</span></span><br><span class="line">    <span class="keyword">with</span> open(outfile, <span class="string">'w'</span>) <span class="keyword">as</span> outdssp:</span><br><span class="line">        <span class="keyword">with</span> open(dsspfile, <span class="string">'r'</span>) <span class="keyword">as</span> inputdssp:</span><br><span class="line">            <span class="keyword">for</span> eachline <span class="keyword">in</span> inputdssp:</span><br><span class="line">                oneline = eachline.split(<span class="string">'\t'</span>)</span><br><span class="line">                <span class="comment">#preNum = oneline[2].strip()     </span></span><br><span class="line">                <span class="keyword">with</span> open(chainname,<span class="string">'r'</span>) <span class="keyword">as</span> chainfile:       </span><br><span class="line">                    <span class="keyword">for</span> eachchain <span class="keyword">in</span> chainfile:</span><br><span class="line">                        protein_ame = eachchain[<span class="number">1</span>:<span class="number">5</span>]</span><br><span class="line">                        chain_id = eachchain[<span class="number">6</span>:<span class="number">7</span>]</span><br><span class="line">                        <span class="keyword">if</span> oneline[<span class="number">0</span>].strip() == protein_ame <span class="keyword">and</span> oneline[<span class="number">3</span>].strip() == chain_id:</span><br><span class="line">                            outdssp.write(eachline)</span><br><span class="line">                            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>
<p>测试实例：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dsspfile = <span class="string">'/ifs/home/liudiwei/datasets/832.protein/DSSPdir1/DSSP'</span></span><br><span class="line">chainname = <span class="string">'/ifs/home/liudiwei/experiment/step1/832p.cluster/cdhit/protein.chain'</span></span><br><span class="line">outfile = <span class="string">'/ifs/home/liudiwei/experiment/step1/832p.cluster/cdhit/DSSP'</span></span><br><span class="line">extractDSSP(dsspfile, chainname, outfile )</span><br></pre></td></tr></table></figure>
<hr>
<center><br><strong>本栏目持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong><br></center>]]></content>
    <summary type="html">
    <![CDATA[<p>以下代码为个人原创，python实现，是处理PDB文件的部分常用代码，仅供参考！</p>
<h3 id="1-下载PDB文件">1.下载PDB文件</h3><p>下面是一个下载PDB文件的函数，传入的参数是一个写有pdb名字的namefile文件，函数的核心部分是三个系统命令，先通过wget下载，然后解压，最后替换名字。</p>]]>
    
    </summary>
    
      <category term="BioInfo Python" scheme="http://csuldw.github.io/tags/BioInfo-Python/"/>
    
      <category term="BioInfo Python" scheme="http://csuldw.github.io/categories/BioInfo-Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-Adaboost]]></title>
    <link href="http://csuldw.github.io/2015/07/05/2015-07-12-Adaboost/"/>
    <id>http://csuldw.github.io/2015/07/05/2015-07-12-Adaboost/</id>
    <published>2015-07-05T14:53:12.000Z</published>
    <updated>2015-07-23T02:29:24.423Z</updated>
    <content type="html"><![CDATA[<p><strong>本章内容</strong></p>
<ul>
<li>组合相似的分类器来提高分类性能</li>
<li>应用AdaBoost算法</li>
<li>处理非均衡分类问题</li>
</ul>
<p><strong>主题：</strong>利用AdaBoost元算法提高分类性能</p>
<a id="more"></a>
<h3 id="1-基于数据集多重抽样的分类器">1.基于数据集多重抽样的分类器</h3><table>
<thead>
<tr>
<th>Feature</th>
<th>AdaBoost</th>
</tr>
</thead>
<tbody>
<tr>
<td>优点</td>
<td>泛化错误率低，易编码，可以应用在大部分分类器上，无需参数调整</td>
</tr>
<tr>
<td>缺点</td>
<td>对离群点敏感</td>
</tr>
<tr>
<td>数据</td>
<td>数值型和标称型数据</td>
</tr>
</tbody>
</table>
<p><br></p>
<p>bagging:基于数据随机重抽样的分类器构建方法</p>
<p>自举汇聚法(bootstrap aggregating),也称为bagging方法，是从原始数据集选择S次后得到S个新数据集的一种技术。新数据集和原始数据集的大小相等。每个数据集都是通过在原始数据集中随机选择一个本来进行替换而得到的。</p>
<p>在S个数据集建好之后，将某个学习算法分别作用域每个数据集得到了S个分类器。当我们对新数据进行分类时，就可以应用S个分类器进行分类。与此同时，选择分类器投票结果最多的类别作为最后的分类结果。</p>
<p>有一些比较先进的bagging方法，如<strong>随机森林</strong>（RF）。</p>
<p>boosting是一种与bagging很类似的技术。不论是boosting还是bagging当中，当使用的多个分类器的类型都是一致的。但是在前者当中，不同的分类器是通过串行训练而获得的，每个新分类器都根据已训练出的分类器的性能来进行训练。boosting是通过训练集中关注被已有分类器错分的那些数据来获得新的分类器。</p>
<p>boosting方法有多个版本，当前最流行便属于<strong>AdaBoost</strong>。</p>
<p><strong>AdaBoost的一般流程</strong></p>
<p>（1）收集数据：可以使用任何方法；<br><br>（2）准备数据：依赖于所使用的若分类器类型；<br><br>（3）分析数据：可以使用任意方法<br><br>（4）训练算法：AdaBoost的大部分时间都用在训练上，分类器将多次在同一数据集上训练若分类器；<br><br>（5）测试算法：计算分类的错误率；<br><br>（6）使用算法：同SVM一样，AdaBoost预测的两个类别中的一个，如果想要把它应用到多个类的场合，那么就像多类SVM中的做法一样对AdaBoost进行修改。</p>
<h3 id="2-训练算法：基于错误提升分类器的性能">2.训练算法：基于错误提升分类器的性能</h3><p>AdaBoost是adaptive boosting（自适应boosting）的缩写，其运行过程：训练集中的每个样本，赋予其一个权重，这些权重构成向量D。一开始，这些权重都初试化成相等值。首先在训练数据上训练处一个若分类器并计算该分类器的错误率，然后在同一数据集上再次训练若分类器。在分类器的第二次训练当中，将会重新调整每个样本的权重，其中第一次分队的样本的权重值将会降低，而第一次分错的样本的权重将会提高。为了从所有分类器中得到最终的分类结果，AdaBoost为每个分类器都分配了一个权重值alpha，这些alpha值是基于每个分类器的错误率进行计算的。其中错误率定义为</p>
<p>$$\epsilon=\dfrac{为正确分类的样本数目}{所有样本数目}$$</p>
<p>alpha计算公式</p>
<p>$$\alpha=\dfrac{1}{2}ln(\dfrac{1-\epsilon}{\epsilon})$$</p>
<p>计算出alpha值之后，可以对权重向量D进行更新，使得正确分类的样本的权重值降低而分错的样本权重值升高，D的计算方法如下<br>如果某个样本被正确分类，更新该样本权重值为：</p>
<p>$$D^{(t+1)}_i=\dfrac{D_i^{(t)} e^{-\alpha}}{Sum(D)}$$</p>
<p>如果某个样本被错误分类，更新该样本的权重值为：</p>
<p>$$D^{(t+1)}_i=\dfrac{D_i^{(t)} e^{\alpha}}{Sum(D)}$$</p>
<p>计算出D后，AdaBoost接着开始下一轮的迭代。AdaBoost算法会不断地重复训练和调整权重的过程，知道训练错误率为0或者若分类器的数目达到用户指定值为止。</p>
<p>在建立完整的AdaBoost算法之前，需要通过一些代码建立若分类器及保存数据集的权重。</p>
<h3 id="3-基于单层决策树构建若分类器">3.基于单层决策树构建若分类器</h3><p>单层决策树是一种简单的决策树。首先构建一个简单的数据集,建立一个adaboost.py文件并加入下列代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadSimpData</span><span class="params">()</span>:</span></span><br><span class="line">    datMat = matrix([[ <span class="number">1.</span> ,  <span class="number">2.1</span>],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.1</span>],</span><br><span class="line">        [ <span class="number">1.3</span>,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">1.</span> ,  <span class="number">1.</span> ],</span><br><span class="line">        [ <span class="number">2.</span> ,  <span class="number">1.</span> ]])</span><br><span class="line">    classLabels = [<span class="number">1.0</span>, <span class="number">1.0</span>, -<span class="number">1.0</span>, -<span class="number">1.0</span>, <span class="number">1.0</span>]</span><br><span class="line">    <span class="keyword">return</span> datMat,classLabels</span><br></pre></td></tr></table></figure>
<p>导入数据</p>
<figure class="highlight ruby"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="prompt">&gt;&gt;</span>&gt; import adaboost</span><br><span class="line"><span class="prompt">&gt;&gt;</span>&gt; datMat,classLabels=adaboost.loadSimpData()</span><br></pre></td></tr></table></figure>
<p>下面两个函数，一个用于测试是否某个值小于或者大于我们正在测试的阈值，一个会在一个加权数据集中循环，并找到具有最低错误率的单层决策树。</p>
<p>伪代码如下：<br></p>
<pre><code>将最小错误率<span class="keyword">min</span>Error设为无穷大
对数据及中的每一个特征（第一层循环）：
    对每个步长（第二层循环）：
        对每个不等号（第三层循环）：
            建立一颗单层决策树并利用加权数据集对它进行测试
            如果错误率低于<span class="keyword">min</span>Error，则将当前单层决策树设置为最佳单层决策树
返回最佳单层决策树
</code></pre><p>单层决策树生成函数代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">stumpClassify</span><span class="params">(dataMatrix,dimen,threshVal,threshIneq)</span>:</span><span class="comment">#just classify the data</span></span><br><span class="line">    retArray = ones((shape(dataMatrix)[<span class="number">0</span>],<span class="number">1</span>))</span><br><span class="line">    <span class="keyword">if</span> threshIneq == <span class="string">'lt'</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &lt;= threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        retArray[dataMatrix[:,dimen] &gt; threshVal] = -<span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> retArray</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">buildStump</span><span class="params">(dataArr,classLabels,D)</span>:</span></span><br><span class="line">    dataMatrix = mat(dataArr); labelMat = mat(classLabels).T</span><br><span class="line">    m,n = shape(dataMatrix)</span><br><span class="line">    numSteps = <span class="number">10.0</span>; bestStump = &#123;&#125;; bestClasEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    minError = inf <span class="comment">#init error sum, to +infinity</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(n):<span class="comment">#loop over all dimensions</span></span><br><span class="line">        rangeMin = dataMatrix[:,i].min(); rangeMax = dataMatrix[:,i].max();</span><br><span class="line">        stepSize = (rangeMax-rangeMin)/numSteps</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(-<span class="number">1</span>,int(numSteps)+<span class="number">1</span>):<span class="comment">#loop over all range in current dimension</span></span><br><span class="line">            <span class="keyword">for</span> inequal <span class="keyword">in</span> [<span class="string">'lt'</span>, <span class="string">'gt'</span>]: <span class="comment">#go over less than and greater than</span></span><br><span class="line">                threshVal = (rangeMin + float(j) * stepSize)</span><br><span class="line">                predictedVals = stumpClassify(dataMatrix,i,threshVal,inequal)<span class="comment">#call stump classify with i, j, lessThan</span></span><br><span class="line">                errArr = mat(ones((m,<span class="number">1</span>)))</span><br><span class="line">                errArr[predictedVals == labelMat] = <span class="number">0</span></span><br><span class="line">                weightedError = D.T*errArr  <span class="comment">#calc total error multiplied by D</span></span><br><span class="line">                <span class="comment">#print "split: dim %d, thresh %.2f, thresh ineqal: %s, the weighted error is %.3f" % (i, threshVal, inequal, weightedError)</span></span><br><span class="line">                <span class="keyword">if</span> weightedError &lt; minError:</span><br><span class="line">                    minError = weightedError</span><br><span class="line">                    bestClasEst = predictedVals.copy()</span><br><span class="line">                    bestStump[<span class="string">'dim'</span>] = i</span><br><span class="line">                    bestStump[<span class="string">'thresh'</span>] = threshVal</span><br><span class="line">                    bestStump[<span class="string">'ineq'</span>] = inequal</span><br><span class="line">    <span class="keyword">return</span> bestStump,minError,bestClasEst</span><br></pre></td></tr></table></figure>
<h3 id="4-AdaBoost算法的实现">4.AdaBoost算法的实现</h3><p>整个实现的伪代码如下：</p>
<pre><code>对每次迭代：
    利用<span class="function"><span class="title">buildStump</span><span class="params">()</span></span>函数找到最佳的单层决策树
    将最佳单层决策树加入到单层决策树数据中
    计算alpha
    计算心的权重向量D
    更新累计类别估计值
    如果错误率低于<span class="number">0.0</span> 则退出循环
</code></pre><p>基于单层决策树的AdaBoost训练过程</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaBoostTrainDS</span><span class="params">(dataArr,classLabels,numIt=<span class="number">40</span>)</span>:</span></span><br><span class="line">    weakClassArr = []</span><br><span class="line">    m = shape(dataArr)[<span class="number">0</span>]</span><br><span class="line">    D = mat(ones((m,<span class="number">1</span>))/m)   <span class="comment">#init D to all equal</span></span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numIt):</span><br><span class="line">        bestStump,error,classEst = buildStump(dataArr,classLabels,D)<span class="comment">#build Stump</span></span><br><span class="line">        <span class="comment">#print "D:",D.T</span></span><br><span class="line">        alpha = float(<span class="number">0.5</span>*log((<span class="number">1.0</span>-error)/max(error,<span class="number">1e-16</span>)))<span class="comment">#calc alpha, throw in max(error,eps) to account for error=0</span></span><br><span class="line">        bestStump[<span class="string">'alpha'</span>] = alpha  </span><br><span class="line">        weakClassArr.append(bestStump)                  <span class="comment">#store Stump Params in Array</span></span><br><span class="line">        <span class="comment">#print "classEst: ",classEst.T</span></span><br><span class="line">        expon = multiply(-<span class="number">1</span>*alpha*mat(classLabels).T,classEst) <span class="comment">#exponent for D calc, getting messy</span></span><br><span class="line">        D = multiply(D,exp(expon))                              <span class="comment">#Calc New D for next iteration</span></span><br><span class="line">        D = D/D.sum()</span><br><span class="line">        <span class="comment">#calc training error of all classifiers, if this is 0 quit for loop early (use break)</span></span><br><span class="line">        aggClassEst += alpha*classEst</span><br><span class="line">        <span class="comment">#print "aggClassEst: ",aggClassEst.T</span></span><br><span class="line">        aggErrors = multiply(sign(aggClassEst) != mat(classLabels).T,ones((m,<span class="number">1</span>)))</span><br><span class="line">        errorRate = aggErrors.sum()/m</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"total error: "</span>,errorRate</span><br><span class="line">        <span class="keyword">if</span> errorRate == <span class="number">0.0</span>: <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> weakClassArr,aggClassEst</span><br></pre></td></tr></table></figure>
<h3 id="5-测试算法">5.测试算法</h3><p>拥有了多个若分类器以及其对应的alpha值，进行测试就方便了。</p>
<p>AdaBoost分类函数:利用训练处的多个若分类器进行分类的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adaClassify</span><span class="params">(datToClass,classifierArr)</span>:</span></span><br><span class="line">    dataMatrix = mat(datToClass)<span class="comment">#do stuff similar to last aggClassEst in adaBoostTrainDS</span></span><br><span class="line">    m = shape(dataMatrix)[<span class="number">0</span>]</span><br><span class="line">    aggClassEst = mat(zeros((m,<span class="number">1</span>)))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(classifierArr)):</span><br><span class="line">        classEst = stumpClassify(dataMatrix,classifierArr[i][<span class="string">'dim'</span>],\</span><br><span class="line">                                 classifierArr[i][<span class="string">'thresh'</span>],\</span><br><span class="line">                                 classifierArr[i][<span class="string">'ineq'</span>])<span class="comment">#call stump classify</span></span><br><span class="line">        aggClassEst += classifierArr[i][<span class="string">'alpha'</span>]*classEst</span><br><span class="line">        <span class="keyword">print</span> aggClassEst</span><br><span class="line">    <span class="keyword">return</span> sign(aggClassEst)</span><br></pre></td></tr></table></figure>
<h3 id="6-绘制ROC曲线">6.绘制ROC曲线</h3><p>ROC曲线绘制代码：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plotROC</span><span class="params">(predStrengths, classLabels)</span>:</span></span><br><span class="line">    <span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">    cur = (<span class="number">1.0</span>,<span class="number">1.0</span>) <span class="comment">#cursor</span></span><br><span class="line">    ySum = <span class="number">0.0</span> <span class="comment">#variable to calculate AUC</span></span><br><span class="line">    numPosClas = sum(array(classLabels)==<span class="number">1.0</span>)</span><br><span class="line">    yStep = <span class="number">1</span>/float(numPosClas); xStep = <span class="number">1</span>/float(len(classLabels)-numPosClas)</span><br><span class="line">    sortedIndicies = predStrengths.argsort()<span class="comment">#get sorted index, it's reverse</span></span><br><span class="line">    fig = plt.figure()</span><br><span class="line">    fig.clf()</span><br><span class="line">    ax = plt.subplot(<span class="number">111</span>)</span><br><span class="line">    <span class="comment">#loop through all the values, drawing a line segment at each point</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> sortedIndicies.tolist()[<span class="number">0</span>]:</span><br><span class="line">        <span class="keyword">if</span> classLabels[index] == <span class="number">1.0</span>:</span><br><span class="line">            delX = <span class="number">0</span>; delY = yStep;</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            delX = xStep; delY = <span class="number">0</span>;</span><br><span class="line">            ySum += cur[<span class="number">1</span>]</span><br><span class="line">        <span class="comment">#draw line from cur to (cur[0]-delX,cur[1]-delY)</span></span><br><span class="line">        ax.plot([cur[<span class="number">0</span>],cur[<span class="number">0</span>]-delX],[cur[<span class="number">1</span>],cur[<span class="number">1</span>]-delY], c=<span class="string">'b'</span>)</span><br><span class="line">        cur = (cur[<span class="number">0</span>]-delX,cur[<span class="number">1</span>]-delY)</span><br><span class="line">    ax.plot([<span class="number">0</span>,<span class="number">1</span>],[<span class="number">0</span>,<span class="number">1</span>],<span class="string">'b--'</span>)</span><br><span class="line">    plt.xlabel(<span class="string">'False positive rate'</span>); plt.ylabel(<span class="string">'True positive rate'</span>)</span><br><span class="line">    plt.title(<span class="string">'ROC curve for AdaBoost horse colic detection system'</span>)</span><br><span class="line">    ax.axis([<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>])</span><br><span class="line">    plt.show()</span><br><span class="line">    <span class="keyword">print</span> <span class="string">"the Area Under the Curve is: "</span>,ySum*xStep</span><br></pre></td></tr></table></figure>
<h3 id="References">References</h3><p>【1】Machine Learning in Action 机器学习实战 第七章</p>
<hr>
<p><br></p>
<p><center><strong>本栏目Machine Learning持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>本章内容</strong></p>
<ul>
<li>组合相似的分类器来提高分类性能</li>
<li>应用AdaBoost算法</li>
<li>处理非均衡分类问题</li>
</ul>
<p><strong>主题：</strong>利用AdaBoost元算法提高分类性能</p>]]>
    
    </summary>
    
      <category term="Machine-Learning Python" scheme="http://csuldw.github.io/tags/Machine-Learning-Python/"/>
    
      <category term="Machine-Learning" scheme="http://csuldw.github.io/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-Apriori关联分析]]></title>
    <link href="http://csuldw.github.io/2015/06/04/2015-06-04-Apriori/"/>
    <id>http://csuldw.github.io/2015/06/04/2015-06-04-Apriori/</id>
    <published>2015-06-04T13:53:12.000Z</published>
    <updated>2015-07-23T02:29:24.497Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong> 学习一个算法，我们最关心的并不是算法本身，而是一个算法能够干什么，能应用到什么地方。很多的时候，我们都需要从大量数据中提取出有用的信息，从大规模数据中寻找物品间的隐含关系叫做关联分析(association analysis)或者关联规则学习(association rule learning)。比如在平时的购物中，那些商品一起捆绑购买销量会比较好，又比如购物商城中的那些推荐信息，都是根据用户平时的搜索或者是购买情况来生成的。如果是蛮力搜索的话代价太高了，所以Apriori就出现了，就是为了解决这类问题的。</p>
<a id="more"></a>
<p><strong>内容纲要</strong></p>
<ul>
<li>关联分析</li>
<li>Apriori算法理论</li>
<li><p>Apriori实现</p>
<ul>
<li>频繁项集生成</li>
<li>关联规则生成</li>
</ul>
</li>
<li><p>reference</p>
</li>
</ul>
<p><strong>Apriori算法</strong></p>
<ul>
<li>优点：易编码实现</li>
<li>缺点：在大数据集上可能较慢</li>
<li>适合数据类型：数值型或者标称型数据</li>
</ul>
<h3 id="1_关联分析"><strong>1 关联分析</strong></h3><p>说到关联分析，顾名思义的就可以联想到，所谓关联就是两个东西之间存在的某种联系。关联分析最有名的例子是“尿布和啤酒”，以前在美国西部的一家连锁店，店家发现男人们在周四购买尿布后还会购买啤酒。于是他便得出一个推理，尿布和啤酒存在某种关联。但是具体怎么来评判呢？</p>
<p>那么，这里用的是支持度和可信度来评判!</p>
<p>一个项集的支持度（support）被定义为数据集中包含该数据集的记录所占的比例。可信度或置信度（confidence）是正对一条关联规则来定义的，比如{尿布}-&gt;{啤酒}，这条规则的可信度定义为“支持度{尿布，啤酒}/支持度{尿布}”</p>
<p>支持度和可信度使用来量化关联分析是否成功的方法。下面使用apriori算法来实现这一理论。</p>
<h3 id="2_Apriori理论"><strong>2 Apriori理论</strong></h3><p><strong>算法的一般过程：</strong></p>
<ul>
<li>收集数据：使用任何方法</li>
<li>准备数据：任意数据类型都可以，因为我们只保存集合</li>
<li>分析数据：使用任何方法</li>
<li>训练算法：使用Apriori算法来找到频繁项集</li>
<li>测试算法：不需要测试过程</li>
<li>使用算法：用于发现频繁项集以及物品之间的关联规则</li>
</ul>
<p>使用Apriori算法，首先计算出单个元素的支持度，然后选出单个元素置信度大于我们要求的数值，比如0.5或是0.7等。然后增加单个元素组合的个数，只要组合项的支持度大于我们要求的数值就把它加到我们的频繁项集中，依次递归。</p>
<p>然后根据计算的支持度选出来的频繁项集来生成关联规则。</p>
<h3 id="3_Apriori实现"><strong>3 Apriori实现</strong></h3><p>首先定义一些算法的辅助函数<br>加载数据集的</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    list = [[<span class="number">1</span>, <span class="number">3</span>, <span class="number">4</span>], [<span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">5</span>], [<span class="number">2</span>, <span class="number">5</span>]]</span><br><span class="line">    <span class="keyword">return</span> list</span><br></pre></td></tr></table></figure>
<p>根据数据集构建集合C1，该集合是大小为1的所有候选集的集合。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createC1</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    C1 = [] <span class="comment">#C1是大小为1的所有候选项集的集合</span></span><br><span class="line">    <span class="keyword">for</span> transaction <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">for</span> item <span class="keyword">in</span> transaction:</span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> [item] <span class="keyword">in</span> C1:</span><br><span class="line">                C1.append([item])             </span><br><span class="line">    C1.sort()</span><br><span class="line">    <span class="keyword">return</span> map(frozenset, C1)<span class="comment">#use frozen set so we can use it as a key in a dict</span></span><br></pre></td></tr></table></figure>
<p>根据构建出来的频繁项集，选出满足我们需要的大于我们给定的支持度的项集</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#D表示数据集，CK表示候选项集，minSupport表示最小的支持度，自己设定</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span>:</span></span><br><span class="line">    ssCnt = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> tid <span class="keyword">in</span> D:</span><br><span class="line">        <span class="keyword">for</span> can <span class="keyword">in</span> Ck:</span><br><span class="line">            <span class="keyword">if</span> can.issubset(tid):</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> ssCnt.has_key(can): ssCnt[can]=<span class="number">1</span></span><br><span class="line">                <span class="keyword">else</span>: ssCnt[can] += <span class="number">1</span></span><br><span class="line">    numItems = float(len(D))</span><br><span class="line">    retList = [] <span class="comment">#存储满足最小支持度要求的项集</span></span><br><span class="line">    supportData = &#123;&#125; <span class="comment">#每个项集的支持度字典</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> ssCnt:  <span class="comment">#计算所有项集的支持度</span></span><br><span class="line">        support = ssCnt[key]/numItems</span><br><span class="line">        <span class="keyword">if</span> support &gt;= minSupport:</span><br><span class="line">            retList.insert(<span class="number">0</span>,key)</span><br><span class="line">        supportData[key] = support</span><br><span class="line">    <span class="keyword">return</span> retList, supportData</span><br></pre></td></tr></table></figure>
<h4 id="3-1_频繁项集"><strong>3.1 频繁项集</strong></h4><p>关于频繁项集的产生，我们单独的抽取出来<br>首先需要一个生成合并项集的函数，将两个子集合并的函数</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#LK是频繁项集列表，K表示接下来合并的项集中的单个想的个数&#123;1,2,3&#125;表示k=3</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">aprioriGen</span><span class="params">(Lk, k)</span>:</span> <span class="comment">#creates Ck</span></span><br><span class="line">    retList = []</span><br><span class="line">    lenLk = len(Lk)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(lenLk):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(i+<span class="number">1</span>, lenLk): </span><br><span class="line">            L1 = list(Lk[i])[:k-<span class="number">2</span>]; L2 = list(Lk[j])[:k-<span class="number">2</span>] <span class="comment">#前k-2个项相同时，将两个集合合并</span></span><br><span class="line">            L1.sort(); L2.sort()</span><br><span class="line">            <span class="keyword">if</span> L1==L2: <span class="comment">#if first k-2 elements are equal</span></span><br><span class="line">                retList.append(Lk[i] | Lk[j]) <span class="comment">#set union</span></span><br><span class="line">    <span class="keyword">return</span> retList</span><br></pre></td></tr></table></figure>
<p>接着定义生成频繁项集的函数</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#只需要输入数据集和支持度即可</span><br><span class="line">def <span class="function"><span class="title">apriori</span><span class="params">(dataSet, minSupport = <span class="number">0.5</span>)</span></span>:</span><br><span class="line">    C1 = <span class="function"><span class="title">createC1</span><span class="params">(dataSet)</span></span></span><br><span class="line">    D = <span class="function"><span class="title">map</span><span class="params">(set, dataSet)</span></span></span><br><span class="line">    L1, supportData = <span class="function"><span class="title">scanD</span><span class="params">(D, C1, minSupport)</span></span></span><br><span class="line">    L = [L1]</span><br><span class="line">    k = <span class="number">2</span></span><br><span class="line">    while (<span class="function"><span class="title">len</span><span class="params">(L[k-<span class="number">2</span>])</span></span> &gt; <span class="number">0</span>):</span><br><span class="line">        Ck = <span class="function"><span class="title">aprioriGen</span><span class="params">(L[k-<span class="number">2</span>], k)</span></span></span><br><span class="line">        Lk, supK = <span class="function"><span class="title">scanD</span><span class="params">(D, Ck, minSupport)</span></span><span class="id">#scan</span> DB to get Lk</span><br><span class="line">        supportData.<span class="function"><span class="title">update</span><span class="params">(supK)</span></span></span><br><span class="line">        L.<span class="function"><span class="title">append</span><span class="params">(Lk)</span></span></span><br><span class="line">        k += <span class="number">1</span></span><br><span class="line">    return L, supportData#返回频繁项集和每个项集的支持度值</span><br></pre></td></tr></table></figure>
<h4 id="3-2_关联规则生成"><strong>3.2 关联规则生成</strong></h4><p>通过频繁项集，我们可以得到相应的规则，但是具体规则怎么得出来的呢？下面给出一个规则生成函数，具体原理参考注释</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#输入的参数分别为：频繁项集、支持度数据字典、自定义的最小支持度，返回的是可信度规则列表</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">generateRules</span><span class="params">(L, supportData, minConf=<span class="number">0.7</span>)</span>:</span>  <span class="comment">#支持度是通过scanD得到的字典</span></span><br><span class="line">    bigRuleList = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, len(L)):<span class="comment">#只去频繁项集中元素个数大于2的子集，如&#123;1,2&#125;&#123;1,2,3&#125;，不取&#123;2&#125;&#123;3&#125;,etc...</span></span><br><span class="line">        <span class="keyword">for</span> freqSet <span class="keyword">in</span> L[i]:</span><br><span class="line">            H1 = [frozenset([item]) <span class="keyword">for</span> item <span class="keyword">in</span> freqSet]</span><br><span class="line">            <span class="keyword">if</span> (i &gt; <span class="number">1</span>):</span><br><span class="line">                rulesFromConseq(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                calcConf(freqSet, H1, supportData, bigRuleList, minConf)</span><br><span class="line">    <span class="keyword">return</span> bigRuleList</span><br></pre></td></tr></table></figure>
<p>下面定义一个用来计算置信度的函数，通过该函数抽取出符合我们要求的规则，如freqSet为{1,2}，H为{1}，{2}，可以计算出{1}—&gt;{2}和{2}—&gt;{1}的质心度，即下面的conf变量，然后用if语句判断是否符合我们的要求。代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#计算可信度，找到满足最小可信度的要求规则</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">calcConf</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span>:</span></span><br><span class="line">    prunedH = [] <span class="comment">#create new list to return</span></span><br><span class="line">    <span class="keyword">for</span> conseq <span class="keyword">in</span> H:</span><br><span class="line">        conf = supportData[freqSet]/supportData[freqSet-conseq] <span class="comment">#calc confidence</span></span><br><span class="line">        <span class="keyword">if</span> conf &gt;= minConf: </span><br><span class="line">            <span class="keyword">print</span> freqSet-conseq,<span class="string">'--&gt;'</span>,conseq,<span class="string">'conf:'</span>,conf</span><br><span class="line">            brl.append((freqSet-conseq, conseq, conf))</span><br><span class="line">            prunedH.append(conseq)</span><br><span class="line">    <span class="keyword">return</span> prunedH</span><br></pre></td></tr></table></figure>
<p>下面的函数是用来合并子集的，比如我现在的频繁项集是{2,3,5},它的构造元素是{2},{3},{5}，所以需要将{2},{3},{5}两两合并然后再根据上面的calcConf函数计算置信度。代码如下：</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#从最初的项集中生成更多的规则</span><br><span class="line">def <span class="function"><span class="title">rulesFromConseq</span><span class="params">(freqSet, H, supportData, brl, minConf=<span class="number">0.7</span>)</span></span>:</span><br><span class="line">    m = <span class="function"><span class="title">len</span><span class="params">(H[<span class="number">0</span>])</span></span></span><br><span class="line">    <span class="keyword">if</span> (<span class="function"><span class="title">len</span><span class="params">(freqSet)</span></span> &gt; (m + <span class="number">1</span>)): #进一步合并项集</span><br><span class="line">        Hmp1 = <span class="function"><span class="title">aprioriGen</span><span class="params">(H, m+<span class="number">1</span>)</span></span><span class="id">#create</span> Hm+<span class="number">1</span> new candidates</span><br><span class="line">        Hmp1 = <span class="function"><span class="title">calcConf</span><span class="params">(freqSet, Hmp1, supportData, brl, minConf)</span></span></span><br><span class="line">        <span class="keyword">if</span> (<span class="function"><span class="title">len</span><span class="params">(Hmp1)</span></span> &gt; <span class="number">1</span>):    <span class="id">#need</span> at least two sets to merge</span><br><span class="line">            <span class="function"><span class="title">rulesFromConseq</span><span class="params">(freqSet, Hmp1, supportData, brl, minConf)</span></span></span><br></pre></td></tr></table></figure>
<h4 id="3-3_测试"><strong>3.3 测试</strong></h4><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">dataSet = <span class="function"><span class="title">loadDataSet</span><span class="params">()</span></span>			 	#加载数据集</span><br><span class="line">L,suppoData = <span class="function"><span class="title">apriori</span><span class="params">(dataSet)</span></span>		#计算频繁项集</span><br><span class="line">rules = <span class="function"><span class="title">generateRules</span><span class="params">(L,suppoData,minConf=<span class="number">0.7</span>)</span></span> #抽取规则</span><br></pre></td></tr></table></figure>
<p>得到的结果为：<br><img src="http://img.blog.csdn.net/20150604094036589" alt="这里写图片描述"></p>
<p>L表示的是符合条件的频繁项集，rules表示最后抽取出来的符合条件的规则；还可以查看各个项集的支持度，如下所示。<br><img src="http://img.blog.csdn.net/20150604094213762" alt="这里写图片描述"></p>
<h3 id="Reference"><strong>Reference</strong></h3><p>[1]<strong>《机器学习实战》</strong>书籍第11章</p>
<p><br></p>
<hr>
<center><strong>本栏目Machine Learning 算法实现持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a></strong></center>

<p><br><br><br></p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong> 学习一个算法，我们最关心的并不是算法本身，而是一个算法能够干什么，能应用到什么地方。很多的时候，我们都需要从大量数据中提取出有用的信息，从大规模数据中寻找物品间的隐含关系叫做关联分析(association analysis)或者关联规则学习(association rule learning)。比如在平时的购物中，那些商品一起捆绑购买销量会比较好，又比如购物商城中的那些推荐信息，都是根据用户平时的搜索或者是购买情况来生成的。如果是蛮力搜索的话代价太高了，所以Apriori就出现了，就是为了解决这类问题的。</p>]]>
    
    </summary>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/tags/Machine-Learning-python/"/>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/categories/Machine-Learning-python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法--K-means聚类]]></title>
    <link href="http://csuldw.github.io/2015/06/03/2015-06-03-K-means/"/>
    <id>http://csuldw.github.io/2015/06/03/2015-06-03-K-means/</id>
    <published>2015-06-03T04:30:00.000Z</published>
    <updated>2015-07-23T02:29:24.536Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong> k均值算法是一种聚类算法，所谓聚类，他是一种无监督学习，将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果和分类相同，而只是类别没有预先定义。</p>
<a id="more"></a>
<p><strong>算法的目的：</strong> 使各个样本与所在类均值的<strong>误差平方和达到最小</strong>（这也是评价K-means算法最后聚类效果的评价标准）</p>
<center><strong>K-均值聚类</strong></center>

<ul>
<li>优点：容易实现</li>
<li>缺点：可能收敛到局部最小值，在大规模数据上收敛较慢</li>
<li>适合数据类型：数值型数据</li>
</ul>
<h3 id="伪代码"><strong>伪代码</strong></h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#创建k个点作为起始质心（经常随机选择）</span></span><br><span class="line"><span class="comment">#当任意一个点的蔟分配结果发生变化时</span></span><br><span class="line">	<span class="comment">#对数据集中的每个数据点</span></span><br><span class="line">		<span class="comment">#对每个质心</span></span><br><span class="line">			<span class="comment">#计算质心到数据点之间的距离</span></span><br><span class="line">		<span class="comment">#将数据点分配到距其最近的蔟</span></span><br><span class="line">	<span class="comment">#对每个蔟，计算蔟中所有点的均值并将均值作为质心</span></span><br></pre></td></tr></table></figure>
<h3 id="代码实现"><strong>代码实现</strong></h3><p>因为我们用到的是数值类型的数据，这里编写一个加载数据集的函数，返回值是一个矩阵形式。<br>下面代码应写在一个py文件里，我这里写在kMeans.py文件中。</p>
<p>文件的头部引入numpy<br><figure class="highlight capnproto"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure></p>
<p><strong>数据集加载代码</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 加载数据集文件，没有返回类标号的函数</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">(fileName)</span>:</span></span><br><span class="line">    dataMat = []</span><br><span class="line">    openfile = open(fileName)    </span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> openfile.readlines():</span><br><span class="line">        curLine = line.strip().split(<span class="string">'\t'</span>)</span><br><span class="line">        floatLine = map(float,curLine)</span><br><span class="line">        dataMat.append(floatLine)</span><br><span class="line">    <span class="keyword">return</span> dataMat</span><br></pre></td></tr></table></figure>
<p>因为在k均值算法中要计算点到质心的距离，所以这里将距离计算写成一个函数，计算欧几里得距离公式：</p>
<p>$$d=\sqrt{(x_2-x_1)^2+…+(z_2-z_1)^2}$$</p>
<p><strong>函数代码如下：</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 计算两个向量的欧氏距离</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">distEclud</span><span class="params">(vecA,vecB)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sqrt(sum(power(vecA-vecB,<span class="number">2</span>)))</span><br></pre></td></tr></table></figure>
<p><strong>接下来初始化k个蔟的质心函数centroid</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 传入的数据时numpy的矩阵格式</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">randCent</span><span class="params">(dataMat, k)</span>:</span></span><br><span class="line">    n = shape(dataMat)[<span class="number">1</span>]</span><br><span class="line">    centroids = mat(zeros((k,n)))  </span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(n):</span><br><span class="line">        minJ = min(dataMat[:,j]) <span class="comment"># 找出矩阵dataMat第j列最小值</span></span><br><span class="line">        rangeJ = float(max(dataMat[:,j]) - minJ) <span class="comment">#计算第j列最大值和最小值的差</span></span><br><span class="line">        <span class="comment">#赋予一个随机质心，它的值在整个数据集的边界之内</span></span><br><span class="line">        centroids[:,j] = minJ + rangeJ * random.rand(k,<span class="number">1</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids <span class="comment">#返回一个随机的质心矩阵</span></span><br></pre></td></tr></table></figure>
<p><strong>K-means算法</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#k-均值算法</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kMeans</span><span class="params">(dataMat,k,distE = distEclud , createCent=randCent)</span>:</span></span><br><span class="line">    m = shape(dataMat)[<span class="number">0</span>]    <span class="comment"># 获得行数m</span></span><br><span class="line">    clusterAssment = mat(zeros((m,<span class="number">2</span>))) <span class="comment"># 初试化一个矩阵，用来记录簇索引和存储误差                               </span></span><br><span class="line">    centroids = createCent(dataMat,k) <span class="comment"># 随机的得到一个质心矩阵蔟</span></span><br><span class="line">    clusterChanged = <span class="keyword">True</span></span><br><span class="line">    <span class="keyword">while</span> clusterChanged:</span><br><span class="line">        clusterChanged = <span class="keyword">False</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(m):    <span class="comment">#对每个数据点寻找最近的质心</span></span><br><span class="line">            minDist = inf; minIndex = -<span class="number">1</span></span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(k): <span class="comment"># 遍历质心蔟，寻找最近的质心    </span></span><br><span class="line">                distJ1 = distE(centroids[j,:],dataMat[i,:]) <span class="comment">#计算数据点和质心的欧式距离</span></span><br><span class="line">                <span class="keyword">if</span> distJ1 &lt; minDist: </span><br><span class="line">                    minDist = distJ1; minIndex = j</span><br><span class="line">            <span class="keyword">if</span> clusterAssment[i,<span class="number">0</span>] != minIndex:</span><br><span class="line">                clusterChanged = <span class="keyword">True</span></span><br><span class="line">            clusterAssment[i,:] = minIndex,minDist**<span class="number">2</span></span><br><span class="line">        <span class="keyword">print</span> centroids</span><br><span class="line">        <span class="keyword">for</span> cent <span class="keyword">in</span> range(k):    <span class="comment">#更新质心的位置</span></span><br><span class="line">            ptsInClust = dataMat[nonzero(clusterAssment[:,<span class="number">0</span>].A==cent)[<span class="number">0</span>]]    </span><br><span class="line">            centroids[cent,:] = mean(ptsInClust, axis=<span class="number">0</span>) </span><br><span class="line">    <span class="keyword">return</span> centroids, clusterAssment</span><br></pre></td></tr></table></figure>
<p><strong>测试：</strong></p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataMat = <span class="function"><span class="title">mat</span><span class="params">(loadDataSet(<span class="string">'testSet.txt'</span>)</span></span>)</span><br><span class="line"><span class="function"><span class="title">kMeans</span><span class="params">(dataMat,<span class="number">4</span>)</span></span></span><br></pre></td></tr></table></figure>
<p><strong>输出结果：</strong></p>
<p>===================</p>
<p>[[-3.66087851  2.30869657]<br> [ 3.24377288  3.04700412]<br> [ 2.52577861 -3.12485493]<br> [-2.79672694  3.19201596]]<br>[[-3.78710372 -1.66790611]<br> [ 2.6265299   3.10868015]<br> [ 1.62908469 -2.92689085]<br> [-2.18799937  3.01824781]]<br>[[-3.53973889 -2.89384326]<br> [ 2.6265299   3.10868015]<br> [ 2.65077367 -2.79019029]<br> [-2.46154315  2.78737555]]</p>
<p>===================</p>
<p>上面的结果给出了四个质心。可以看出，经过3次迭代之后K-均值算法收敛。质心会保存在第一个返回值中，第二个是每个点的簇分布情况。</p>
<p><strong>附件：</strong></p>
<p>上面测试的数据集为：</p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">1<span class="class">.658985</span>	4<span class="class">.285136</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.453687</span>	3<span class="class">.424321</span></span><br><span class="line">4<span class="class">.838138</span>	<span class="tag">-1</span><span class="class">.151539</span></span><br><span class="line"><span class="tag">-5</span><span class="class">.379713</span>	<span class="tag">-3</span><span class="class">.362104</span></span><br><span class="line">0<span class="class">.972564</span>	2<span class="class">.924086</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.567919</span>	1<span class="class">.531611</span></span><br><span class="line">0<span class="class">.450614</span>	<span class="tag">-3</span><span class="class">.302219</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.487105</span>	<span class="tag">-1</span><span class="class">.724432</span></span><br><span class="line">2<span class="class">.668759</span>	1<span class="class">.594842</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.156485</span>	3<span class="class">.191137</span></span><br><span class="line">3<span class="class">.165506</span>	<span class="tag">-3</span><span class="class">.999838</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.786837</span>	<span class="tag">-3</span><span class="class">.099354</span></span><br><span class="line">4<span class="class">.208187</span>	2<span class="class">.984927</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.123337</span>	2<span class="class">.943366</span></span><br><span class="line">0<span class="class">.704199</span>	<span class="tag">-0</span><span class="class">.479481</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.392370</span>	<span class="tag">-3</span><span class="class">.963704</span></span><br><span class="line">2<span class="class">.831667</span>	1<span class="class">.574018</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.790153</span>	3<span class="class">.343144</span></span><br><span class="line">2<span class="class">.943496</span>	<span class="tag">-3</span><span class="class">.357075</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.195883</span>	<span class="tag">-2</span><span class="class">.283926</span></span><br><span class="line">2<span class="class">.336445</span>	2<span class="class">.875106</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.786345</span>	2<span class="class">.554248</span></span><br><span class="line">2<span class="class">.190101</span>	<span class="tag">-1</span><span class="class">.906020</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.403367</span>	<span class="tag">-2</span><span class="class">.778288</span></span><br><span class="line">1<span class="class">.778124</span>	3<span class="class">.880832</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.688346</span>	2<span class="class">.230267</span></span><br><span class="line">2<span class="class">.592976</span>	<span class="tag">-2</span><span class="class">.054368</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.007257</span>	<span class="tag">-3</span><span class="class">.207066</span></span><br><span class="line">2<span class="class">.257734</span>	3<span class="class">.387564</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.679011</span>	0<span class="class">.785119</span></span><br><span class="line">0<span class="class">.939512</span>	<span class="tag">-4</span><span class="class">.023563</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.674424</span>	<span class="tag">-2</span><span class="class">.261084</span></span><br><span class="line">2<span class="class">.046259</span>	2<span class="class">.735279</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.189470</span>	1<span class="class">.780269</span></span><br><span class="line">4<span class="class">.372646</span>	<span class="tag">-0</span><span class="class">.822248</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.579316</span>	<span class="tag">-3</span><span class="class">.497576</span></span><br><span class="line">1<span class="class">.889034</span>	5<span class="class">.190400</span></span><br><span class="line"><span class="tag">-0</span><span class="class">.798747</span>	2<span class="class">.185588</span></span><br><span class="line">2<span class="class">.836520</span>	<span class="tag">-2</span><span class="class">.658556</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.837877</span>	<span class="tag">-3</span><span class="class">.253815</span></span><br><span class="line">2<span class="class">.096701</span>	3<span class="class">.886007</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.709034</span>	2<span class="class">.923887</span></span><br><span class="line">3<span class="class">.367037</span>	<span class="tag">-3</span><span class="class">.184789</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.121479</span>	<span class="tag">-4</span><span class="class">.232586</span></span><br><span class="line">2<span class="class">.329546</span>	3<span class="class">.179764</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.284816</span>	3<span class="class">.273099</span></span><br><span class="line">3<span class="class">.091414</span>	<span class="tag">-3</span><span class="class">.815232</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.762093</span>	<span class="tag">-2</span><span class="class">.432191</span></span><br><span class="line">3<span class="class">.542056</span>	2<span class="class">.778832</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.736822</span>	4<span class="class">.241041</span></span><br><span class="line">2<span class="class">.127073</span>	<span class="tag">-2</span><span class="class">.983680</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.323818</span>	<span class="tag">-3</span><span class="class">.938116</span></span><br><span class="line">3<span class="class">.792121</span>	5<span class="class">.135768</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.786473</span>	3<span class="class">.358547</span></span><br><span class="line">2<span class="class">.624081</span>	<span class="tag">-3</span><span class="class">.260715</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.009299</span>	<span class="tag">-2</span><span class="class">.978115</span></span><br><span class="line">2<span class="class">.493525</span>	1<span class="class">.963710</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.513661</span>	2<span class="class">.642162</span></span><br><span class="line">1<span class="class">.864375</span>	<span class="tag">-3</span><span class="class">.176309</span></span><br><span class="line"><span class="tag">-3</span><span class="class">.171184</span>	<span class="tag">-3</span><span class="class">.572452</span></span><br><span class="line">2<span class="class">.894220</span>	2<span class="class">.489128</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.562539</span>	2<span class="class">.884438</span></span><br><span class="line">3<span class="class">.491078</span>	<span class="tag">-3</span><span class="class">.947487</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.565729</span>	<span class="tag">-2</span><span class="class">.012114</span></span><br><span class="line">3<span class="class">.332948</span>	3<span class="class">.983102</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.616805</span>	3<span class="class">.573188</span></span><br><span class="line">2<span class="class">.280615</span>	<span class="tag">-2</span><span class="class">.559444</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.651229</span>	<span class="tag">-3</span><span class="class">.103198</span></span><br><span class="line">2<span class="class">.321395</span>	3<span class="class">.154987</span></span><br><span class="line"><span class="tag">-1</span><span class="class">.685703</span>	2<span class="class">.939697</span></span><br><span class="line">3<span class="class">.031012</span>	<span class="tag">-3</span><span class="class">.620252</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.599622</span>	<span class="tag">-2</span><span class="class">.185829</span></span><br><span class="line">4<span class="class">.196223</span>	1<span class="class">.126677</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.133863</span>	3<span class="class">.093686</span></span><br><span class="line">4<span class="class">.668892</span>	<span class="tag">-2</span><span class="class">.562705</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.793241</span>	<span class="tag">-2</span><span class="class">.149706</span></span><br><span class="line">2<span class="class">.884105</span>	3<span class="class">.043438</span></span><br><span class="line"><span class="tag">-2</span><span class="class">.967647</span>	2<span class="class">.848696</span></span><br><span class="line">4<span class="class">.479332</span>	<span class="tag">-1</span><span class="class">.764772</span></span><br><span class="line"><span class="tag">-4</span><span class="class">.905566</span>	<span class="tag">-2</span><span class="class">.911070</span></span><br></pre></td></tr></table></figure>]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong> k均值算法是一种聚类算法，所谓聚类，他是一种无监督学习，将相似的对象归到同一个蔟中。蔟内的对象越相似，聚类的效果越好。聚类和分类最大的不同在于，分类的目标事先已知，而聚类则不一样。因为其产生的结果和分类相同，而只是类别没有预先定义。</p>]]>
    
    </summary>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/tags/Machine-Learning-python/"/>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/categories/Machine-Learning-python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-朴素贝叶斯Python实现]]></title>
    <link href="http://csuldw.github.io/2015/05/28/2015-05-28-NB/"/>
    <id>http://csuldw.github.io/2015/05/28/2015-05-28-NB/</id>
    <published>2015-05-28T04:59:00.000Z</published>
    <updated>2015-07-23T02:29:24.569Z</updated>
    <content type="html"><![CDATA[<p><strong>引文：</strong>前面提到的K最近邻算法和决策树算法，数据实例最终被明确的划分到某个分类中，下面介绍一种不能完全确定数据实例应该划分到哪个类别，或者说只能给数据实例属于给定分类的概率。</p>
<a id="more"></a>
<h3 id="基于贝叶斯决策理论的分类方法之朴素贝叶斯"><strong>基于贝叶斯决策理论的分类方法之朴素贝叶斯</strong></h3><ul>
<li>优点：在数据较少的情况下仍然有效，可以处理多类别问题</li>
<li>缺点：对于输入数据的准备方式较为敏感 </li>
<li>适用数据类型：标称型数据。</li>
</ul>
<h3 id="朴素贝叶斯的一般过程"><strong>朴素贝叶斯的一般过程</strong></h3><ul>
<li>收集数据：可以使用任何方式</li>
<li>准备数据：需要数据型或是布尔型数据</li>
<li>分类数据：有大量特征时，绘制特征作用不大，此时使用直方图效果更好</li>
<li>训练算法：计算不同的独立特征的条件概率</li>
<li>测试算法：计算错误率</li>
<li>使用算法：文档分类</li>
</ul>
<h3 id="原理"><strong>原理</strong></h3><p>主要是运用<strong>贝叶斯定理</strong></p>
<p>$$ P(H|X) = \dfrac{P(X|H) p(H)}{P(X)} $$</p>
<h3 id="算法实现"><strong>算法实现</strong></h3><p>下面做一个简单的留言板分类，自动判别留言类别：侮辱类和非侮辱类，分别使用1和0表示。下面来做一下这个实验。以下函数全部写在一个叫bayes.py文件中，后面的实验室通过导入bayes.py，调用里面的函数来做的。</p>
<p>导入numpy包</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br></pre></td></tr></table></figure>
<h4 id="1-加载数据集"><strong>1.加载数据集</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">loadDataSet</span><span class="params">()</span>:</span></span><br><span class="line">    postingList=[[<span class="string">'my'</span>, <span class="string">'dog'</span>, <span class="string">'has'</span>, <span class="string">'flea'</span>, <span class="string">'problems'</span>, <span class="string">'help'</span>, <span class="string">'please'</span>],</span><br><span class="line">                 [<span class="string">'maybe'</span>, <span class="string">'not'</span>, <span class="string">'take'</span>, <span class="string">'him'</span>, <span class="string">'to'</span>, <span class="string">'dog'</span>, <span class="string">'park'</span>, <span class="string">'stupid'</span>],</span><br><span class="line">                 [<span class="string">'my'</span>, <span class="string">'dalmation'</span>, <span class="string">'is'</span>, <span class="string">'so'</span>, <span class="string">'cute'</span>, <span class="string">'I'</span>, <span class="string">'love'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'stop'</span>, <span class="string">'posting'</span>, <span class="string">'stupid'</span>, <span class="string">'worthless'</span>, <span class="string">'garbage'</span>],</span><br><span class="line">                 [<span class="string">'mr'</span>, <span class="string">'licks'</span>, <span class="string">'ate'</span>, <span class="string">'my'</span>, <span class="string">'steak'</span>, <span class="string">'how'</span>, <span class="string">'to'</span>, <span class="string">'stop'</span>, <span class="string">'him'</span>],</span><br><span class="line">                 [<span class="string">'quit'</span>, <span class="string">'buying'</span>, <span class="string">'worthless'</span>, <span class="string">'dog'</span>, <span class="string">'food'</span>, <span class="string">'stupid'</span>]]</span><br><span class="line">    classVec = [<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">0</span>,<span class="number">1</span>]    <span class="comment">#1 is abusive, 0 not</span></span><br><span class="line">    <span class="keyword">return</span> postingList,classVec</span><br></pre></td></tr></table></figure>
<p>该函数返回的是<strong>词条切分集合</strong>和<strong>类标签</strong>。</p>
<h4 id="2-根据样本创建一个词库"><strong>2.根据样本创建一个词库</strong></h4><p>下面的函数是根据上面给出来的样本数据所创建出来的一个词库。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">createVocabList</span><span class="params">(dataSet)</span>:</span></span><br><span class="line">    vocabSet = set([])  <span class="comment">#create empty set</span></span><br><span class="line">    <span class="keyword">for</span> document <span class="keyword">in</span> dataSet:</span><br><span class="line">        vocabSet = vocabSet | set(document) <span class="comment">#union of the two sets</span></span><br><span class="line">    <span class="keyword">return</span> list(vocabSet)</span><br></pre></td></tr></table></figure>
<h4 id="3-统计每个样本在词库中的出现情况"><strong>3.统计每个样本在词库中的出现情况</strong></h4><p>下面的函数功能是把单个样本映射到词库中去，统计单个样本在词库中的出现情况，1表示出现过，0表示没有出现，函数如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">setOfWords2Vec</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>: <span class="keyword">print</span> <span class="string">"the word: %s is not in my Vocabulary!"</span> % word</span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<h4 id="4-计算条件概率和类标签概率"><strong>4.计算条件概率和类标签概率</strong></h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">trainNB0</span><span class="params">(trainMatrix,trainCategory)</span>:</span></span><br><span class="line">    numTrainDocs = len(trainMatrix)</span><br><span class="line">    numWords = len(trainMatrix[<span class="number">0</span>])</span><br><span class="line">    pAbusive = sum(trainCategory)/float(numTrainDocs) <span class="comment">#计算某个类发生的概率</span></span><br><span class="line">    p0Num = ones(numWords); p1Num = ones(numWords) <span class="comment">#初始样本个数为1，防止条件概率为0，影响结果       </span></span><br><span class="line">    p0Denom = <span class="number">2.0</span>; p1Denom = <span class="number">2.0</span>  <span class="comment">#作用同上                      </span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(numTrainDocs):</span><br><span class="line">        <span class="keyword">if</span> trainCategory[i] == <span class="number">1</span>:</span><br><span class="line">            p1Num += trainMatrix[i]</span><br><span class="line">            p1Denom += sum(trainMatrix[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            p0Num += trainMatrix[i]</span><br><span class="line">            p0Denom += sum(trainMatrix[i])</span><br><span class="line">    p1Vect = log(p1Num/p1Denom)         <span class="comment">#计算类标签为1时的其它属性发生的条件概率</span></span><br><span class="line">    p0Vect = log(p0Num/p0Denom)         <span class="comment">#计算标签为0时的其它属性发生的条件概率</span></span><br><span class="line">    <span class="keyword">return</span> p0Vect,p1Vect,pAbusive       <span class="comment">#返回条件概率和类标签为1的概率</span></span><br></pre></td></tr></table></figure>
<p>说明：</p>
<h4 id="5-训练贝叶斯分类算法"><strong>5.训练贝叶斯分类算法</strong></h4><p>该算法包含四个输入，vec2Classify表示待分类的样本在词库中的映射集合，p0Vec表示条件概率$P(w_i|c=0)$，p1Vec表示条件概率$P(w_i|c=1)$，pClass1表示类标签为1时的概率$P(c=1)$。</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def <span class="function"><span class="title">classifyNB</span><span class="params">(vec2Classify, p0Vec, p1Vec, pClass1)</span></span>:</span><br><span class="line">    p1 = <span class="function"><span class="title">sum</span><span class="params">(vec2Classify * p1Vec)</span></span> + <span class="function"><span class="title">log</span><span class="params">(pClass1)</span></span>    <span class="id">#element-wise</span> mult</span><br><span class="line">    p0 = <span class="function"><span class="title">sum</span><span class="params">(vec2Classify * p0Vec)</span></span> + <span class="function"><span class="title">log</span><span class="params">(<span class="number">1.0</span> - pClass1)</span></span></span><br><span class="line">    <span class="keyword">if</span> p1 &gt; p0:</span><br><span class="line">        return <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>: </span><br><span class="line">        return <span class="number">0</span></span><br></pre></td></tr></table></figure>
<p>其中p1和p0表示的是</p>
<p>$$lnp(w_1|c=1)p(w_2|c=1)…p(w_n|c=1)*p(c=1)$$</p>
<p>和</p>
<p>$$lnp(w_1|c=0)p(w_2|c=0)…p(w_n|c=0)*p(c=0)$$</p>
<p>取对数是因为防止p(w_1|c=1)p(w_2|c=1)p(w_3|c=1)…p(w_n|c=1)多个小于1的数相乘结果值下溢。</p>
<h4 id="6-文档词袋模型,修改函数setOfWords2Vec"><strong>6.文档词袋模型,修改函数setOfWords2Vec</strong></h4><p>词袋模型主要修改上面的第三个步骤，因为有的词可能出现多次，所以在单个样本映射到词库的时候需要多次统计。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bagOfWords2VecMN</span><span class="params">(vocabList, inputSet)</span>:</span></span><br><span class="line">    returnVec = [<span class="number">0</span>]*len(vocabList)</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> inputSet:</span><br><span class="line">        <span class="keyword">if</span> word <span class="keyword">in</span> vocabList:</span><br><span class="line">            returnVec[vocabList.index(word)] += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> returnVec</span><br></pre></td></tr></table></figure>
<h4 id="7-测试函数"><strong>7.测试函数</strong></h4><p>下面给出一个测试函数，直接调用该测试函数就可以实现简单的分类，测试结果看下个部分。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">testingNB</span><span class="params">()</span>:</span></span><br><span class="line">	<span class="comment">#step1：加载数据集和类标号</span></span><br><span class="line">    listOPosts,listClasses = loadDataSet()</span><br><span class="line">    <span class="comment">#step2：创建词库</span></span><br><span class="line">    myVocabList = createVocabList(listOPosts)</span><br><span class="line">    <span class="comment"># step3：计算每个样本在词库中的出现情况</span></span><br><span class="line">    trainMat=[]</span><br><span class="line">    <span class="keyword">for</span> postinDoc <span class="keyword">in</span> listOPosts:</span><br><span class="line">        trainMat.append(setOfWords2Vec(myVocabList, postinDoc))</span><br><span class="line">    <span class="comment">#step4：调用第四步函数，计算条件概率</span></span><br><span class="line">    p0V,p1V,pAb = trainNB0(array(trainMat),array(listClasses))</span><br><span class="line">    <span class="comment"># step5</span></span><br><span class="line">    <span class="comment"># 测试1 </span></span><br><span class="line">    testEntry = [<span class="string">'love'</span>, <span class="string">'my'</span>, <span class="string">'dalmation'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry,<span class="string">'classified as: '</span>,classifyNB(thisDoc,p0V,p1V,pAb)</span><br><span class="line">    <span class="comment"># 测试2</span></span><br><span class="line">    testEntry = [<span class="string">'stupid'</span>, <span class="string">'garbage'</span>]</span><br><span class="line">    thisDoc = array(setOfWords2Vec(myVocabList, testEntry))</span><br><span class="line">    <span class="keyword">print</span> testEntry,<span class="string">'classified as: '</span>,classifyNB(thisDoc,p0V,p1V,pAb)</span><br></pre></td></tr></table></figure>
<h4 id="8-实验"><strong>8.实验</strong></h4><p>首先导入库，然后导入bayes.py文件</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line">os.chdir(<span class="string">r"E:\3-CSU\Academic\Machine Leaning\机器学习实战\src\machinelearninginaction\Ch04"</span>)</span><br><span class="line"><span class="keyword">import</span> bayes</span><br></pre></td></tr></table></figure>
<p><img src="http://img.blog.csdn.net/20150528122404620" alt="这里写图片描述"></p>
<p>可以看出，贝叶斯算法将[‘love’, ‘my’, ‘dalmation’]分为“无侮辱”一类，将[‘stupid’, ‘garbage’]分为“侮辱”性质的一类。</p>
<h3 id="Reference"><strong>Reference</strong></h3><p><strong>[1]《Machine Learning in Action 》机器学习实战</strong></p>
<hr>
<center><br><br>本栏目Machine Learning 算法实现持续更新中，欢迎关注：<a href="http://blog.csdn.net/dream_angel_z" target="_blank" rel="external">Dream_Angel_Z博客</a><br><br></center>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文：</strong>前面提到的K最近邻算法和决策树算法，数据实例最终被明确的划分到某个分类中，下面介绍一种不能完全确定数据实例应该划分到哪个类别，或者说只能给数据实例属于给定分类的概率。</p>]]>
    
    </summary>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/tags/Machine-Learning-python/"/>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/categories/Machine-Learning-python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-K最近邻从原理到实现]]></title>
    <link href="http://csuldw.github.io/2015/05/21/2015-05-21-KNN/"/>
    <id>http://csuldw.github.io/2015/05/21/2015-05-21-KNN/</id>
    <published>2015-05-21T12:34:00.000Z</published>
    <updated>2015-07-23T02:29:24.603Z</updated>
    <content type="html"><![CDATA[<p><strong>引文</strong>：决策树和基于规则的分类器都是<strong>积极学习方法</strong>（eager learner）的例子，因为一旦训练数据可用，他们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为<strong>消极学习法</strong>（lazy learner）。<strong>最近邻分类器</strong>就是这样的一种方法。</p>
<a id="more"></a>
<h3 id="1-K最近邻分类器原理"><strong>1.K最近邻分类器原理</strong></h3><p>首先给出一张图，根据这张图来理解最近邻分类器，如下：</p>
<center><img src="http://img.blog.csdn.net/20150521201557111" alt="这里写图片描述"><br></center>

<p>根据上图所示，有两类不同的样本数据，分别用<strong>蓝色的小正方形</strong>和<strong>红色的小三角形</strong>表示，而图正中间的那个<strong>绿色的圆</strong>所标示的数据则是待分类的数据。也就是说，现在， 我们不知道中间那个绿色的数据是从属于哪一类（蓝色小正方形or红色小三角形），下面，我们就要解决这个问题：给这个绿色的圆分类。</p>
<p>　　我们常说，物以类聚，人以群分，判别一个人是一个什么样品质特征的人，常常可以从他or她身边的朋友入手，所谓观其友，而识其人。我们不是要判别上图中那个绿色的圆是属于哪一类数据么，好说，从它的邻居下手。但一次性看多少个邻居呢？从上图中，你还能看到：</p>
<ul>
<li>如果K=3，绿色圆点的最近的3个邻居是2个红色小三角形和1个蓝色小正方形，少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于红色的三角形一类。</li>
<li>如果K=5，绿色圆点的最近的5个邻居是2个红色三角形和3个蓝色的正方形，还是少数从属于多数，基于统计的方法，判定绿色的这个待分类点属于蓝色的正方形一类。</li>
</ul>
<p>于此我们看到，当无法判定当前待分类点是从属于已知分类中的哪一类时，我们可以依据统计学的理论看它所处的位置特征，衡量它周围邻居的权重，而把它归为(或分配)到权重更大的那一类。这就是K近邻算法的核心思想。</p>
<p>KNN算法中，所选择的邻居都是已经正确分类的对象。该方法在定类决策上只依据最邻近的一个或者几个样本的类别来决定待分样本所属的类别。</p>
<p>KNN 算法本身简单有效，它是一种 lazy-learning 算法，分类器不需要使用训练集进行训练，训练时间复杂度为0。KNN 分类的计算复杂度和训练集中的文档数目成正比，也就是说，如果训练集中文档总数为 n，那么 KNN 的分类时间复杂度为O(n)。</p>
<p>前面的例子中强调了选择合适的K值的重要性。如果太小，则最近邻分类器容易受到训练数据的噪声而产生的过分拟合的影响；相反，如果K太大，最近分类器可能会误会分类测试样例，因为最近邻列表中可能包含远离其近邻的数据点。（如下图所示）</p>
<center><img src="http://img.blog.csdn.net/20150521203027253" alt="这里写图片描述"><br><br><strong>K较大时的最近邻分类</strong><br><br></center>


<p>可见，K值的选取还是非常关键。</p>
<hr>
<h3 id="2-算法"><strong>2.算法</strong></h3><p>算法步骤如下所示：</p>
<center><img src="http://img.blog.csdn.net/20150521203212596" alt="这里写图片描述"></center>

<p>对每个测试样例$z = (x’,y’)$，算法计算它和所有训练样例$（x,y）属于D$之间的距离（或相似度），以确定其最近邻列表$D_z$。如果训练样例的数目很大，那么这种计算的开销就会很大。不过，可以使索引技术降低为测试样例找最近邻是的计算量。</p>
<p>一旦得到最近邻列表，测试样例就可以根据最近邻的多数类进行分类，使用多数表决方法。</p>
<hr>
<h3 id="3-K最邻近算法实现（Python）"><strong>3.K最邻近算法实现（Python）</strong></h3><p>KNN.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> operator</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNN</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">createDataset</span><span class="params">(self)</span>:</span></span><br><span class="line">        group = array([[<span class="number">1.0</span>,<span class="number">1.1</span>],[<span class="number">1.0</span>,<span class="number">1.0</span>],[<span class="number">0</span>,<span class="number">0</span>],[<span class="number">0</span>,<span class="number">0.1</span>]])</span><br><span class="line">        labels = [<span class="string">'A'</span>,<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'B'</span>]</span><br><span class="line">        <span class="keyword">return</span> group,labels</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">KnnClassify</span><span class="params">(self,testX,trainX,labels,K)</span>:</span></span><br><span class="line">        [N,M]=trainX.shape</span><br><span class="line">    </span><br><span class="line">    <span class="comment">#calculate the distance between testX and other training samples</span></span><br><span class="line">        difference = tile(testX,(N,<span class="number">1</span>)) - trainX <span class="comment"># tile for array and repeat for matrix in Python, == repmat in Matlab</span></span><br><span class="line">        difference = difference ** <span class="number">2</span> <span class="comment"># take pow(difference,2)</span></span><br><span class="line">        distance = difference.sum(<span class="number">1</span>) <span class="comment"># take the sum of difference from all dimensions</span></span><br><span class="line">        distance = distance ** <span class="number">0.5</span></span><br><span class="line">        sortdiffidx = distance.argsort()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># find the k nearest neighbours</span></span><br><span class="line">        vote = &#123;&#125; <span class="comment">#create the dictionary</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(K):</span><br><span class="line">            ith_label = labels[sortdiffidx[i]];</span><br><span class="line">            vote[ith_label] = vote.get(ith_label,<span class="number">0</span>)+<span class="number">1</span> <span class="comment">#get(ith_label,0) : if dictionary 'vote' exist key 'ith_label', return vote[ith_label]; else return 0</span></span><br><span class="line">        sortedvote = sorted(vote.iteritems(),key = <span class="keyword">lambda</span> x:x[<span class="number">1</span>], reverse = <span class="keyword">True</span>)</span><br><span class="line">        <span class="comment"># 'key = lambda x: x[1]' can be substituted by operator.itemgetter(1)</span></span><br><span class="line">        <span class="keyword">return</span> sortedvote[<span class="number">0</span>][<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">k = KNN() <span class="comment">#create KNN object</span></span><br><span class="line">group,labels = k.createDataset()</span><br><span class="line">cls = k.KnnClassify([<span class="number">0</span>,<span class="number">0</span>],group,labels,<span class="number">3</span>)</span><br><span class="line"><span class="keyword">print</span> cls</span><br></pre></td></tr></table></figure>
<hr>
<p>运行：</p>
<ol>
<li>在Python Shell 中可以运行KNN.py</li>
</ol>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> os</span><br><span class="line">&gt;&gt;&gt;os.chdir(<span class="string">"/home/liudiwei/code/data_miningKNN/"</span>)</span><br><span class="line">&gt;&gt;&gt;execfile(<span class="string">"KNN.py"</span>)</span><br></pre></td></tr></table></figure>
<p>输出:B<br>（B表示类别）</p>
<p>2.或者terminal中直接运行</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ python KNN.py</span><br></pre></td></tr></table></figure>
<p>3.也可以不在KNN.py中写输出，而选择在Shell中获得结果，i.e.,</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">import</span> KNN</span><br><span class="line"><span class="prompt">&gt;&gt;&gt; </span>KNN.k.KnnClassify([<span class="number">0</span>,<span class="number">0</span>],KNN.group,KNN.labels,<span class="number">3</span>)</span><br></pre></td></tr></table></figure>
<h3 id="References"><strong>References</strong></h3><p>【1】Introduction to Data Mining <a href="http://vdisk.weibo.com/s/akTUdytgliZM8" target="_blank" rel="external">数据挖掘导论</a><br>【2】<a href="http://blog.csdn.net/abcjennifer/article/details/19757987" target="_blank" rel="external">Rachel Zhang-K近邻分类算法实现 in Python</a></p>
<hr>
<p>附件（两张自己的计算过程图）：</p>
<center><img src="http://img.blog.csdn.net/20150524192410343" alt="这里写图片描述"><br><strong>图1 KNN算法核心部分</strong><br></center><br><center><br><img src="http://img.blog.csdn.net/20150524192640924" alt="这里写图片描述"><br><strong>图2 简易计算过程</strong><br></center><br>说明：上述图片仅供参考，看不懂就自己测试一组数据如[0,1]慢慢推导一下吧<br><br>———-<br><br><center><strong>本栏目机器学习算法持续更新中……</strong></center>]]></content>
    <summary type="html">
    <![CDATA[<p><strong>引文</strong>：决策树和基于规则的分类器都是<strong>积极学习方法</strong>（eager learner）的例子，因为一旦训练数据可用，他们就开始学习从输入属性到类标号的映射模型。一个相反的策略是推迟对训练数据的建模，直到需要分类测试样例时再进行。采用这种策略的技术被称为<strong>消极学习法</strong>（lazy learner）。<strong>最近邻分类器</strong>就是这样的一种方法。</p>]]>
    
    </summary>
    
      <category term="Machine-Learning Python" scheme="http://csuldw.github.io/tags/Machine-Learning-Python/"/>
    
      <category term="Machine-Learning Python" scheme="http://csuldw.github.io/categories/Machine-Learning-Python/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[机器学习算法-决策树理论]]></title>
    <link href="http://csuldw.github.io/2015/05/08/2015-05-08-decision%20tree/"/>
    <id>http://csuldw.github.io/2015/05/08/2015-05-08-decision tree/</id>
    <published>2015-05-08T13:53:12.000Z</published>
    <updated>2015-07-23T02:29:24.635Z</updated>
    <content type="html"><![CDATA[<p><strong>用较少的东西，同样可以做好的事情。越是小的决策树，越优越于大的决策树。</strong></p>
<h2 id="分类引文"><strong>分类引文</strong></h2><p>数据分类是一个两阶段过程，包括学习阶段（构建分类模型）和分类阶段（使用模型预测给定数据的类标号）。决策树分类算法是监督学习的一种，即supervised learning。</p>
<ul>
<li>分类过程的第一阶段也可以看做学习一个映射或函数y=f(x),它可以预测给定元组X的类标号y。</li>
<li>在第二阶段，使用模型进行分类。首先评估分类器的预测准确率。这个过程要尽量的减少<strong>过分拟合</strong>。（为什么是尽量减少而不是避免呢，因为过拟合是避免不了的，再好的模型都会有过拟合的情况）</li>
</ul>
<a id="more"></a>
<h2 id="1_决策树归纳"><strong>1 决策树归纳</strong></h2><p> 决策树归纳是从有类标号的训练元组中学习决策树。常用的决策树算法有ID3，C4.5和CART。它们都是采用贪心（即非回溯的）方法，其中决策树自顶向下递归的分治方法构造。</p>
<h2 id="2_基本原理"><strong>2 基本原理</strong></h2><h3 id="2-1_算法优点"><strong>2.1 算法优点</strong></h3><p> 决策树算法的优点如下：<br>（1）分类精度高；<br>（2）成的模式简单；<br>（3）对噪声数据有很好的健壮性。<br>因而是目前应用最为广泛的归纳推理算法之一，在数据挖掘中受到研究者的广泛关注。</p>
<h3 id="2-2_算法一般流程"><strong>2.2 算法一般流程</strong></h3><p>（1）收集数据：任意方法和途径。<br>（2）准备数据：书构造算法只适用于标称型数据，因此数据必须离散化。<br>（3）分析数据：构造树完成后，检查图形是否符合预测。<br>（4）训练算法：决策树的数据构造。<br>（5）测试算法：一般将决策树用于分类，可以用错误率衡量，而错误率使用经验率计算。<br>（6）使用算法：决策树可以用于任何监督学习算法。</p>
<h3 id="2-3_实例"><strong>2.3 实例</strong></h3><p><strong>信息增益和熵（克劳德.香农提出）</strong></p>
<h4 id="1-使用信息增益进行决策树归纳"><strong>1.使用信息增益进行决策树归纳</strong></h4><p><strong>信息增益度量属性选择</strong><br><strong>熵的计算公式</strong></p>
<p>熵定义为信息增益的期望值。对$D$中的元组分类所需要的期望信息由下列公式给出：</p>
<p>$$entropy=Info(D)=-∑_i^{n}p_ilog_2(p_i)$$</p>
<p>$$p_i是D中任意元组属于类C_i非零概率。$$</p>
<p><strong>信息增益计算公式</strong></p>
<p>$$Info_A(D)=∑_j\dfrac{|D_j|}{|D|}\times Info(D_j)$$</p>
<p>$$项\dfrac{|D_i|}{|D|}充当第j个分区的权重。$$</p>
<p><strong>需要的期望信息越小，分区的纯度越高。</strong></p>
<p>下面是<strong>训练元组数据D</strong></p>
<p><img src="http://img.blog.csdn.net/20150513110022176" alt="这里写图片描述"><br>在这里</p>
<p>$$Info(D)=\dfrac{5}{14}\times(-\dfrac{2}{5}log_2\dfrac{2}{5}-\dfrac{3}{5}log_2 \dfrac{3}{5})+\dfrac{4}{14}\times(-\dfrac{4}{4}log_2\dfrac{0}{4}-\dfrac{0}{4}log_2 \dfrac{0}{4})+\dfrac{5}{14}\times(-\dfrac{3}{5}log_2\dfrac{3}{5}-\dfrac{2}{5}log_2 \dfrac{2}{5})=0.694位$$</p>
<p>因此这种划分的信息增益</p>
<p>$$Gain(age)=Info(D)-Info_age(D)=0.940-0.964=0.246位$$</p>
<p>类似的可以计算出：</p>
<p>$$ Grain(income)=0.029位，<br>Grain(student)=0.151位，Grain(credit_rating)=0.048位 $$</p>
<p>由于age在属性中具有最高的信息增益，所以它被选作分裂属性。<br>下面再进行递归计算信息增益，在此就不展示了。</p>
<h4 id="2-使用增益率计算"><strong>2.使用增益率计算</strong></h4><p><strong>分裂信息计算公式：</strong></p>
<p>$$SplitInfo_A(D)=-∑\dfrac{|D_j|}{|D|}\times log_2(\dfrac{|D_j|}{|D|})$$</p>
<p>增益率定义为：</p>
<p>$$GrainRate(A)=\dfrac{Grain(A)}{SplitInfo_A(D)}$$</p>
<p>选择具有最大增益率的属性作为分裂属性。</p>
<h4 id="3-基尼指数计算"><strong>3.基尼指数计算</strong></h4><p>$$Gini(D)=1-∑p^{2}_i$$</p>
<p>$$其中，p_i是D中元组数以C_i类的概率，对m个类计算和。$$</p>
]]></content>
    <summary type="html">
    <![CDATA[<p><strong>用较少的东西，同样可以做好的事情。越是小的决策树，越优越于大的决策树。</strong></p>
<h2 id="分类引文"><strong>分类引文</strong></h2><p>数据分类是一个两阶段过程，包括学习阶段（构建分类模型）和分类阶段（使用模型预测给定数据的类标号）。决策树分类算法是监督学习的一种，即supervised learning。</p>
<ul>
<li>分类过程的第一阶段也可以看做学习一个映射或函数y=f(x),它可以预测给定元组X的类标号y。</li>
<li>在第二阶段，使用模型进行分类。首先评估分类器的预测准确率。这个过程要尽量的减少<strong>过分拟合</strong>。（为什么是尽量减少而不是避免呢，因为过拟合是避免不了的，再好的模型都会有过拟合的情况）</li>
</ul>]]>
    
    </summary>
    
      <category term="Machine-Learning python" scheme="http://csuldw.github.io/tags/Machine-Learning-python/"/>
    
      <category term="Machine-Learning" scheme="http://csuldw.github.io/categories/Machine-Learning/"/>
    
  </entry>
  
  <entry>
    <title><![CDATA[代码校验工具 SublimeLinter 的安装与使用]]></title>
    <link href="http://csuldw.github.io/2015/03/26/2015-03-26-sublimeLinter/"/>
    <id>http://csuldw.github.io/2015/03/26/2015-03-26-sublimeLinter/</id>
    <published>2015-03-26T07:14:54.000Z</published>
    <updated>2015-07-23T02:29:24.294Z</updated>
    <content type="html"><![CDATA[<h2 id="序">序</h2><p>本文我将讲述一下 SublimeLinter 的安装过程。<br>其组件 jshint 的安装与使用。<br>其组件 csslint 的安装与使用。<br>我将基于 <a href="http://sublimetext.com/3" target="_blank" rel="external">Sublime Text 3</a> 来安装。<br>使用 Sublime Text 2 的用户阅读本文是没有帮助的。   </p>
<p>SublimeLinter 是 Sublime 的插件，它的作用是检查代码语法是否有错误，并提示。习惯了 IDE 下写代码的人一定需要一款在 Sublime 上类似的语法检查工具。下面我们开始。   </p>
<a id="more"></a>
<hr>
<h2 id="安装_SublimeLinter">安装 SublimeLinter</h2><p>如同其他插件一样使用 Package Control 来安装。   </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter</code>。进行安装.   </li>
</ol>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-sublimeLinter.jpg" alt="SublimeLinter">   </p>
<p>安装完成后可以看到这样一段话：   </p>
<pre><code class="markdown">Welcome to SublimeLinter, a linter framework for Sublime Text 3.

                  * * * IMPORTANT! * * *

         SublimeLinter 3 is NOT a drop-in replacement for
        earlier versions.

         Linters *NOT* included with SublimeLinter 3, 
         they must be installed separately.

         The settings are different.

                 * * * READ THE DOCS! * * *

 Otherwise you will never know how to install linters, nor will
 you know about all of the great new features in SublimeLinter 3.

 For complete documentation on how to install and use SublimeLinter,
 please see:

 http://www.sublimelinter.com</code></pre>   

<p>可以看到具体的 Linters 组件<strong>不</strong>被包含在 SublimeLinter 3 中，所以我们要额外独立安装组件。<br>可以针对不同的语言安装不同的组件。   </p>
<hr>
<h2 id="JavaScript_语法检查">JavaScript 语法检查</h2><p>SublimeLinter-jshint 是基于 nodeJS 下的 jshint 的插件，实际上 SublimeLinter-jshint 调用了 nodeJS 中 jshint 的接口来进行语法检查的。   </p>
<hr>
<h3 id="安装_SublimeLinter-jshint">安装 SublimeLinter-jshint</h3><p>为了让 JavaScript 代码有语法检查，我们安装 SublimeLinter-jshint<br>同样的方法，我们安装 SublimeLinter-jshint    </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter-jshint</code>。进行安装.   </li>
</ol>
<p>如下图   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint.jpg" alt="SublimeLinter-jshint">   </p>
<p>安装完成后我们可以看到下面的一段话   </p>
<h2 id="SublimeLinter-jshint"><pre><code class="markdown">SublimeLinter-jshint</code></pre></h2><p>  This linter plugin for SublimeLinter provides an interface to jshint.</p>
<p>  <strong> IMPORTANT! </strong></p>
<p>  Before this plugin will activate, you <em>must</em><br>  follow the installation instructions here:</p>
<p>  <a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">https://github.com/SublimeLinter/SublimeLinter-jshint</a><br></p>
<hr>
<h3 id="安装_nodeJS_和_jshint">安装 nodeJS 和 jshint</h3><p>在插件开始工作之前，我们必须再看一下上述插件的<a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">安装说明</a><br>通过 <a href="https://github.com/SublimeLinter/SublimeLinter-jshint" target="_blank" rel="external">SublimeLinter-jshint 的说明</a> 我们可以看到，这个组件依赖于 nodeJS 下的 jshint，所以我们安装 nodeJS 环境和 nodeJS 下的 jshint。   </p>
<ol>
<li>安装 <a href="https://nodejs.org/" target="_blank" rel="external">Node.js</a>   </li>
<li>通过 npm 安装<code>jshint</code>   </li>
</ol>
<p>在命令行下输入如下代码，完成安装   </p>
<pre><code>npm <span class="keyword">install</span> -g jshint
</code></pre><p>安装完成后命令行中出现如下的信息   </p>
<pre><code><span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\jshint -&gt; <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\jshint\bin\jshint
jshint<span class="variable">@2</span>.<span class="number">6.3</span> <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\jshint
├── strip-json-comments<span class="variable">@1</span>.<span class="number">0.2</span>
├── underscore<span class="variable">@1</span>.<span class="number">6.0</span>
├── exit<span class="variable">@0</span>.<span class="number">1.2</span>
├── shelljs<span class="variable">@0</span>.<span class="number">3.0</span>
├── console-browserify<span class="variable">@1</span>.<span class="number">1.0</span> (date-now<span class="variable">@0</span>.<span class="number">1.4</span>)
├── htmlparser2<span class="variable">@3</span>.<span class="number">8.2</span> (domelementtype<span class="variable">@1</span>.<span class="number">3.0</span>, entities<span class="variable">@1</span>.<span class="number">0.0</span>, domhandler<span class="variable">@2</span>.<span class="number">3.0</span>, readable-stream<span class="variable">@1</span>.<span class="number">1.13</span>, domutils<span class="variable">@1</span>.<span class="number">5.1</span>)
├── minimatch<span class="variable">@1</span>.<span class="number">0.0</span> (sigmund<span class="variable">@1</span>.<span class="number">0.0</span>, lru-cache<span class="variable">@2</span>.<span class="number">5.0</span>)
└── cli<span class="variable">@0</span>.<span class="number">6.6</span> (glob<span class="variable">@3</span>.<span class="number">2.11</span>)
</code></pre><p>可以查看 jshint 版本，已确认安装完成。  </p>
<pre><code>C:<span class="command">\Users</span><span class="command">\Administrator</span>&gt;jshint -v
jshint v2.6.3
</code></pre><p>现在，恭喜你，我们使用 Sublime 编辑 JavaScript 文件，就会有语法检查了！   </p>
<p>在编辑过程中，会有如下提示   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint-test.jpg" alt="SublimeLinter-jshint-test"></p>
<p>点击提示点后，Sublime 状态栏也会有相应的说明   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-jshint-test2.jpg" alt="SublimeLinter-jshint-test2"></p>
<hr>
<h2 id="css_语法检查">css 语法检查</h2><p>与 jshint 同理，SublimeLinter-csslint 也是基于 nodeJS 下的 csslint 的插件，实际上 SublimeLinter-csslint 调用了 nodeJS 中 csslint 的接口来进行语法检查的。   </p>
<hr>
<h3 id="安装_SublimeLinter-csslint">安装 SublimeLinter-csslint</h3><p>同样的方法。   </p>
<ol>
<li>按下 <code>Ctrl+Shift+p</code> 进入 Command Palette   </li>
<li>输入<code>install</code>进入 Package Control: Install Package   </li>
<li>输入<code>SublimeLinter-csslint</code>。进行安装.   </li>
</ol>
<p>如下图   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint.jpg" alt="SublimeLinter-csslint">   </p>
<p>安装完成后我们可以看到下面的一段话   </p>
<pre><code><span class="header">SublimeLinter-csslint
-------------------------------</span>
This linter plugin for SublimeLinter provides an interface to csslint.

<span class="bullet">** </span>IMPORTANT! **

Before this plugin will activate, you <span class="strong">*must*</span>
follow the installation instructions here:

https://github.com/SublimeLinter/SublimeLinter-csslint
</code></pre><p>在使用插件之前，必须遵循上述网址中的<a href="https://github.com/SublimeLinter/SublimeLinter-csslint" target="_blank" rel="external">安装说明</a>   </p>
<hr>
<h3 id="在_nodeJS_下安装_csslint">在 nodeJS 下安装 csslint</h3><p>进入上述的 GitHub 地址，csslint 的说明页。我们知道了和 jshint 一样，csslint 也是基于 nodeJS 下的 csslint 来使用的。   </p>
<p>这里安装 nodeJS 过程省略。<br>只需用 npm 安装 csslint 即可。   </p>
<p>在命令行中输入     </p>
<pre><code>npm <span class="keyword">install</span> -g csslint   
</code></pre><p>安装完成后命令行中出现如下的信息     </p>
<pre><code><span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\csslint -&gt; <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\csslint\cli.js
csslint<span class="variable">@0</span>.<span class="number">10.0</span> <span class="attribute">C</span>:\Users\Administrator\AppData\Roaming\npm\node_modules\csslint
└── parserlib<span class="variable">@0</span>.<span class="number">2.5</span>
</code></pre><p>可以查看 csslint 版本，已确认安装完成。   </p>
<pre><code>C:<span class="command">\Users</span><span class="command">\Administrator</span>&gt;csslint --version
v0.10.0
</code></pre><p>现在，恭喜你，我们使用 Sublime 编辑 css 文件，就会有语法检查了！     </p>
<p>在编辑过程中，会有如下提示   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint-test.jpg" alt="SublimeLinter-csslint-test"></p>
<p>点击提示点后，Sublime 状态栏也会有相应的说明   </p>
<p><img src="http://7q5cdt.com1.z0.glb.clouddn.com/SublimeLinter-csslint-test2.jpg" alt="SublimeLinter-csslint-test2"></p>
]]></content>
    <summary type="html">
    <![CDATA[<h2 id="序">序</h2><p>本文我将讲述一下 SublimeLinter 的安装过程。<br>其组件 jshint 的安装与使用。<br>其组件 csslint 的安装与使用。<br>我将基于 <a href="http://sublimetext.com/3">Sublime Text 3</a> 来安装。<br>使用 Sublime Text 2 的用户阅读本文是没有帮助的。   </p>
<p>SublimeLinter 是 Sublime 的插件，它的作用是检查代码语法是否有错误，并提示。习惯了 IDE 下写代码的人一定需要一款在 Sublime 上类似的语法检查工具。下面我们开始。   </p>]]>
    
    </summary>
    
      <category term="SublimeLinter" scheme="http://csuldw.github.io/tags/SublimeLinter/"/>
    
      <category term="SublimeLinter" scheme="http://csuldw.github.io/categories/SublimeLinter/"/>
    
  </entry>
  
</feed>