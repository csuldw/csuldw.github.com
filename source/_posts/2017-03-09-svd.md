---
layout: post
date: 2017-03-09 20:31
title: "SVD奇异值分解逐步推导"
categories: 推荐系统
tag:
  - SVD
  - 奇异值
  - 推荐系统
comment: true
---

在先前的文章[PCA主成分分析Python实现](https://www.csuldw.com/2016/02/28/2016-02-28-pca/)中介绍了一种降维方法，本文主要介绍另一种方法：SVD(Singular Value Decomposition，奇异值分解)。该算法在推荐系统中经常用到，而且也可用于降维，PCA的实现除了通过求解协方差矩阵、特征值、特征向量来计算以外，还可以使用SVD来求解，具体细节本文就不讨论了，下面主要来介绍下SVD的推导。

<!--more-->

## 基本理论

1. 对于$n$阶方阵$A$，有$Ax=\lambda x$，则$\lambda$特征值，$x$特征向量;
2. 如果向量$\vec{a}$和向量$\vec{b}$正交，则$\vec{a} \cdot \vec{b} = 0$;
3. 一个内积空间的正交基（orthogonal basis）是元素两两正交的基,基中的元素称为基向量。如果一个正交基的基向量的模长都是单位长度1，则称这正交基为标准正交基或"规范正交基"（Orthonormal basis）；
4. A与A的转置矩阵是有相同的特征值,但是他们各自的特征向量没有关系；


## SVD推导

### Step1：矩阵分解

对于矩阵$A$，有$A^TA = \lambda\_{i} v\_{i}$，其中$\lambda\_{i}$为特征值，$v\_{i}$为特征向量。假定$(v\_{i}, v\_{j})$是一组正交基，那么有$v\_{i}^{T} \cdot v\_{j} = 0$，那么：

$$
\begin{align}
(Av\_{i}, Av\_{j}) 
&= (Av\_{i})^{T} \cdot Av\_{j} \\\\
&= v\_{i}^{T} A^T Av\_{j} \\\\
&= v\_{i}^{T} \lambda\_{j} v\_{j} \\\\
&= \lambda\_{j} \color{red}{v\_{i}^{T} v\_{j}} \\\\
&= 0
\end{align}
\label{1}\tag{1}
$$

可以得出$Av\_{i}, Av\_{j}$也是一组正交基，根据公式\eqref{1}可以推导出$(Av\_{i}, Av\_{i}) = \lambda\_{i} v\_{i}^{T} v\_{i}=\lambda\_{i} $，从而可以得到：

$$
\begin{align}
& |Av\_{i}|^2 = \lambda\_{i} \\\\
& |Av\_{i}| = \sqrt{\lambda\_{i}}
\end{align}
\label{2}\tag{2}
$$

根据公式\eqref{2}，有 $\frac{Av\_{i}}{|Av\_{i}|} = \frac{1}{\sqrt{\lambda\_{i}}} Av\_{i}$，令$ \frac{1}{\sqrt{\lambda\_{i}}} Av\_{i}= u\_{i}$，可以得到

$$
Av\_{i}= \sqrt{\lambda\_{i}}u\_{i}=\delta\_{i}u\_{i}
\label{3}\tag{3}
$$

其中$\delta\_{i} = \sqrt{\lambda\_{i}}$，得到这个表达之后，我们可以进一步推导：


$$
\begin{align}
AV &= A(v\_{1}, v\_{2}, \dots, v\_{n} ) \\\\
&= (Av\_{1}, Av\_{2}, \dots, Av\_{n} ) \\\\
&= (\delta\_{1}u\_{1}, \delta\_{2}u\_{2}, \dots, \delta\_{n}u\_{n} ) \\\\
&= U\Sigma
\end{align}
\label{4}\tag{4}
$$


从而可以得出：


$$
A = U\Sigma V^T
\label{5}\tag{5}
$$

### Step2：矩阵计算

得到矩阵$A$的表示之后，我们应该如何计算向量$U$和$V$呢？继续往下面分析：

首先计算出$A$的转置$A^T$:

$$
\begin{align}
A^T =  V\Sigma^TU^T
\end{align}
\label{6}\tag{6}
$$

得到$A$的转置之后，我们接下来计算$A^TA$值：

$$
\begin{align}
A^TA 
&= V\Sigma^TU^T U\Sigma V^T \\\\
&= V\Sigma^2V^T
\end{align}
\label{7}\tag{7}
$$

通过公式\eqref{7}，可以得到$A^TA v\_{i} = \lambda\_{i}v\_{i}$，只需要求出$A^TA$的特征向量即可得到$V$.

同理可得$AA^T$的值：

$$
\begin{align}
A A^T
&= U\Sigma V^T V\Sigma^TU^T \\\\
&= U\Sigma^2U^T
\end{align}
\label{8}\tag{8}
$$

通过公式\eqref{8}，可以得到$AA^T u\_{i} = \lambda\_{i}u\_{i}$，只需要求出$AA^T$的特征向量即可得到$U$.

## Conclusion

SVD将矩阵分解为$U$和$V$，得到$A = U\Sigma V^T$。我们可以取前k个非零奇异值，将这k个对应的奇异向量合并来进行降维。SVD还可以用于PCA的求解过程，主要用到SVD的右奇异矩阵$V$。

## References

1. [Singular_value_decomposition](https://en.wikipedia.org/wiki/Singular_value_decomposition)


