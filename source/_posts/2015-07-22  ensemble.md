---
layout: post
date: 2015-07-22 06:53
title: "机器学习-组合算法总结"
tags: 
	- Machine Learning
comment: true
categories: ML
---


## __组合模型__


下面简单的介绍下Bootstraping, Bagging, Boosting, AdaBoost, RandomForest 和Gradient boosting这些组合型算法.

### __1.Bootstraping__

**Bootstraping**: 名字来自成语“pull up by your own bootstraps”，意思就是依靠你自己的资源，称为自助法，它是一种有放回的抽样方法，它是非参数统计中一种重要的估计统计量方差进而进行区间估计的统计方法。其核心思想和基本步骤如下：  
 <!-- more -->
>（1）采用重抽样技术从原始样本中抽取一定数量（自己给定）的样本，此过程允许重复抽样。    
>（2）根据抽出的样本计算给定的统计量T。  
>（3）重复上述N次（一般大于1000），得到N个统计量T。  
>（4）计算上述N个统计量T的样本方差，得到统计量的方差。 


应该说Bootstrap是现代统计学较为流行的一种统计方法，在小样本时效果很好。通过方差的估计可以构造置信区间等，其运用范围得到进一步延伸。

---

### **2.装袋bagging**

装袋算法相当于多个专家投票表决，对于多次测试，每个样本返回的是多次预测结果较多的那个。

装袋算法描述

```
模型生成
	令n为训练数据的实例数量
	对于t次循环中的每一次
		从训练数据中采样n个实例
		将学习应用于所采样本
		保存结果模型
分类
	对于t个模型的每一个
		使用模型对实例进行预测
	返回被预测次数最多的一个
```

bagging：bootstrap aggregating的缩写。让该学习算法训练多轮，每轮的训练集由从初始的训练集中随机取出的n个训练样本组成，某个初始训练样本在某轮训练集中可以出现多次或根本不出现，训练之后可得到一个预测函数序列

$$h_1，⋯ ⋯h_n$$ 

最终的预测函数H对分类问题采用**投票方式**，对回归问题采用**简单平均方法**对新示例进行判别。

[训练R个分类器f_i，分类器之间其他相同就是参数不同。其中f_i是通过从训练集合中(N篇文档)随机取(取后放回)N次文档构成的训练集合训练得到的。对于新文档d，用这R个分类器去分类，得到的最多的那个类别作为d的最终类别。]

使用scikit-learn测试bagging方法

```
>>> from sklearn.ensemble import BaggingClassifier
>>> from sklearn.neighbors import KNeighborsClassifier
>>> bagging = BaggingClassifier(KNeighborsClassifier(),
...                             max_samples=0.5, max_features=0.5)
```

---

### __3.提升Boosting与Adaboost__

**提升算法描述**

```
模型生成
	赋予每个训练实例相同的权值
	t次循环中的每一次：
		将学习算法应用于加了权的数据集上并保存结果模型
		计算模型在加了权的数据上的误差e并保存这个误差
		结果e等于0或者大于等于0.5：
			终止模型
		对于数据集中的每个实例：
			如果模型将实例正确分类
				将实例的权值乘以e/(1-e)
		将所有的实例权重进行正常化
分类
	赋予所有类权重为0
	对于t（或小于t）个模型中的每一个：
		给模型预测的类加权 -log(e/(1-e))
	返回权重最高的类
```

这个模型提供了一种巧妙的方法生成一系列互补型的专家。

**boosting**: 其中主要的是**AdaBoost**（Adaptive boosting，自适应boosting）。初始化时对每一个训练例赋相等的权重1／N，然后用该学算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在后续的学习中集中对比较难的训练例进行学习，从而得到一个预测函数序列$h_1,⋯, h_m$ , 其中h_i也有一定的权重，预测效果好的预测函数权重较大，反之较小。最终的预测函数H对分类问题采用有权重的投票方式，对回归问题采用加权平均的方法对新示例进行判别。

提升算法理想状态是这些模型对于其他模型来说是一个补充，每个模型是这个领域的一个专家，而其他模型在这部分却不能表现很好，就像执行官一样要寻觅那些技能和经验互补的顾问，而不是重复的。这与装袋算法有所区分。

Adaboost算法描述

```
模型生成
	训练数据中的每个样本，并赋予一个权重，构成权重向量D，初始值为1/N
	t次循环中的每一次：
		在训练数据上训练弱分类器并计算分类器的错误率e
		如果e等于0或者大于等于用户指定的阈值：
			终止模型，break
		重新调整每个样本的权重，其中alpha=0.5*ln((1-e)/e)
		对权重向量D进行更新，正确分类的样本的权重降低而错误分类的样本权重值升高
		对于数据集中的每个样例：
			如果某个样本正确分类：
				权重改为D^(t+1)_i = D^(t)_i * e^(-a)/Sum(D)
			如果某个样本错误分类：
				权重改为D^(t+1)_i = D^(t)_i * e^(a)/Sum(D)
分类
	赋予所有类权重为0
	对于t（或小于t）个模型（基分类器）中的每一个：
		给模型预测的类加权 -log(e/(1-e))
	返回权重最高的类
```
（类似Bagging方法，但是训练是串行进行的，第k个分类器训练时关注对前k-1分类器中错分的文档，即不是随机取，而是加大取这些文档的概率。)

__bagging与boosting的区别__：

二者的主要区别是**取样方式不同**。bagging采用**均匀取样**，而Boosting根据**错误率来取样**，因此boosting的分类精度要优于Bagging。bagging的训练集的选择是随机的，各轮训练集之间相互独立，而boostlng的各轮训练集的选择与前面各轮的学习结果有关；bagging的各个预测函数没有权重，而boosting是有权重的；bagging的各个预测函数可以并行生成，而boosting的各个预测函数只能顺序生成。对于象神经网络这样极为耗时的学习方法。bagging可通过并行训练节省大量时间开销。

bagging和boosting都可以有效地提高分类的准确性。在大多数数据集中，boosting的准确性比bagging高。在有些数据集中，boosting会引起退化--- Overfit。  

Boosting思想的一种改进型AdaBoost方法在邮件过滤、文本分类方面都有很好的性能。 

**Gradient boosting（又叫Mart, Treenet)**：Boosting是一种思想，Gradient Boosting是一种实现Boosting的方法，它主要的思想是，每一次建立模型是在之前建立模型**损失函数的梯度下降方向**。**损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错。**如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是**让损失函数在其梯度（Gradient)的方向上下降**。  

使用scikit-learn测试adaboost算法

```
>>> from sklearn.cross_validation import cross_val_score
>>> from sklearn.datasets import load_iris
>>> from sklearn.ensemble import AdaBoostClassifier
>>> iris = load_iris()
>>> clf = AdaBoostClassifier(n_estimators=100)
>>> scores = cross_val_score(clf, iris.data, iris.target)
>>> scores.mean()                             
0.9...
```



---

### __4.Random Forest__

**Random Forest**： 随机森林，顾名思义，是用随机的方式建立一个森林，森林里面有很多的决策树组成，随机森林的每一棵决策树之间是没有关联的。在得到森林之后，当有一个新的输入样本进入的时候，就让森林中的每一棵决策树分别进行一下判断，看看这个样本应该属于哪一类（对于分类算法），然后看看哪一类被选择最多，就预测这个样本为那一类。 在建立每一棵决策树的过程中，有两点需要注意——**采样**与**完全分裂**。首先是两个随机采样的过程，random forest对输入的数据要进行行和列的采样。对于行采样，采用有放回的方式，也就是在采样得到的样本集合中，可能有重复的样本。假设输入样本为N个，那么采样的样本也为N个。这样使得在训练的时候，每一棵树的输入样本都不是全部的样本，使得相对不容易出现over-fitting。然后进行列采样，从M个feature中，选择m个(m << M)。之后就是对采样之后的数据使用完全分裂的方式建立出决策树，这样决策树的某一个叶子节点要么是无法继续分裂的，要么里面的所有样本的都是指向的同一个分类。**一般很多的决策树算法都一个重要的步骤——剪枝，但随机森林不这样做，由于之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现over-fitting。** 按这种算法得到的随机森林中的每一棵都是很弱的，但是大家组合起来就很厉害了。可以这样比喻随机森林算法：每一棵决策树就是一个精通于某一个窄领域的专家（因为我们从M个feature中选择m让每一棵决策树进行学习），这样在随机森林中就有了很多个精通不同领域的专家，对一个新的问题（新的输入数据），可以用不同的角度去看待它，最终由各个专家，投票得到结果。  

__Random forest与bagging的区别__：

(1)Random forest是选与输入样本的数目相同多的次数（可能一个样本会被选取多次，同时也会造成一些样本不会被选取到），而bagging一般选取比输入样本的数目少的样本；  
(2)bagging是用全部特征来得到分类器，而Random forest是需要从全部特征中选取其中的一部分来训练得到分类器； **一般Random forest效果比bagging效果好！**

使用scikit-learn测试随机森林算法

```
>>> from sklearn.ensemble import RandomForestClassifier
>>> X = [[0, 0], [1, 1]]
>>> Y = [0, 1]
>>> clf = RandomForestClassifier(n_estimators=10)
>>> clf = clf.fit(X, Y)
```


### __5.Gradient boosting__

梯度提升树或者梯度提升回归树(GBRT)是任意一个不同损失函数的泛化。GBRT是一个灵敏的并且高效程序，可以用在回归和分类中。梯度提升树模型在许多领域中都有使用，如web搜索排行榜和社会生态学中。它主要的思想是，每一次建立模型是在之前建立模型损失函数的梯度下降方向。这句话有一点拗口，损失函数(loss function)描述的是模型的不靠谱程度，损失函数越大，则说明模型越容易出错（其实这里有一个方差、偏差均衡的问题，但是这里就假设损失函数越大，模型越容易出错）。如果我们的模型能够让损失函数持续的下降，则说明我们的模型在不停的改进，而最好的方式就是让损失函数在其梯度（Gradient)的方向上下降。


GRBT的优势：

- 混合数据类型的自然处理
- 预测力强
- 健壮的输出空间

Boosting主要是一种思想，表示“知错就改”。而Gradient Boosting是在这个思想下的一种函数（也可以说是模型）的优化的方法，首先将函数分解为可加的形式（其实所有的函数都是可加的，只是是否好放在这个框架中，以及最终的效果如何）。然后进行m次迭代，通过使得损失函数在梯度方向上减少，最终得到一个优秀的模型。值得一提的是，每次模型在梯度方向上的减少的部分，可以认为是一个“小”的或者“弱”的模型，最终我们会通过加权(也就是每次在梯度方向上下降的距离）的方式将这些“弱”的模型合并起来，形成一个更好的模型。

------

<br>
